[{"title":"SkyWalking 全链路追踪（一）部署和初体验","url":"/p/45444/","content":"前言SkyWalking 是什么？\n分布式系统的应用程序性能监视工具，专为微服务、云原生架构和基于容器（Docker、K8s、Mesos）架构而设计。提供分布式追踪、服务网格遥测分析、度量聚合和可视化一体化解决方案。\n\n简单来说，SkyWalking 是一款全链路追踪系统，可以视为 OpenTracing 的一种实现，类似的还有 Zipkin、Jaeger 等等，但是 SkyWalking 的接入方式采用了 Java Agent 的方式，达到了 0 代码无侵入，接入成本几乎为零。所以非常推荐使用。\n部署参考官方文档，简要的总结下 DockerCompose 的部署方式。\n新建 docker-compose.yaml 文件version: '3.8'services:  elasticsearch:    image: elasticsearch:7.17.6    container_name: elasticsearch    ports:      - \"9200:9200\"    volumes:      - ./esdata01:/usr/share/elasticsearch/data    healthcheck:      test: [ \"CMD-SHELL\", \"curl --silent --fail localhost:9200/_cluster/health || exit 1\" ]      interval: 30s      timeout: 10s      retries: 3      start_period: 10s    environment:      - discovery.type=single-node      - bootstrap.memory_lock=true      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"    ulimits:      memlock:        soft: -1        hard: -1  oap:    image: apache/skywalking-oap-server:8.9.1    container_name: oap    depends_on:      elasticsearch:        condition: service_healthy    links:      - elasticsearch    ports:      - \"11800:11800\"      - \"12800:12800\"    healthcheck:      test: [ \"CMD-SHELL\", \"/skywalking/bin/swctl ch\" ]      interval: 30s      timeout: 10s      retries: 3      start_period: 10s    environment:      SW_STORAGE: elasticsearch      SW_STORAGE_ES_CLUSTER_NODES: elasticsearch:9200      SW_HEALTH_CHECKER: default      SW_TELEMETRY: prometheus      JAVA_OPTS: \"-Xms2048m -Xmx2048m\"  ui:    image: apache/skywalking-ui:8.9.1    container_name: ui    depends_on:      oap:        condition: service_healthy    links:      - oap    ports:      - \"8090:8080\"    environment:      SW_OAP_ADDRESS: http://oap:12800\n运行 docker-compose 命令运行一下命令就可以启动 SkyWalking，存储后端是 Elasticsearch\ndocker-compose up -d...Container elasticsearch  StartedContainer elasticsearch  WaitingContainer elasticsearch  HealthyContainer oap  StartingContainer oap  StartedContainer oap  WaitingContainer oap  HealthyContainer ui  StartingContainer ui  Started\n然后打开 http://localhost:8090/\n这样就部署完成了，是不是非常简单？\n更换存储后端SkyWalking 官方推荐使用 Elasticsearch 作为存储后端，但是还支持其他的作为存储后端。具体可以参考：https://skyapm.github.io/document-cn-translation-of-skywalking/zh/8.0.0/setup/backend/backend-storage.html\n原生支持的存储\n\nH2\nElasticSearch 6, 7\nMySQL\nTiDB\nInfluxDB\n\n支持存储的重分发版本。\n\nElasticSearch 5\n\n各个存储后端的配置如下，我们如果要更换，只需要配置 docker-compose.yaml 的环境变量即可\nH2\nH2 是嵌入式数据库，一般测试使用，生产禁止使用\n\nstorage:  selector: ${SW_STORAGE:h2}  h2:    driver: org.h2.jdbcx.JdbcDataSource    url: jdbc:h2:mem:skywalking-oap-db    user: sa\n\nElasticSearchstorage:  selector: ${SW_STORAGE:elasticsearch}  elasticsearch:    # nameSpace: ${SW_NAMESPACE:\"\"}    user: ${SW_ES_USER:\"\"} # User needs to be set when Http Basic authentication is enabled    password: ${SW_ES_PASSWORD:\"\"} # Password to be set when Http Basic authentication is enabled    clusterNodes: ${SW_STORAGE_ES_CLUSTER_NODES:localhost:443}    trustStorePath: ${SW_SW_STORAGE_ES_SSL_JKS_PATH:\"../es_keystore.jks\"}    trustStorePass: ${SW_SW_STORAGE_ES_SSL_JKS_PASS:\"\"}    protocol: ${SW_STORAGE_ES_HTTP_PROTOCOL:\"https\"}    indexShardsNumber: ${SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2}    indexReplicasNumber: ${SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0}    # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html    bulkActions: ${SW_STORAGE_ES_BULK_ACTIONS:2000} # Execute the bulk every 2000 requests    bulkSize: ${SW_STORAGE_ES_BULK_SIZE:20} # flush the bulk every 20mb    flushInterval: ${SW_STORAGE_ES_FLUSH_INTERVAL:10} # flush the bulk every 10 seconds whatever the number of requests    concurrentRequests: ${SW_STORAGE_ES_CONCURRENT_REQUESTS:2} # the number of concurrent requests    advanced: ${SW_STORAGE_ES_ADVANCED:\"\"}\n\nMySQLstorage:  selector: ${SW_STORAGE:mysql}  mysql:    properties:      jdbcUrl: ${SW_JDBC_URL:\"jdbc:mysql://localhost:3306/swtest\"}      dataSource.user: ${SW_DATA_SOURCE_USER:root}      dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234}      dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true}      dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250}      dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048}      dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true}    metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000}\n\nTiDBstorage:  selector: ${SW_STORAGE:mysql}  mysql:    properties:      jdbcUrl: ${SW_JDBC_URL:\"jdbc:mysql://localhost:3306/swtest\"}      dataSource.user: ${SW_DATA_SOURCE_USER:root}      dataSource.password: ${SW_DATA_SOURCE_PASSWORD:root@1234}      dataSource.cachePrepStmts: ${SW_DATA_SOURCE_CACHE_PREP_STMTS:true}      dataSource.prepStmtCacheSize: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_SIZE:250}      dataSource.prepStmtCacheSqlLimit: ${SW_DATA_SOURCE_PREP_STMT_CACHE_SQL_LIMIT:2048}      dataSource.useServerPrepStmts: ${SW_DATA_SOURCE_USE_SERVER_PREP_STMTS:true}    metadataQueryMaxSize: ${SW_STORAGE_MYSQL_QUERY_MAX_SIZE:5000}\n\nInfluxDBstorage:  selector: ${SW_STORAGE:influxdb}  influxdb:    url: ${SW_STORAGE_INFLUXDB_URL:http://localhost:8086}    user: ${SW_STORAGE_INFLUXDB_USER:root}    password: ${SW_STORAGE_INFLUXDB_PASSWORD:}    database: ${SW_STORAGE_INFLUXDB_DATABASE:skywalking}    actions: ${SW_STORAGE_INFLUXDB_ACTIONS:1000} # the number of actions to collect    duration: ${SW_STORAGE_INFLUXDB_DURATION:1000} # the time to wait at most (milliseconds)    fetchTaskLogMaxSize: ${SW_STORAGE_INFLUXDB_FETCH_TASK_LOG_MAX_SIZE:5000} # the max number of fetch task log in a request\n\n以 InfluxDB 举例替换存储后端\nversion: '3.8'services:  influxdb:    image: bitnami/influxdb:1.8.5    container_name: influxdb-server    ports:      - \"8086:8086\"    environment:      - INFLUXDB_ADMIN_USER_TOKEN=FvSo2szLLZ88qJrk      - INFLUXDB_ADMIN_USER_PASSWORD=FvSo2szLLZ88qJrk      - INFLUXDB_USER=gcdd      - INFLUXDB_USER_PASSWORD=FvSo2szLLZ88qJrk      - INFLUXDB_DB=skywalking    volumes:      - \"./data:/bitnami/influxdb\"  oap:    image: apache/skywalking-oap-server:8.9.1    container_name: oap    links:      - influxdb    ports:      - \"11800:11800\"      - \"12800:12800\"    environment:      SW_STORAGE: influxdb      SW_STORAGE_INFLUXDB_URL: http://influxdb:8086      SW_STORAGE_INFLUXDB_USER: admin      SW_STORAGE_INFLUXDB_PASSWORD: FvSo2szLLZ88qJrk      SW_HEALTH_CHECKER: default      SW_TELEMETRY: prometheus      JAVA_OPTS: \"-Xms2048m -Xmx2048m\"  ui:    image: apache/skywalking-ui:8.9.1    container_name: ui    links:      - oap    ports:      - \"8090:8080\"    environment:      SW_OAP_ADDRESS: http://oap:12800\n\n初体验SkyWalking 的 UI 做的还是非常可以的，美观且实用。\n参考：APM-Skywalking UI 使用全攻略\n\n指标仪表盘服务指标点击仪表盘，选择要查询的应用，如 “is-file-store”, 再切换仪表盘为 “Service” 模式，即可查询对应服务的指标\n\n服务主要指标包括\nApdexScore： 性能指数，Apdex (Application Performance Index) 是一个国际通用标准，Apdex 是用户对应用性能满意度的量化值。它提供了一个统一的测量和报告用户体验的方法，把最终用户的体验和应用性能作为一个完整的指标进行统一度量，其中最高为 1 最低为 0；\nResponseTime：响应时间，即在选定时间内，服务所有请求的平均响应时间 (ms)；\nThroughput: 吞吐量，即在选定时间内，每分钟服务响应的请求量 (cpm)\nSLA: service level agreement，服务等级协议，SW 中特指每分钟内响应成功请求的占比。\n\n大盘中会列出以上指标的当前的平均值，和历史走势。\n服务慢端点 Service Slow Endpoint服务指标仪表盘会列举出当前服务响应时间最大的端点 Top5，如果有端点的响应时间过高，则需要进一步关注其指标（点击可以复制端点名称）。\n\n运行中的实例 Running ServiceInstance该服务目前所有实例的吞吐量情况，通过此可以推断出实例之间的负载情况。如果发现某个实例吞吐量较低，就需要查询实例指标（如查询该实例是不是发生了 GC，或则 CPU 利用率过高）\n\n端点指标如果发现有端点的响应时间过高，可以进一步查询该端点的指标信息。和服务指标类似，端点指标也包括吞吐量、SLA、响应时间等指标，这里不再赘述。\n端点仪表盘会有如下特有信息：\n\nDependency Map: 依赖关系图，代表哪些服务在依赖（调用）该端点，如果是前端直接调用，会显示为用户（User）依赖中；\nSlow Traces: 即慢调用请求记录，SW 会自动列出当前时间段内端点最慢的调用记录和 TraceID，通过这个 ID 可以在追踪功能找到具体的调用链信息，便于定位。\n\n\n服务实例指标选择服务的实例并切换仪表盘，即可查看服务某个实例的指标数据。除了常规的吞吐量、SLA、响应时间等指标外，实例信息中还会给出 JVM 的信息，如堆栈使用量，GC 耗时和次数等。\n\nDB 数据指标查询除了服务本身的指标，SW 也监控了服务依赖的 DB 指标。切换 DB 指标盘并选择对应 DB 实例，就可以看到从服务角度（client）来看该 DB 实例的吞吐量、SLA、响应时间等指标。\n更进一步，该 DB 执行慢 SQL 会被自动列出，可以直接粘贴出来，便于定位耗时原因。\n\n拓扑结构\n不同于仪表盘来展示单一服务的指标，拓扑图是来展示服务和服务之间的依赖关系。\n用户可以选择单一服务查询，也可以将多个服务设定为一组同时查询。\n点击服务图片会自动显示当前的服务指标；\nSW 会根据请求数据，自动探测出依赖的服务，DB 和中间件等。\n点击依赖线上的圆点，会显示服务之间的依赖情况，如每分钟吞吐量，平均延迟时间，和侦察端模式（client/Server）。\n\n\n请求追踪当用户发现服务的 SLA 降低，或者某个具体的端口响应时间上扬明显，可以使用追踪功能查询具体的请求记录。\n\n最上方为搜索区，用户可以指定搜索条件，如隶属于哪个服务、哪个实例、哪个端口，或者请求是成功还是失败；也可以根据上文提到的 TraceID 精确查询。\n整个调用链上每一个跨度的耗时和执行结果都会被列出（默认是列表，也可选择树形结构和表格的形式）；\n如果有步骤失败，该步骤会标记为红色。\n\n\n\n点击跨度，会显示跨度详情，如果有异常发生，异常的种类、信息和堆栈都会被自动捕获；\n\n\n\n如果跨度为数据库操作，执行的 SQL 也会被自动记录。\n\n\n性能剖析追踪功能展示出的跨度是服务调用粒度的，如果要看应用实时的堆栈信息，可以选择性能剖析功能。\n\n新建分析任务；\n选指定的服务和端点作为分析对象；\n设定采样频率和次数；\n\n\n注意：如果端点的响应时间小于监控间隔，可能会导致采样分析失败。\n\n\n新建任务后，SW 将开始采集应用的实时堆栈信息。采样结束后，用户点击分析即可查看具体的堆栈信息。\n\n点击跨度右侧的 “查看”，可以看到调用链的具体详情；\n跨度目录下方是 SW 收集到的具体进程堆栈信息和耗时情况。\n\n\n需要提醒的时候，性能剖析功能因为要实时高频率收集服务的 JVM 堆栈信息，对于服务本身有一定的性能消耗，只适用于耗时端点的行为分析。\n指标对比当用户需要对比不同端点指标的关联情况的话，可以使用性能对比功能。选择待对比的端点和指标，SW 将会列出相同时间段的指标记录。如下图中，两个端点虽然属于不同的应用，但是在响应时间的指标，表现出一定的关联性。实际上两个端点有依赖关系，一个响应时间变多，另一个也会变多。\n\n参考资料\nSkyWalking 极简入门\nSkyWalking 文档中文版（社区提供）\n\n","categories":["数据结构与算法入门"],"tags":["数据结构与算法"]},{"title":"SKyWalking 全链路追踪（二）Java 应用接入 SkyWalking Agent","url":"/p/17672/","content":"Java Agent 简介参考：Java Agent 介绍与使用\n\nJava Agent 是在 Java 1.5 版本之才有的东西，他可以构建一个独立 Java 服务外的一个代理程序，也就是 Agent。通常会用它来做一下 Java 服务的监控，或者替换其他 JVM 上的程序，还可以实现虚拟机上的 AOP 功能。\n\nSkyWalking 使用了 Java Agent 作为接入方式，实现了代码 0 入侵，非常方便。具体的 Java Agent 技术不在这过多赘述。\nSpringBoot 接入 SkyWalking下载 Java Agent官方地址是：https://skywalking.apache.org/downloads/，不过下载速度堪忧\n\n这里准备了一个 Java Agent 8.11 版本的阿里云盘链接，需要的可以从这里下载：https://www.aliyundrive.com/s/z8sst2mgPxC 提取码：BpcF\n安装 Java Agent把上面下载的好的文件解压，得到一个如下文件夹\n\n参考资料\n安装 Java Agent\n\n","categories":["数据结构与算法入门"],"tags":["数据结构与算法"]},{"title":"切换博客到七牛云","url":"/p/62528/","content":"很早之前就已经把博客的图床更换为了七牛云，但是博客本身还是托管在 Github 上面。最近，Github 的访问速度是在太慢了，所以准备把博客整体迁移到七牛云上面，加快下国内的访问速度。\n准备照着将 hexo 博客一键部署到七牛云这个应该能准备妥当，这里不再赘述。\n域名申请域名要玩七牛云，首先得有个域名（必须已备案），申请好域名，然后进行下一步\n新建七牛云空间空间管理 -&gt; 新建空间。\n存储空间名称随便填，不重复就行。\n访问控制选择公开，毕竟我们是做博客，总要让人访问吧。\n\n开启默认首页设置\n绑定域名\n再次强调：七牛云要求域名必须是备案域名\n\n然后空间管理，点击域名，填入域名，然后选择证书，然后照着提示一步步配置就行，没什么坑。\n\n绑定完成后如下图所示\n\n这边注意下，七牛云的 CDN 加速是收费的，但是不贵，之前做图床的时候，半年才花了 5 毛钱。\n不知道用来做博客，费用多不多\n\n\nHexo 打包一键发布到七牛云\n一键发布，用到了七牛云的一个官方工具，可以用于批量上传文件至七牛云空间。\n\n下载 qshellhttps://github.com/qiniu/qshell，进入 release 找个最新版本下载就行\n\n下载完成之后，我们解压到 hexo 目录，并改名为 qshell\n登录 qshellqshell account &lt;Your AccessKey&gt; &lt;Your SecretKey&gt; &lt;Your Name&gt;\n\nAccessKey 和 SecretKey 在这里可以找到 https://portal.qiniu.com/user/key\n\n配置 upload.conf在博客目录下面，新建文件 upload.conf，然后配置信息\n{  // 这个地址是根目录地址，不可使用相对路径  \"src_dir\": \"D:/WorkSpace/Personal/blog-source/public\",  // 储存空间名称  \"bucket\": \"gcdd-hexo\",  // 是否覆盖  \"overwrite\" : true,  // 检查新增文件  \"rescan_local\" : true}\n\n执行命令\nqshell qupload upload.confWriting upload log to file C:\\Users\\13983\\.qshell\\qupload\\2496be155bc149325a6994afc3b76d8f\\2496be155bc149325a6994afc3b76d8f.logUploading D:\\WorkSpace\\Personal\\blog-source\\public\\404.html =&gt; 404.html [1/245, 0.4%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\CNAME =&gt; CNAME [2/245, 0.8%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\01\\index.html =&gt; archives/2019/01/index.html [3/245, 1.2%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\01\\page\\2\\index.html =&gt; archives/2019/01/page/2/index.html [4/245, 1.6%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\03\\index.html =&gt; archives/2019/03/index.html [5/245, 2.0%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\04\\index.html =&gt; archives/2019/04/index.html [6/245, 2.4%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\05\\index.html =&gt; archives/2019/05/index.html [7/245, 2.9%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\06\\index.html =&gt; archives/2019/06/index.html [8/245, 3.3%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\archives\\2019\\07\\index.html =&gt; archives/2019/07/index.html [9/245, 3.7%] ...Uploading D:\\WorkSpace\\Personal\\blog-source\\public\\tags\\默认\\index.html =&gt; tags/默认/index.html [245/245, 100.0%] ...See upload log at path C:\\Users\\13983\\.qshell\\qupload\\2496be155bc149325a6994afc3b76d8f\\2496be155bc149325a6994afc3b76d8f.log\n\n等进度条转完，也就部署完毕了，速度很快。\n访问下试试：https://blog.gcdd.top/，嗯，速度非常快，可以当成主力博客来使用了。\n一键打包上传自己写了个脚本，用用用用\n#!/usr/bin/env bash# 发布hexo clean &amp;&amp;  # 清理旧的网站  hexo generate &amp;&amp; gulp &amp;&amp; # 生成新的页面  hexo d &amp;&amp;    # 生成新的静态网站  hexo algolia # 生成搜索索引qshell qupload upload.conf # 上传至七牛云\n\n参考\n将 hexo 博客一键部署到七牛云\n\n","categories":["数据结构与算法入门"],"tags":["数据结构与算法"]},{"title":"Kotlin 从入门到精通 | 第七章 类型进阶","url":"/p/23035/","content":"本章节主要介绍 Kotlin 类型一些不为人知的秘密\n\n\n构造器构造器的基本写法class Person(    var age: Int, // 类内全局可见    name: String // 构造器内可见（init块，属性初始化）)\n\ninit 块\ninit 块可以有多个\n\nclass Person(var age: Int, name: String) {    var name: String    init {        this.name = name    }    val firstName = name.split(\" \")[0]    init {        // ...    }}\n\n属性必须被初始化\n继承父类abstract class Animalclass Person(var age: Int, var name: String) : Animal() // 调用父类构造器\n\n副构造器class Person(var age: Int, var name: String) : Animal() {    constructor(age: Int) : this(age, \"unknown\") // 副构造器，调用主构造器，确保构造路径唯一性}\n\n不定义主构造器（不推荐）\n主构造器默认参数\n可以为主构造器定义默认参数，使用 @JvmOverloads 可以在 Java 代码中以重载的形式调用\n\n\n使用同名函数作为工厂函数val persons = HashMap&lt;String, Person&gt;()fun Person(name: String): Person {    return persons[name]        ?: Person(1, name).also { persons[name] = it }}\n\n可见性类的可见性\n\n\n可见性类型\n Java\nKotlin\n\n\n\npublic\n 公开\n与 Java 相同，默认\n\n\n internal\n❌\n模块内可见\n\n\n default\n 包内可见，默认\n❌\n\n\nprotected\n 包内及子类可见\n类内及子类可见\n\n\n private\n 类内可见\n类或文件内可见\n\n\n修饰对象\n\n\n可见性类型\n顶级声明\n类\n成员\n\n\n\n public\n✔️\n✔️\n✔️\n\n\ninternal\n✔️，模块\n✔️，模块\n✔️，模块\n\n\n protected\n❌\n❌\n✔️\n\n\nprivate\n✔️，文件\n✔️，文件\n✔️，类\n\n\n模块直观的讲，大致可以认为是一个 Jar 包、一个 aar\ninternal VS default\n一般由 SDK 或公共组件开发者用于隐藏模块内部细节实现\ndefault 可通过外部创建相同包名来访问，访问控制非常弱\ndefault 会导致不同抽象层次的类聚集到相同包之下\ninternal 可方便处理内外隔离，提升模块内聚减少接口暴露\ninternal 修饰的 kotlin 类或成员在 Java 当中可直接访问\n\n类的可见性class Personprivate constructor(var age: Int, var name: String) // 构造器私有化\n\n属性的可见性class Person(var age: Int, var name: String) {    private var firstName: String = \"\" // 私有化属性 firstName，外部无法访问    var secondName: String = \"\"        private set // 私有化属性secondName的setter，外部只能读取    \tprivate get // 编译器报错，getter的可见性必须与属性可见性一致    \tpublic set // 编译器报错，setter的可见性不得大于属性的可见性}\n\n顶级声明的可见性\n顶级声明指文件内直接定义的属性、函数、类等\n顶级声明不支持 protected\n顶级声明被 private 修饰标识文件内部可见\n\n密封类（sealed）\n密封类是一种特殊的抽象类\n密封类的子类定义在与自身相同的文件中\n密封类的子类个数是有限的\n\n\n密封类的子类sealed class PlayerStateobject Idle : PlayerState()class Playing(val song: Song) : PlayerState() {    fun start() {}    fun stop() {}}class Error(val errorInfo: ErrorInfo) : PlayerState() {    fun recover() {}}\n\n子类分支\n子类可数，分支完备，所以不需要 else 分支\n\nthis.state = when (val state = this.state) {            Idle -&gt; {                Playing(song).also(Playing::start)            }            is Playing -&gt; {                state.stop()                Playing(song).also(Playing::start)            }            is Error -&gt; {                state.recover()                Playing(song).also(Playing::start)            }}\n\n密封类 VS 枚举类\n\n\n\n密封类\n枚举类\n\n\n\n状态实现\n子类继承\n类实例化\n\n\n状态可数\n子类可数\n实例可数\n\n\n状态差异\n类型差异\n值差异\n\n\n内联类（inline）\n内联类是对某一个类型的包装\n内联类是类似于 Java 装箱类型的一种类型\n编译器会尽可能使用被包装的类型进行优化\n内联类在 1.3 中处于公测阶段，谨慎使用\n\n\n内联类可以实现接口，但不能继承父类，也不能被继承\n内联类的限制\n主构造器必须有且只有一个只读属性\n不能定义有 backing-field 的其他属性\n被包装类型必须不能是泛型类型\n不能继承父类也不能被继承\n内联类不能定义为其他类的内部类\n\n内联类 VS 类型别名\n\n\n\ntypealias\ninline class\n\n\n\n类型\n没有新类型\n有包装类型产生\n\n\n实例\n与原类型一致\n必要时使用包装类型\n\n\n场景\n类型更直观\n优化包装类型性能\n\n\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Kotlin 从入门到精通 | 第三章 Kotlin 内置类型","url":"/p/16155/","content":"本章节主要介绍 Kotlin 的内置类型和简单用法\n变量的声明val b: String = \"Hello Kotlin\"\nKotlin 的变量声明方式，有点类似于 TypeScript，是比较现代的一种做法，一般形式为修饰符 变量名: 类型 = 值，其中，类型声明可以省略。\n修饰符有两种\n\nval：只读变量\n var：可读写变量，定义时必须指定值，且不可更改\n\n与 Java 对比int a = 2;final String b = \"Hello Java\";\n\nvar a = 2val b = \"Hello Kotlin\"\n\n易混淆的 Long 类型标记在 Java 里，Long 类型结束使用 l 是可以编译通过，只是不推荐（小写的 l，看起来就跟数字 1 一样）\nlong c = 1234567890l; // ok but not good.long d = 1234567890L; // ok\n\n在 Kotlin 里面，直接就编译不通过，强制要求修改为 L\nval c = 1234567890l // compile error.val d = 1234567890L // ok\n\nKotlin 数值类型转换在 Java 里，int 类型可以隐式转换为 long\nint e = 10;long f = e; // implicit conversion\n\n而到了 Kotlin 这，对不起，不支持\nval e: Int = 10val f: Long = e // implicitness not allowedval f: Long = e.toLong() // ok\n\n无符号类型（兼容 C Native）从 v1.3 开始，Kotlin 支持无符号类型\n字符串定义先看下 Kotlin 定义 HTML 字符串的代码\nval n = \"\"\"    &lt;!doctype html&gt;    &lt;html&gt;    &lt;head&gt;        &lt;meta charset=\"UTF-8\"/&gt;        &lt;title&gt;Hello World&lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;div id=\"container\"&gt;            &lt;H1&gt;Hello World&lt;/H1&gt;            &lt;p&gt;This is a demo page.&lt;/p&gt;        &lt;/div&gt;    &lt;/body&gt;    &lt;/html&gt;    \"\"\".trimIndent()println(n)\n对比 Java，简直太简洁了，使用 Java 定义的一段同样行为的代码，一堆换行符，看了都头大\nKotlin 字符串字符串比较\na == b：比较内容，等价于 Java 的 equals\na === b：比较对象是否是同一个对象\n\n字符串模板\n“hello, $name” =&gt; “Hello, 小明”\n\n数组\n\n\n\nKotlin\nJava\n\n\n\n 整型\n IntArray\nint[]\n\n\n 整型装箱\n Array\nInteger[]\n\n\n 字符\n CharArray\nchar[]\n\n\n 字符装箱\n Array\nCharacter[]\n\n\n 字符串\n Array\nString[]\n\n\n数组的创建int[] c = new int[]{1, 2, 3, 4, 5};\n在 Kotlin 里面，数组使用以下方式创建\nval c0 = intArrayOf(1, 2, 3, 4, 5)val c1 = IntArray(5){ it + 1 }\n\n\n数组的长度Java\nint[] a = new int[5];System.out.println(a.length); // only array uses 'length'\n\nKotlin\nval a = IntArray(5)println(a.size) // same with the Collections(e.g. List)\n\n数组的读写Java\nString[] d = new String[]{\"Hello\", \"World\"};d[1] = \"Java\";System.out.println(d[0] + \", \" + d[1]);\n\nKotlin\nval d = arrayOf(\"Hello\", \"World\")d[1] = \"Kotlin\"println(\"${d[0]}, ${d[1]}\")\n\n数组的遍历Java\nfloat[] e = new float[]{1, 3, 5, 7};for(float element : e) {  System.out.println(element);}\n\nKotlin，有点像 Python 里面的元素遍历了\nval e = floatArrayOf(1f, 3f, 5f, 7f)for (element in e) {  println(element)}\n或者还可以使用 forEach 高阶函数\ne.forEach { element -&gt; println(element) }\n\n数组的包含关系Java\nfor(float element : e) {  if(element == 1f) {    System.out.println(\"1f exists in variable 'e'\");    break;  }}\n而在 Kotlin 里面，简单的不行\nif(1f in e) {  println(\"1f exists in variable 'e'\")}\n\n区间这个 Java 里是没有，所以只看 Kotlin 的写法\n区间的创建闭区间（..）val intRange = 1..10 // [1,10]val charRange = 'a'..'z'val longRange = 1L..100L\n\n开闭区间，前闭后开（until）val intRangeExclusive = 1 until 10 // [1,10)val charRangeExclusive = 'a' until 'z'val longRangeExclusive = 1L until 100L\n\n倒序区间（downTo）val intRangeReverse = 10 downTo 1 // [10,9,...,1]val charRangeReverse = 'z' downTo 'a'val longRangeReverse = 100L downTo 1L\n\n区间的步长（step）\n在定义区间时，我们还可以定义步长，默认步长为 1\n\nval intRangeWithStep = 1..10 step 2 // [1, 3, 5, 7, 9]val charRange = 'a'..'z' step 2val longRange = 1L..100L step 5\n\n区间的迭代\n区间的迭代跟数组基本是类似的\n\nfor(element in intRange) {  println(element)}intRange.forEach{ println(it) } // 高阶函数默认的参数叫做it\n\n区间的包含关系if(3 in intRange) {  println(\"3 in range 'intRange'\")}if(12 !in intRange) {  println(\"12 not in range 'intRange'\")}\n\n区间的应用val array = intArrayOf(1, 3, 5, 7)for(i in array.indices) {  println(array[i])}\n其中，array.indices 返回的就是数组索引范围的区间\n集合框架Kotlin 在 Java 集合的基础上，做出了一些增强，具体表现为以下几点\n\n增加了 “不可变” 集合框架的接口\n没有另起炉灶，复用 Java Api 的所有实现类型\n提供了丰富医用的方法，例如 forEach/map/flatMap 等\n\n\nScala 也是一门 JVM 语言，Kotlin 很多特性都参考了 Scala\n\n集合框架的接口类型对比\n\n\n\nKotlin\nJava\n\n\n\n 不可变 List\nList\nList\n\n\n 可变 List\nMutableList\nList\n\n\n 不可变 Map\nMap&lt;K, V&gt;\nMap&lt;K, V&gt;\n\n\n 可变 Map\nMutableMap&lt;K, V&gt;\nMap&lt;K, V&gt;\n\n\n 不可变 Set\nSet\nSet\n\n\n 可变 Set\nMutableSet\nSet\n\n\n集合框架的创建Java\nList&lt;Integer&gt; intList = new ArrayList&lt;&gt;(Arrays.asList(1, 2, 3));\n\nKotlin\nval intList: List&lt;Int&gt; = listOf(1, 2, 3) // 不能添加或者删除元素val intList2: MutableList&lt;Int&gt; = mutableListOf(1, 2, 3) // 可以添加或者删除元素val map: Map&lt;String, Any&gt; = mapOf(\"name\" to \"benny\", \"age\" to 20)val map2: Map&lt;String, Any&gt; = mutableMapOf(\"name\" to \"benny\", \"age\" to 20) // 其中的 \"name\" to \"benny\" 是一个中缀表达式\n\n集合实现类复用与类型别名我们来比较一下 Java 与 Kotlin 创建集合的代码Java\nList&lt;String&gt; stringList = new ArrayList&lt;&gt;(); // java.util.ArrayList\n\nKotlin\nval stringList = ArrayList&lt;String&gt;() // kotlin.collections.ArrayList\n\nKotlin 里面集合的类型别名定义\n@SinceKotlin(\"1.1\") public actual typealias ArrayList&lt;E&gt; = java.util.ArrayList&lt;E&gt;@SinceKotlin(\"1.1\") public actual typealias LinkedHashMap&lt;K, V&gt; = java.util.LinkedHashMap&lt;K, V&gt;@SinceKotlin(\"1.1\") public actual typealias HashMap&lt;K, V&gt; = java.util.HashMap&lt;K, V&gt;@SinceKotlin(\"1.1\") public actual typealias LinkedHashSet&lt;E&gt; = java.util.LinkedHashSet&lt;E&gt;@SinceKotlin(\"1.1\") public actual typealias HashSet&lt;E&gt; = java.util.HashSet&lt;E&gt;\n\n\nKotlin 使用类型别名，是出于跨平台的考虑，同一份代码，Kotlin 不只是希望能跑在 JVM 平台，也可以是 Native 平台，所以，说不定有朝一日，Kotlin 编译出来的不再是 Java 字节码，而是二进制机器码！\n\n集合框架的读写Kotlin 还支持运算符重载\n添加元素Java\nfor(int i = 0; i &lt; 10; i++) {  stringList.add(\"num: \" + i);}\n\nKotlin\nfor(i in 0..10) {  stringList += \"num: $i\"}\n\n删除元素Java\nfor(int i = 0; i &lt; 10; i++) {  stringList.remove(\"num: \" + i);}\n\nKotlin\nfor(i in 0..10) {  stringList -= \"num: $i\"}\n\n修改元素Java\nstringList.set(5, \"HelloWorld\");String valueAt5 = stringList.get(5);\n\nKotlin\nstringList[5] = \"HelloWorld\"val valueAt5 = stringList[5]\n\n如果是 MapJava\nHashMap&lt;String, Integer&gt; map = new HashMap&lt;&gt;();map.put(\"Hello\", 10);System.out.println(map.get(\"Hello\"));\n\nKotlin\nval map = HashMap&lt;String, Int&gt;()map[\"Hello\"] = 10println(map[\"Hello\"])\n\n几个重要的数据结构Pair可以理解为键值对，包含 first 和 second 两个字段的数据结构\nval pair = \"Hello\" to \"Kotlin\"val pair2 = Pair(\"Hello\", \"Kotlin\") // 两种创建方式val first = pair.first // 获取对应元素val second = pair.secondval (x, y) = pair // 解构表达式\n\nTriple跟 Pair 类似，不过含有三个值\nval triple = Triple(\"x\", 2, 3.0)val first = triple.first // 获取对应元素val second = triple.secondval third = triple.thirdval (x, y, z) = triple // 解构表达式\n\n函数\n在 Kotlin 里面，函数也是类型的一种，是一等公民，可以赋值、传递，并在合适的条件下调用\n\n学习路线图\n函数的定义一个函数包含：函数名，函数参数列表，函数返回值，函数体\nfun main(args: Array&lt;String&gt;):Unit {  println(args.contentToString())}\n\n函数 VS 方法\n\n方法可以认为是函数的一种特殊类型\n从形式上，有 receiver 的函数即为方法\n\n函数的类型\n在 Kotlin 里，函数也是有类型的\n\nfun foo() { } // () -&gt; Unitfun foo(p(): Int): String { ... } // (Int) -&gt; Stringclass Foo {  fun bar(p0: String, p1: Long): Any { ... } // Foo.(String, Long) -&gt; Any，其中 Foo就是bar方法的receiver，类型等同于 (Foo, String, Long) -&gt; Any}\n\n函数的引用\n函数的引用类似于 C 语言中的函数指针，可用于函数传递\n\nfun foo() { } // val f: () -&gt; Unit = ::foofun foo(p(): Int): String { ... } // val g: (Int) -&gt; String = ::fooclass Foo {  fun bar(p0: String, p1: Long): Any { ... } // val h: (Foo, String, Long) -&gt; Any = Foo::bar}\n\n绑定 receiver 的函数引用\nval foo = Foo()val m: (String, Long) -&gt; Any = foo::bar // 绑定receiver的函数引用，其中foo是对象实例f(r, x, y) = r * (x + y)// 令：r = 2m(x, y) = f(2, x, y) = 2 * (x + y)\n\n变长参数 (vararg 关键字)fun main(vararg args: String) {  println(args.contentToString())}\n\n多返回值\nPair 或 Triple 定义返回值，使用结构获取返回值\n\nfun multiReturnValues(): Triple&lt;Int, Long, Double&gt; {  return Triple(1, 3L, 4.0)}val (a, b, c) = multiReturnValues() // 解构获取返回值\n\n默认参数\nKotlin 允许为参数指定默认值，这样一来，就不必像 Java 一样定义方法重载了\n\nfun defaultParameter(x: Int, y: String, z: Long = 0L) {  TODO()}defaultParameter(5, \"Hello\") // 这里的z使用默认的0L\n\n具名函数fun defaultParameter(x: Int = 5, y: String, z: Long = 0L) { // 不是最后的参数  TODO()}defaultParameter(y = \"Hello\") // 只传递y参数，其余使用默认值\n\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Kotlin 从入门到精通 | 第二章 开发环境搭建","url":"/p/55865/","content":"本章节主要介绍 Kotlin 的安装和常用命令\nKotlin 编译器安装方式直接下载安装GitHub - JetBrains/kotlin: The Kotlin Programming Language.\n1、例如在 Windows 平台，可以选择下载 kotlin-compiler-1.5.31.zip\n2、下载完毕之后，解压到合适的位置，例如 D:\\DevTools\\Kotlin\n3、配置环境变量把 kotlinc\\bin 添加到环境变量 Path 中，如图\n4、确认配置完成\n$ kotlinc -versioninfo: kotlinc-jvm 1.5.31 (JRE 11.0.11+9)\n\n使用包管理器\nLinux SDKMAN\nMac OSX: Homebrew\nWindows: Scoop\n\n常用命令\nkotlin：运行 kotlin 脚本 / REPL\n\n$ kotlinWelcome to Kotlin version 1.5.31 (JRE 11.0.11+9)Type :help for help, :quit for quit&gt;&gt;&gt; 1 + 1res0: kotlin.Int = 2&gt;&gt;&gt; :helpAvailable commands::help                   show this help:quit                   exit the interpreter:dump bytecode          dump classes to terminal:load &lt;file&gt;            load script from specified file\n\n\nkotlinc：编译 kotlin 源码 \n\n$ kotlinc 1.kt\n\n使用 IDE推荐使用 IDEA，开箱即用，无需配置，因为 Jetbrains 和 Kotlin 本来就是一家嘛。不推荐使用 Eclipse 开发 Kotlin 程序，所以根本没研究怎么配置 Eclipse 下的 Kotlin 开发环境\n使用 Gradle 构建工具Gradle 是一个灵活高效且支持多语言多平台的构建工具，是 Kotlin 生态下推荐的构建工具，Maven 好像没法作为 Kotlin 的构建工具？没去仔细研究，但是，既然已经用了 Kotlin，为什么还要去使用 Maven 呢？赶紧体验强大现代的 Gradle 吧！\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Kotlin 从入门到精通 | 第六章 函数进阶","url":"/p/31745/","content":"本章节主要对第四章描述的函数类型进行更进一步的描述，讲解 Kotlin 高阶函数、内联函数等细节。\n\n高阶函数\n参数类型包含函数类型或返回值类型为函数类型的函数为高阶函数\n\nfun needsFunction(block: () -&gt; Unit) {}fun returnsFunction(): () -&gt; Long {  return {    System.currentTimeMillis()  }}\n\n以 intArray 扩展函数示例\n// 不带返回值，参数类型为函数inline fun IntArray.forEach(action: (Int) -&gt; Unit): Unit {    for (element in this) action(element)}// 返回值类型为函数inline fun &lt;R&gt; IntArray.map(transform: (Int) -&gt; R): List&lt;R&gt; {  return mapTo(ArrayList&lt;R&gt;(size), transform) // 这里进一步传递transform函数}\n\n高阶函数的调用intArray.forEach{  println(\"Hello $it\") // 只有一个Lambda表达式作为参数，可省略小括号}\n\n内联函数（减少函数调用开销）\n上面的 IntArray 的示例，使用了 inline 关键字，这是内联函数的定义，内联函数会在编译器将函数定义搬运到调用处，而不是调用此函数\n\n\n高阶函数内联\n高阶函数与内联更配\n\n\n函数本身被内联到调用处\n函数的函数参数被内联到调用处 \n\nval start = System.currentTimeMillis()println(\"Hello\")println(System.currentTimeMillis() - start)\n\n内联高阶函数的返回值（return@高阶函数名称）val ints = intArrayOf(1, 2, 3, 4)ints.forEach{  if(it == 3) return@forEach  println(\"Hello $it\")}\n\n代码执行逻辑等同于\nval ints = intArrayOf(1, 2, 3, 4)for(element in ints) {  if(it == 3) continue  println(\"Hello $it\")}\n\nnon-local returninline fun nonLocalReturn(block: () -&gt; Unit) {  block()}fun main() {  nonLocalReturn {    return // 从main函数返回  }}\n如果在高阶函数内部，直接跳出到 main 函数，显然是不对的，例如\ninline fun Runnable(block: () -&gt; Unit): Runnable {  return object: Runnable {    override fun run() {      block() // 有可能存在不合法的`non-local return`，因为block的调用处与定义处不在同一个调用上下文    }  }}\n\ncrossinline：禁止 non-local return可以使用 crossinline 禁止 non-local return\ninline fun Runnable(crossinline /*禁止non-local return*/ block: () -&gt; Unit): Runnable {  return object: Runnable {    override fun run() {      block()    }  }}\n\nnoinline：禁止函数参数被内联inline fun Runnable(noinline /*禁止函数参数被内联*/ block: () -&gt; Unit): Runnable {  return object: Runnable {    override fun run() {      block()    }  }}\n\n内联属性\n没有 backing-field 的属性的 getter/setter 可以被内联\n\nvar pocket: Double = 0.0var money: Double  inline get() = pocket  inline set(value) {    pocket = value  }\n\n内联函数的限制\npublic/protected 的内联方法只能访问对应类的 public 成员\n内联函数的内联函数参数不能被存储（赋值给变量）\n内联函数的内联函数参数只能传递给其他内联函数参数\n\n几个有用的高阶函数\n\n\n函数名\n介绍\n推荐指数\n\n\n\n let\nval r = X.let { x -&gt; R }\n⭐⭐⭐\n\n\nrun\nval r = X.run { this: X -&gt; R }\n⭐\n\n\nalso\nval x = X.also { x -&gt; Unit }\n⭐⭐⭐\n\n\napply\nval r = X.apply { this: X -&gt; Unit }\n⭐\n\n\nuse\nval r = Closeable.use{ c -&gt; R }\n⭐⭐⭐\n\n\n这里的推荐指数，是教程作者根据是否绑定 receiver 做判断的，实际上这几个函数是有各自的意义的，参考下图，明确每个函数的使用场景\n\n集合变换与序列filter 变换Java\nlist.stream().filter(e -&gt; e % 2 == 0);\n\nKotlin\nlist.filter{ it % 2 == 0 }\n\n转换为懒序列，即只有执行到该元素时，才会执行 filter{ xxx } 的 xxx 函数Java\nlist.stream().filter(e -&gt; e % 2 == 0);\n\nKotlin\nlist.asSequence() // 转换为懒序列  .filter{ it % 2 == 0 }\n\nmap 变换Java\nlist.stream().map(e -&gt; e * 2 + 1);\n\nKotlin\nlist.map{ it * 2 + 1 }\n\nflatMap 变换\n实际上是 map 与 flatten（展平）结合起来\n\n\n集合的聚合操作\n\n\n函数名\n说明\n\n\n\n sum\n 所有元素求和\n\n\n reduce\n 将元素一次按规则聚合，结果与元素类型一致\n\n\n fold\n 给定初始化值，将元素按规则聚合，结果与初始化值类型一致\n\n\nSAM\n仅具有一种抽象方法的接口被称为功能接口，并且也被称为单一抽象方法接口（SAM 接口）。一个抽象方法意味着允许使用默认方法或默认实现的抽象方法。\n\nJava 的 SAM 转换\n一个参数类型为只有一个方法的接口的方法调用时可用 Lambda 表达式做转换作为参数\n\n() -&gt; System.out.println(\"run in executor.\")// SAM转换new Runnable() {  @Override  public void run() {    System.out.println(\"run in executor.\");  }}\n\nKotlin 的 SAM 转换executor.submit { println(\"run in executor.\") }// SAM转换executor.submit(object: Runnable {  override fun run() {    println(\"run in executor.\")  }})\n\nDSL 领域特定语言\n建议查看示例：AdvancedFunctions-Htmls.kt\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Kotlin 从入门到精通 | 第五章 表达式","url":"/p/49764/","content":"本章节主要介绍 Kotlin 自带的一些表达式和简单使用\n常量和变量变量Java\nint a = 2;a = 3;\n\nKotlin\nvar a = 2a = 3\n\n只读变量Java\nfinal int b = 3;\n\nKotlin\nval b = 3\n\n常量Java\nstatic final int b = 3;\n\nKotlin\n\n只能定义在全局范围\n只能修饰基本类型\n必须立即用字面量初始化 \n\nconst val b = 3\n\n编译期和运行时常量const val b = 3 // 编译期即可确定常量的值，并用值替换调用处val c: Int运行时才能确定值，调用处通过引用获取值\n\n表达式if … elseJava\n\nJava 支持三目运算符，所以可以使用以下语法\n\nc = a == 3 ? 4 : 5;\n\nKotlin\n\nKotlin 中，if ... else ... 也属于表达式，所以 Kotlin 开发者没有提供三目运算符，直接使用 if ... else ... 可以达到相同的效果\n\nc = if(a == 3) 4 else 5\n\nwhen …Java\nswitch(a) {  case 0: c = 5; break;  case 1: c = 100; break;  default: c = 20;}\n\nKotlin\nwhen(a) {  0 -&gt; c = 5  1 -&gt; c = 100  else -&gt; c = 20}\n\n在 Java 里，switch 表达式只支持 byte、short、int、char、String或者枚举，但是 Kotlin 中，相当于 Scala 的模式匹配，支持的语句相当丰富，比如下面的也是可以的\nwhen(person) {  is Male -&gt; println(\"${person.name} 是男人\")  is FeMale -&gt; println(\"${person.name} 是女人\")  else -&gt; println(\"${person.name} emm...\")}\n\n条件可以转移到分支\nvar x: Any = ...when {  x is String -&gt; c = x.length  x == 1 -&gt; c = 100  else -&gt; c = 20}\n\n由于 Kotlin 里面，when 是一个表达式，所以允许有值返回，上述也可以写成下面这样\nc = when {  x is String -&gt; x.length  x == 1 -&gt; 100  else -&gt; 20}\n\ntry … catch …Java\ntry {  c = a / b;} catch(Exception e) {  e.printStackTrace();  c = 0;}\n\nKotlin\nc = try {  a / b} catch(e: Exception) {  e.printStackTrace()  0}\n\n运算符与中缀表达式运算符\nKotlin 支持运算符重载\n运算符的范围仅限官方指定的符号\n\n一元操作一元前缀操作符\n\n\n表达式\n翻译为\n\n\n\n +a\na.unaryPlus()\n\n\n-a\na.unaryMinus()\n\n\n!a\na.not()\n\n\n当编译器处理例如表达式 +a 时，它执行以下步骤：\n\n确定 a 的类型，令其为 T；\n为接收者 T 查找一个带有 operator 修饰符的无参函数 unaryPlus()，即成员函数或扩展函数；\n如果函数不存在或不明确，则导致编译错误；\n如果函数存在且其返回类型为 R，那就表达式 +a 具有类型 R；\n\n\n注意：这些操作以及所有其他操作都针对基本类型做了优化，不会为它们引入函数调用的开销。\n\n以下是如何重载一元减运算符的示例\ndata class Point(val x: Int, val y: Int)operator fun Point.unaryMinus() = Point(-x, -y)val point = Point(10, 20)fun main() {   println(-point)  // 输出“Point(x=-10, y=-20)”}\n\n递增与递减\n\n\n表达式\n翻译为\n\n\n\n a++\na.inc()\n\n\na–\na.dec()\n\n\n二元操作算术运算符\n\n\n表达式\n翻译为\n\n\n\n a + b\na.plus(b)\n\n\na - b\na.minus(b)\n\n\na * b\na.times(b)\n\n\na / b\na.div(b)\n\n\na % b\na.rem (b) 或者 a.mod (b) 已弃用\n\n\n a..b\na.rangeTo(b)\n\n\n下面是一个从给定值起始的 Counter 类的示例，它可以使用重载的 + 运算符来增加计数\ndata class Counter(val dayIndex: Int) {    operator fun plus(increment: Int): Counter {        return Counter(dayIndex + increment)    }}\n\nIn 操作符\n\n\n表达式\n翻译为\n\n\n\n a in b\nb.contains(a)\n\n\na !in b\n!b.contains(a)\n\n\n索引访问操作符\n\n\n表达式\n翻译为\n\n\n\n a[i]\na.get(i)\n\n\na[i, j]\na.get(i, j)\n\n\na[i_1, ……, i_n]\na.get(i_1, ……, i_n)\n\n\na[i] = b\na.set(i, b)\n\n\na[i, j] = b\na.set(i, j, b)\n\n\na[i_1, ……, i_n] = b\na.set(i_1, ……, i_n, b)\n\n\n调用操作符\n\n\n表达式\n翻译为\n\n\n\n a()\na.invoke()\n\n\na(i)\na.invoke(i)\n\n\na(i, j)\na.invoke(i, j)\n\n\na(i_1, ……, i_n)\na.invoke(i_1, ……, i_n)\n\n\n广义赋值\n\n\n表达式\n翻译为\n\n\n\n a += b\na.plusAssign(b)\n\n\na -= b\na.minusAssign(b)\n\n\na *= b\na.timesAssign(b)\n\n\na /= b\na.divAssign(b)\n\n\na %= b\na.remAssign (b), a.modAssign (b)（已弃用）\n\n\n相等与不等操作符\n\n\n表达式\n翻译为\n\n\n\n a == b\na?.equals(b) ?: (b === null)\n\n\na != b\n!(a?.equals(b) ?: (b === null))\n\n\n比较操作符\n所有的比较都转换为对 compareTo 的调用，这个函数需要返回 Int 值\n\n\n\n\n表达式\n翻译为\n\n\n\n a &gt; b\na.compareTo(b) &gt; 0\n\n\na &lt; b\na.compareTo(b) &lt; 0\n\n\na &gt;= b\na.compareTo(b) &gt;= 0\n\n\na &lt;= b\na.compareTo(b) &lt;= 0\n\n\n属性委托操作符中缀表达式（infix）前面章节提到过，初始化 Map 时，可以使用 mapOf(1 to 2)，这里的 x to y 就是中缀表达式，这个函数表现为\ninfix fun &lt;A, B&gt; A.to(that: B): Pair&lt;A, B&gt; = Pair(this, thar) // 加infix关键字\n\nLambda 表达式\n匿名函数的类型，类型与普通函数一致\n\nval func: () -&gt; Unit = fun() {  println(\"Hello\")}\n\nLambda 表达式的定义无参数无返回值Java(since 1.8)\nRunnable lambda = () -&gt; {  System.out.println(\"Hello\");}\n\nKotlin\nval lambda = {  println(\"Hello\")}\n\n有参数无返回值Java(since 1.8)\ninterface Function1 {  void invoke(int p);}Function1 f1 = (p) -&gt; {  System.out.println(p);}\n\nKotlin\nval f1: (Int) -&gt; Unit = { p: Int -&gt; println(p) }\n\n还可以简写为\nval f1: (Int) -&gt; Unit = { println(it) } // 默认的匿名函数参数名称为`it`\n\n\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Kotlin 从入门到精通 | 第四章 类型初步","url":"/p/58149/","content":"本章节主要介绍 Kotlin 的类型定义和简单使用\n类的定义Kotlin 类默认为 public，类内无内容可省略\nKotlin 类成员变量，方法同 Java 类似\nKotlin 类默认带无参构造器，如果需要定义其他构造器，使用 constructor 关键字创建\n也可以直接定义到类上\n类的实例化\n直观感受是，省略了 new 关键字，获得对象再也不需要 new 了\n\nJava\nSimpleClass simpleClass = new SimpleClass(9);System.out.println(simpleClass.x);simpleClass.y();\n\nKotlin\nval simpleClass = SimpleClass(9)println(simpleClass.x)simpleClass.y()\n\n接口的定义\n基本和 Java 一致\n\n\n接口的实现\nimplements 关键字换成了:\n@override 注解换成了 override 关键字\n\n\n抽象类的定义\n由于 Kotlin 的类默认是 final，所以需要添加 open 关键字，使之可以被继承\n\n\n类的继承\nextends 关键字换成了:，跟接口实现保持了一致\n需要调用被继承方的构造器（默认为无参构造器）\n\n\n类的属性（成员变量）\nvar：默认带 getter 和 setter\nval：默认带 getter\n\n也可以自己定义 getter 和 setter 方法\nclass Person(age: Int, name: String) {  var age: Int = age    get() {      return field    }    set(value) {      field = value    }  var name: String = name}\n\n属性引用fun main() {    val ageRef = Person::age // 未绑定 Receiver    val person = Person(18, \"Bennyhuo\")    val nameRef = person::name // 绑定 Receiver    // 属性引用    ageRef.set(person, 20)    nameRef.set(\"Andyhuo\")}\n\n接口属性\n接口可以定义属性，但是不能赋值\n\ninterface Guy {    var moneyLeft: Double        get() {            return 0.0        }        set(value) {        }    fun noMoney() {        println(\"no money called.\")    }}\n\n接口属性没有 backing field我们尝试跟上面的类一样定义 getter 和 setter 方法\ninterface Guy {    var moneyLeft: Double        get() {            return field        }        set(value) {            field = value        }    fun noMoney() {        println(\"no money called.\")    }}\n将会获得编译器提示”Property in an interface cannot have a backing field”\n扩展方法\n这是 Kotlin 的大杀器，非常好用，一些工具类完全可以用扩展方法来替代，并且使用起来非常方便\n\n可以为现存的类定义新的方法\noperator fun String.minus(right: Any?) = this.replaceFirst(right.toString(), \"\")operator fun String.times(right: Int): String {    return (1..right).joinToString(\"\") { this }}operator fun String.div(right: Any?): Int {    val right = right.toString()    return this.windowed(right.length, 1, transform = {        it == right    }) // [false, false, false, false ... false, true, ..., true]        .count { it }}fun main() {    val value = \"HelloWorld World\"    println(value - \"World\")    println(value * 2)    val star = \"*\"    println(\"*\" * 20)    println(value / 3)    println(value / \"l\")    println(value / \"ld\")}\n\n空指针安全特性空类型安全\n注意：String 和 String? 不是一个类型\n\nvar nonNull: String = \"Hello\"nonNull = null // 编译器报错var nullable: String? = \"Hello\"nonNull = null // 编译通过\n\n强转为不可空类型var nullable: String? = \"Hello\"val length = nullable!!.length\n\n安全访问var nullable: String? = \"Hello\"nullable = nullval length = nullable?.length\n\n?. 的语法结构，我最早是在 TypeScript 里面看到的，基本逻辑就是，如果为空，则不会继续往下执行，一定程度上杜绝了 NPE 异常，而现在 Kotlin 把它引入了，非常棒\nelvis 运算符 (?:)var nullable: String? = \"Hello\"nullable = nullval length: Int = nullable?.length ?: 0\n\n\nnullable == null 时，length = 0\nnullable != null 时，length = nullable!!.length\n\n平台类型Kotlin 对于 Java 类库，是百分百支持的，但是 Java 里面没有 String? 这种类型，Kotlin 怎么处理呢？是当成非空还是可空类型？答案是：Kotlin 编译器不判断，有用户自己来判断是否可空还是非空比如在 Java 里面定义了一个 Person\npublic class Person {  public String getTitle() {    // ...  }}\n那么在 Kotlin 里面调用时，person.title 的类型是 String!，这个 String! 就是平台类型，可以非空也可以可空\nval person = Person()val title: String! = person.title\n\n智能类型转换自动转换为子类型val kotliner: Kotliner = Person(\"benny\", 20) // Person是Kotliner的子类if(kotliner is Person) {  println(kotliner.name) // 自动转换类型为 Person，不用向Java一样进行强转}\n\n可空转非空var value: String? = nullvalue = \"benny\"if(value != null) {  println(value.length) // 可空转非空，所以我们也可以说，非空类型是对应的可空类型的子类}\n\n不支持智能类型转换的场景\n在线程不安全的调用下，智能类型转换失效\n\nvar tag: String? = nullfun main() {  if(tag != null) {    println(tag.length) // 虽然判断不为空，但是其他线程可能对它进行修改  }}\n\n类型的安全转换 (as?)val kotliner: Kotliner = ...println(kotliner as? Person).name) // 安全转换，失败返回null\n\n针对类型的智能转换，有几个建议\n\n尽可能使用 val 来声明不可变引用，让程序的含义更加清晰确定\n尽可能减少函数对外部变量的访问，也为函数式编程提供基础（函数式编程的精髓就是拒绝副作用）\n必要时创建局部变量指向外部变量，避免因它变化引起程序错误\n\n","categories":["Kotlin从入门到精通"],"tags":["Kotlin"]},{"title":"Cassandra 学习笔记","url":"/p/60138/","content":"准备按照 Cassandra 集群部署搭建两台测试机，环境信息如下：\n\n\n\n名称\n IP\n 数据中心名称\n\n\n\n node-01\n192.168.198.130\ndatacenter1\n\n\nnode-02\n192.168.198.131\ndatacenter1\n\n\n\nKeyspace创建 Keyspacecreate_keyspace_statement ::=  CREATE KEYSPACE [ IF NOT EXISTS ] keyspace_name WITH options\n\n示例：\n## 使用SimpleStrategy复制策略CREATE KEYSPACE excelsior    WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};## 使用NetworkTopologyStrategy复制策略# 1. 确认分区名称$ nodetool statusDatacenter: datacenter1...# 2. 使用NetworkTopologyStrategy复制策略创建keyspaceCREATE KEYSPACE excalibur    WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1' : 1, 'DC2' : 3}    AND durable_writes = false;\n\n使用 Keyspaceuse_statement ::=  USE keyspace_name\n\n修改 Keyspace（replication factor）alter_keyspace_statement ::=  ALTER KEYSPACE keyspace_name WITH options\n\n示例：\nALTER KEYSPACE excelsior \tWITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 4};\n\n查看 KeyspaceDESCRIBE KEYSPACE &lt;keyspace name&gt;;\n\n使用该语句查看创建的键空间是否正确：\nDESCRIBE KEYSPACE excelsior;CREATE KEYSPACE excelsior WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3} AND durable_writes = true;\n\n删除 Keyspacedrop_keyspace_statement ::=  DROP KEYSPACE [ IF EXISTS ] keyspace_name\n\nDROP KEYSPACE excelsior;DESCRIBE excelsior;'excelsior' not found in keyspaces\n\nTable创建 Tablecreate_table_statement ::=  CREATE TABLE [ IF NOT EXISTS ] table_name                            '('                                column_definition                                ( ',' column_definition )*                                [ ',' PRIMARY KEY '(' primary_key ')' ]                            ')' [ WITH table_options ]column_definition      ::=  column_name cql_type [ STATIC ] [ PRIMARY KEY]primary_key            ::=  partition_key [ ',' clustering_columns ]partition_key          ::=  column_name                            | '(' column_name ( ',' column_name )* ')'clustering_columns     ::=  column_name ( ',' column_name )*table_options          ::=  COMPACT STORAGE [ AND table_options ]                            | CLUSTERING ORDER BY '(' clustering_order ')' [ AND table_options ]                            | optionsclustering_order       ::=  column_name (ASC | DESC) ( ',' column_name (ASC | DESC) )*\n\n创建 Table 必须指定主键，主键是用于在表中唯一标识某一行，可以是一列或多列。\n示例，在 excelsior 键空间创建一张名为 excelsior_alt_stats 的表：\nCREATE TABLE excelsior.excelsior_alt_stats (\tid UUID PRIMARY KEY,\tlastname text,\tbirthday timestamp,\tnationality text,\tweight text,\theight text);\n\ncassandra 还支持 collection（map, set, 或者 list）类型作为列：\nCREATE TABLE excelsior.whimsey (     id UUID PRIMARY KEY,     lastname text,     excelsior_teams set&lt;text&gt;,     events list&lt;text&gt;,     teams map&lt;int,text&gt; );\n\n甚至是嵌套的元组类型（tuple）：\nCREATE TABLE excelsior.route (    race_id int,     race_name text,     point_id int,     lat_long tuple&lt;text, tuple&lt;float,float&gt;&gt;,     PRIMARY KEY (race_id, point_id));\n\n更多数据类型请参阅下一节 Cassandra 数据结构\n静态列某些列可以在表定义中声明为 STATIC。静态的列将由属于同一分区（具有相同分区键）的所有行 “共享”。例如：\nCREATE TABLE t (    pk int,    t int,    v text,    s text static,    PRIMARY KEY (pk, t));INSERT INTO t (pk, t, v, s) VALUES (0, 0, 'val0', 'static0');INSERT INTO t (pk, t, v, s) VALUES (0, 1, 'val1', 'static1');SELECT * FROM t;   pk | t | v      | s  ----+---+--------+-----------   0  | 0 | 'val0' | 'static1'   0  | 1 | 'val1' | 'static1'   ## 所有记录中的静态列将永远展示最后一次更新的值   \n\n修改 Tablealter_table_statement   ::=  ALTER TABLE table_name alter_table_instructionalter_table_instruction ::=  ADD column_name cql_type ( ',' column_name cql_type )*                             | DROP column_name ( column_name )*                             | WITH options\n\n示例：\nALTER TABLE addamsFamily ADD gravesite varchar;ALTER TABLE addamsFamily       WITH comment = 'A most excellent and useful table';\n\n修改 Table 可以：\n\n向表中添加新列（通过 ADD 指令）。请注意，无法更改表的主键，因此新添加的列将不会成为主键的一部分。\n从表中删除列。这会丢弃列及其所有内容。\n更改一些表选项（通过 WITH 指令）。支持的选项与创建表时相同（在创建后无法更改的 COMPACT STORAGE 和 CLUSTERING ORDER 之外）。\n\n删除 Tabledrop_table_statement ::=  DROP TABLE [ IF EXISTS ] table_name\n\n截断 Table（清空表数据）truncate_statement ::=  TRUNCATE [ TABLE ] table_name\n\n由于表是唯一可以在当前截断的对象，因此可以省略 TABLE 关键字。\n截断表会永久删除表中的所有现有数据，但不会删除表本身。\nCassandra 数据结构\nCQL 是一种类型化语言，支持丰富的数据类型集，包括本地类型，集合类型，用户定义类型，元组类型和自定义类型：\n\ncql_type ::=  native_type | collection_type | user_defined_type | tuple_type | custom_type\n\n本地类型（Native Types）\n\n\n类型\n常量支持\n说明\n\n\n\n ascii\nstring\nASCII 字符串\n\n\nbigint\ninteger\n64 位无符号整数\n\n\nblob\nblob\n任意字节（无验证）\n\n\nboolean\nboolean\ntrue 或 false\n\n\ncounter\ninteger\n计数器列（64 位有符号值）\n\n\ndate\ninteger， string\n日期（没有相应的时间值）\n\n\ndecimal\ninteger， float\n十进制可变精度\n\n\ndouble\ninteger float\n64 位 IEEE-754 浮点\n\n\nduration\nduration\n持续时间（纳秒精度）\n\n\nfloat\ninteger， float\n32 位 IEEE-754 浮点\n\n\ninet\nstring\nIP 地址，IPv4（4 字节长）或 IPv6（16 字节长）\n\n\nint\ninteger\n32 位无符号整数\n\n\nsmallint\ninteger\n16 位有符号整数\n\n\ntext\nstring\nUTF8 编码的字符串\n\n\ntime\ninteger， string\n具有纳秒精度的时间（没有相应的日期值）\n\n\ntimestamp\ninteger， string\n时间戳（日期和时间），精度为毫秒\n\n\ntimeuuid\nuuid\nUUID（版本 1），通常用作 “无冲突” 时间戳\n\n\ntinyint\ninteger\n8 位有符号整数\n\n\nuuid\nuuid\n一个 UUID（任何版本）\n\n\nvarchar\nstring\nUTF8 编码的字符串\n\n\nvarint\ninteger\n任意精度整数\n\n\n其中需要注意的是时间类型：\ntimestamps\n时间戳类型的值被编码为 64 位有符号整数，表示自标准基准时间（称为纪元：1970 年 1 月 1 日格林威治标准时间 00:00:00）以来的毫秒数。\n\n\n1299038700000\n'2011-02-03 04:05+0000'\n'2011-02-03 04:05:00+0000'\n'2011-02-03 04:05:00.000+0000'\n'2011-02-03T04:05+0000'\n'2011-02-03T04:05:00+0000'\n'2011-02-03T04:05:00.000+0000'\n\n例如：\nSELECT *FROM pointWHERE ts = '2018-11-15 00:00:30.557+0000';\n\n或者\nSELECT *FROM pointWHERE ts = 1542211230557;\n\n其中，+0000 是 RFC 822 4-digit 时区规范，+0000 指 GMT。美国太平洋标准时间为 -0800，中国北京标准时间为 +8000，官方建议每次插入查询都带上时区，不加的话，默认是使用 Cassandra 节点配置的时区，可能会出现时区不一致导致的查询失败问题。\ndates\n 日期类型的值被编码为 32 位无符号整数，表示在该范围的中心处具有 “纪元” 的天数（2^31）。大纪元是 1970 年 1 月 1 日。\n\n至于时间戳，日期可以作为整数或使用日期字符串输入。在后一种情况下，格式应为 yyyy-mm-dd（例如’2011-02-03’）。\ntimes\n时间类型的值被编码为 64 位有符号整数，表示自午夜以来的纳秒数。\n\n对于时间戳，可以以整数或表示时间的字符串的形式输入时间。在后一种情况下，格式应为 hh:mm:ss [.fffffffff]（其中亚秒精度是可选的，如果提供，则可以小于纳秒）。例如，以下是一段时间内的有效输入：\n\n'08:12:54'\n'08:12:54.123'\n'08:12:54.123456'\n'08:12:54.123456789'\n\ndurations\n持续时间类型的值被编码为 3 个有符号整数的可变长度。这是因为一个月的天数可以改变，一天可以有 23 或 25 小时，具体取决于夏令时。\n\n第一个整数表示月数（32 位整数）\n\n第二个表示天数（32 位整数）\n\n第三个表示纳秒数（64 位整数）\n\n\n\n\n支持的单位：\n\n\ny: 年 (12 月)\nmo: 月 (1 月)\nw: 周 (7 天)\nd: 天 (1 天)\nh: 小时 (3,600,000,000,000 纳秒)\nm: 分钟 (60,000,000,000 纳)\ns: 秒 (1,000,000,000 纳)\nms: 毫秒 (1,000,000 纳)\nus or µs : 微妙 (1000 纳)\nns: 纳秒 (1 纳)\n\n\nISO 8601 格式：P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W\nISO 8601 替代格式：P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss]\n\n插入示例：\nINSERT INTO RiderResults (rider, race, result) VALUES ('Christopher Froome', 'Tour de France', 89h4m48s);INSERT INTO RiderResults (rider, race, result) VALUES ('BARDET Romain', 'Tour de France', PT89H8M53S);INSERT INTO RiderResults (rider, race, result) VALUES ('QUINTANA Nairo', 'Tour de France', P0000-00-00T89:09:09);\n\n\n持续时间列不能作为主键。这是由于无法精确确认持续时间。如果没有日期上下文，实际上不可能知道 1 个月是否大于 29 天。\n1 天的持续时间也不等于 24h，因为持续时间类型需要支持夏令时。\n\n集合类型（Collections）cassandra 支持三种类型的集合：Maps, Sets and Lists\ncollection_type ::=  MAP '&lt;' cql_type ',' cql_type '&gt;'                     | SET '&lt;' cql_type '&gt;'                     | LIST '&lt;' cql_type '&gt;'\n\n可以这样输入集合类型的数据：\ncollection_literal ::=  map_literal | set_literal | list_literalmap_literal        ::=  '{' [ term ':' term (',' term : term)* ] '}'set_literal        ::=  '{' [ term (',' term)* ] '}'list_literal       ::=  '[' [ term (',' term)* ] ']'\n\nMaps\nMaps 是一组（有序）键值对，其中键是唯一的，并且按其键排序。\n\nCREATE TABLE users (    id text PRIMARY KEY,    name text,    favs map&lt;text, text&gt; // A map of text keys, and text values);INSERT INTO users (id, name, favs)           VALUES ('jsmith', 'John Smith', { 'fruit' : 'Apple', 'band' : 'Beatles' });// Replace the existing map entirely.UPDATE users SET favs = { 'fruit' : 'Banana' } WHERE id = 'jsmith';\n\n另外，Maps 还具有一些高级特性：\n\n更新或插入一个或多个元素 \n\nUPDATE users SET favs['author'] = 'Ed Poe' WHERE id = 'jsmith';UPDATE users SET favs = favs + { 'movie' : 'Cassablanca', 'band' : 'ZZ Top' } WHERE id = 'jsmith';\n\n\n删除一个或多个元素（如果一个元素不存在，删除它是一个无效操作但不会抛出错误）\n\nDELETE favs['author'] FROM users WHERE id = 'jsmith';UPDATE users SET favs = favs - { 'movie', 'band'} WHERE id = 'jsmith';\n\nSets\n Sets 是唯一值的（已排序）集合。\n\nCREATE TABLE users (    id text PRIMARY KEY,    name text,    favs map&lt;text, text&gt; // A map of text keys, and text values);INSERT INTO users (id, name, favs)           VALUES ('jsmith', 'John Smith', { 'fruit' : 'Apple', 'band' : 'Beatles' });// Replace the existing map entirely.UPDATE users SET favs = { 'fruit' : 'Banana' } WHERE id = 'jsmith';\n\n另外，Sets 也具有一些高级特性：\n\n添加一个或多个元素（因为这是一个集合，插入一个已存在的元素是一个无效操作）\n\nUPDATE images SET tags = tags + { 'gray', 'cuddly' } WHERE name = 'cat.jpg';\n\n\n删除一个或多个元素（如果一个元素不存在，删除它是一个无效操作但不会抛出错误）\n\nUPDATE images SET tags = tags - { 'cat' } WHERE name = 'cat.jpg';\n\nLists\nLists 是非唯一值的（已排序）集合，其中元素按列表中的位置排序。它与 Sets 的区别就在于是否是唯一值。\n\nCREATE TABLE plays (    id text PRIMARY KEY,    game text,    players int,    scores list&lt;int&gt; // A list of integers)INSERT INTO plays (id, game, players, scores)           VALUES ('123-afde', 'quake', 3, [17, 4, 2]);// Replace the existing list entirelyUPDATE plays SET scores = [ 3, 9, 4] WHERE id = '123-afde';\n\n另外，Lists 同样也具有一些高级特性：\n\n在列表头或尾添加元素 \n\nUPDATE plays SET players = 5, scores = scores + [ 14, 21 ] WHERE id = '123-afde';UPDATE plays SET players = 6, scores = [ 3 ] + scores WHERE id = '123-afde';\n\n💡该操作不是幂等的，特别是在其中一个操作超时时，重试操作是不安全的，可能会导致同一数据插入两次。\n\n在列表中指定下标处设置值。该列表必须长度大于此下标，否则将抛出列表太小的错误 \n\nUPDATE plays SET scores[1] = 7 WHERE id = '123-afde';\n\n\n通过列表指定下标删除元素。该列表必须长度大于此下标，否则将抛出列表太小的错误。此外，当操作从列表中删除元素时，列表大小将减 1，从而改变此下标之后所有元素的位置 \n\nDELETE scores[1] FROM plays WHERE id = '123-afde';\n\n\n删除列表中指定下标之间的所有元素 \n\nUPDATE plays SET scores = scores - [ 12, 21 ] WHERE id = '123-afde';\n\n💡以上 2,3,4 操作会出现内部的 read-before-write，会比通常的更新消耗更多的资源，所以尽量使用 Sets 代替 Lists。\n用户自定义类型（User-Defined Types）\nCQL 支持用户定义类型（以下简称 UDT）。可以使用下面 create_type_statement，alter_type_statement 和 drop_type_statement 创建，修改和删除此类型。\n\nuser_defined_type ::=  udt_nameudt_name          ::=  [ keyspace_name '.' ] identifier\n\n创建create_type_statement ::=  CREATE TYPE [ IF NOT EXISTS ] udt_name                               '(' field_definition ( ',' field_definition )* ')'field_definition      ::=  identifier cql_type\n\nUDT 有一个名称（用于声明该类型的列），是一组命名和类型字段。字段名称可以是任何类型，包括集合或其他 UDT。例如：\nCREATE TYPE phone (    country_code int,    number text,)CREATE TYPE address (    street text,    city text,    zip text,    phones map&lt;text, phone&gt;)CREATE TABLE user (    name text PRIMARY KEY,    addresses map&lt;text, frozen&lt;address&gt;&gt;)\n\n💡注意\n\n尝试创建现有类型时请使用 IF NOT EXISTS 选项，否则将会抛出错误。\nUDT 本质上绑定到创建它的键空间，并且只能在该键空间中使用。在创建时，如果类型名称以键空间名称为前缀，则在该键空间中创建它。否则，它将在当前键空间中创建。\n从 Cassandra 4.0 开始，在大多数情况下必须冻结 UDT，因此在上面的表定义中冻结了 &lt;address&gt;。有关详细信息，请参阅冻结部分。\n\n修改alter_type_statement    ::=  ALTER TYPE udt_name alter_type_modificationalter_type_modification ::=  ADD field_definition                             | RENAME identifier TO identifier ( identifier TO identifier )*\n\n修改一个 UDT，可以：\n\n在类型中添加一个新字段 \n\nALTER TYPE address ADD country text\n\n请注意：新添加的字段在之前的记录中，都将被置为 NULL。\n\n重命名该类型的字段 \n\nALTER TYPE address RENAME zip TO zipcode\n\n删除drop_type_statement ::=  DROP TYPE [ IF EXISTS ] udt_name\n\n使用udt_literal ::=  '{' identifier ':' term ( ',' identifier ':' term )* '}'\n\n使用 UDT 有点像 Maps，例如\nINSERT INTO user (name, addresses)          VALUES ('z3 Pr3z1den7', {              'home' : {                  street: '1600 Pennsylvania Ave NW',                  city: 'Washington',                  zip: '20500',                  phones: { 'cell' : { country_code: 1, number: '202 456-1111' },                            'landline' : { country_code: 1, number: '...' } }              },              'work' : {                  street: '1600 Pennsylvania Ave NW',                  city: 'Washington',                  zip: '20500',                  phones: { 'fax' : { country_code: 1, number: '...' } }              }          })\n\n元组（Tuples）\nCQL 还支持元组和元组类型（元素可以是不同类型），类似于匿名的 UDT 或者是 Scala 的 Tuple 类型。\n\ntuple_type    ::=  TUPLE '&lt;' cql_type ( ',' cql_type )* '&gt;'tuple_literal ::=  '(' term ( ',' term )* ')'\n\n例如：\nCREATE TABLE durations (    event text,    duration tuple&lt;int, text&gt;,)INSERT INTO durations (event, duration) VALUES ('ev1', (3, 'hours'));\n\n自定义类型（Custom Types）\n自定义类型主要是为了兼容老项目，不建议使用。使用已有的类型加上用户自定义类型（UDT）就够了。\n\ncustom_type ::=  string\n\n数据增删改查（CRUD）SELECTselect_statement ::=  SELECT [ JSON | DISTINCT ] ( select_clause | '*' )                      FROM table_name                      [ WHERE where_clause ]                      [ GROUP BY group_by_clause ]                      [ ORDER BY ordering_clause ]                      [ PER PARTITION LIMIT (integer | bind_marker) ]                      [ LIMIT (integer | bind_marker) ]                      [ ALLOW FILTERING ]select_clause    ::=  selector [ AS identifier ] ( ',' selector [ AS identifier ] )selector         ::=  column_name                      | term                      | CAST '(' selector AS cql_type ')'                      | function_name '(' [ selector ( ',' selector )* ] ')'                      | COUNT '(' '*' ')'where_clause     ::=  relation ( AND relation )*relation         ::=  column_name operator term                      '(' column_name ( ',' column_name )* ')' operator tuple_literal                      TOKEN '(' column_name ( ',' column_name )* ')' operator termoperator         ::=  '=' | '&lt;' | '&gt;' | '&lt;=' | '&gt;=' | '!=' | IN | CONTAINS | CONTAINS KEYgroup_by_clause  ::=  column_name ( ',' column_name )*ordering_clause  ::=  column_name [ ASC | DESC ] ( ',' column_name [ ASC | DESC ] )*\n\n示例：\nSELECT name, occupation FROM users WHERE userid IN (199, 200, 207);SELECT JSON name, occupation FROM users WHERE userid = 199;SELECT name AS user_name, occupation AS user_occupation FROM users;SELECT time, valueFROM eventsWHERE event_type = 'myEvent'  AND time &gt; '2011-02-03'  AND time &lt;= '2012-01-01'SELECT COUNT (*) AS user_count FROM users;\n\nAllowing filtering\n默认情况下，CQL 仅允许不涉及 “过滤” 服务器端的选择查询，原因是那些 “非过滤” 查询具有可预测的性能，因为它们的查询性能与 Limit 成比例。\n\n举个例子：\nCREATE TABLE users (    username text PRIMARY KEY,    firstname text,    lastname text,    birth_year int,    country text)CREATE INDEX ON users(birth_year);\n\n以下两种查询是不需要添加 ALLOW FILTERING 的：\nSELECT * FROM users;SELECT * FROM users WHERE birth_year = 1981;\n\n因为在这两种情况下，Cassandra 都保证这些查询性能与返回的数据量成正比。\n而下面的这个查询，则需要强制添加：\nSELECT * FROM users WHERE birth_year = 1981 AND country = 'FR' ALLOW FILTERING;\n\n👉🏼 关于如何定义可预测的列，可参考 Cassandra 中的索引\nINSERTinsert_statement ::=  INSERT INTO table_name ( names_values | json_clause )                      [ IF NOT EXISTS ]                      [ USING update_parameter ( AND update_parameter )* ]names_values     ::=  names VALUES tuple_literaljson_clause      ::=  JSON string [ DEFAULT ( NULL | UNSET ) ]names            ::=  '(' column_name ( ',' column_name )* ')'\n\n示例：\nINSERT INTO NerdMovies (movie, director, main_actor, year)                VALUES ('Serenity', 'Joss Whedon', 'Nathan Fillion', 2005)      USING TTL 86400;INSERT INTO NerdMovies JSON '{\"movie\": \"Serenity\",                              \"director\": \"Joss Whedon\",                              \"year\": 2005}';\n\n💡请注意\n\n与 SQL 不同，INSERT 默认情况下不检查行的先前存在：如果之前不存在，则创建行，否则更新。此外，没有办法知道发生了哪些创建或更新。\n\n如果要做到存在则不更新，可以使用 IF NOT EXISTS 条件。但请注意，使用 IF NOT EXISTS 将导致不可忽略的性能成本（内部使用 Paxos），因此应谨慎使用。\n\nINSERT 的所有更新都以原子方式单独应用。\n\n\nUPDATEupdate_statement ::=  UPDATE table_name                      [ USING update_parameter ( AND update_parameter )* ]                      SET assignment ( ',' assignment )*                      WHERE where_clause                      [ IF ( EXISTS | condition ( AND condition )*) ]update_parameter ::=  ( TIMESTAMP | TTL ) ( integer | bind_marker )assignment       ::=  simple_selection '=' term                     | column_name '=' column_name ( '+' | '-' ) term                     | column_name '=' list_literal '+' column_namesimple_selection ::=  column_name                     | column_name '[' term ']'                     | column_name '.' `field_namecondition        ::=  simple_selection operator term\n\n示例：\nUPDATE NerdMovies USING TTL 400   SET director   = 'Joss Whedon',       main_actor = 'Nathan Fillion',       year       = 2005 WHERE movie = 'Serenity';UPDATE UserActions   SET total = total + 2   WHERE user = B70DE1D0-9908-4AE3-BE34-5573E5B09F14     AND action = 'click';\n\n💡请注意\n\n与 SQL 不同，UPDATE 默认情况下不检查行的先前存在（除非通过 IF）：如果之前不存在，则创建行，否则更新。此外，没有办法知道是否发生了创建或更新。\n\n可以通过 IF 在某些列上使用条件，在这种情况下，除非满足条件，否则不会更新行。但请注意，使用 IF 条件会产生不可忽视的性能成本（内部使用 Paxos），因此应谨慎使用。\n\n在 UPDATE 语句中，同一分区键中的所有更新都以原子方式单独应用。\n\n\n此外，UPDATE 操作针对某些数据类型有强制性要求：\n\nc = c + 3\n\n用于递增 / 递减计数器。 \n‘=’符号后面的列名称必须与’=’符号前面的列名相同。请注意，仅在计数器上允许递增 / 递减，并且是计数器上允许的唯一更新操作。\n\nid = id + &lt;some-collection&gt; 和 id[value1] = value2\n\n用于集合。\n\nid.field = 3\n\n在非冻结的用户定义类型上设置字段的值。\nDELETEdelete_statement ::=  DELETE [ simple_selection ( ',' simple_selection ) ]                      FROM table_name                      [ USING update_parameter ( AND update_parameter )* ]                      WHERE where_clause                      [ IF ( EXISTS | condition ( AND condition )*) ]\n\n示例：\nDELETE FROM NerdMovies USING TIMESTAMP 1240003134 WHERE movie = 'Serenity';DELETE phone FROM Users WHERE userid IN (C73DE1D3-AF08-40F3-B124-3FF3E5109F22, B70DE1D0-9908-4AE3-BE34-5573E5B09F14);\n\n💡请注意\n\nWHERE 子句指定要删除的行。使用 IN 运算符可以使用一个语句删除多行。可以使用不等运算符（例如 &gt;=）删除一系列行。\n\n在 DELETE 语句中，同一分区键中的所有删除都以原子方式单独应用。\n\nDELETE 操作可以通过使用 IF 子句来条件化，类似于 UPDATE 和 INSERT 语句。但是，与 INSERT 和 UPDATE 语句一样，这将导致不可忽略的性能成本（内部，将使用 Paxos），因此应谨慎使用。\n\n\n批处理\n批处理只允许包含 UPDATE，INSERT 和 DELETE 语句。\n批处理节省客户端和服务器之间的网络资源消耗。\n\nbatch_statement        ::=  BEGIN [ UNLOGGED | COUNTER ] BATCH                            [ USING update_parameter ( AND update_parameter )* ]                            modification_statement ( ';' modification_statement )*                            APPLY BATCHmodification_statement ::=  insert_statement | update_statement | delete_statement\n\n示例\nBEGIN BATCH   INSERT INTO users (userid, password, name) VALUES ('user2', 'ch@ngem3b', 'second user');   UPDATE users SET password = 'ps22dhds' WHERE userid = 'user3';   INSERT INTO users (userid, password) VALUES ('user4', 'ch@ngem3c');   DELETE name FROM users WHERE userid = 'user1';APPLY BATCH;\n\n💡请注意\n\n属于给定分区键的 BATCH 中的所有更新都是单独执行的。\n默认情况下，批处理中的所有操作都按记录执行，以确保所有变更都最终完成（或不执行任何操作）。类似于 SQL 事务，但不完全等同于 SQL 事务。\n\nUNLOGGED batches默认情况下，Cassandra 使用批处理日志来确保所有变更都最终完成（或不执行任何操作）【请注意，操作仅在单个分区中隔离】。\n批处理跨越多个分区时，批处理在性能上会有所损失。可以使用 UNLOGGED 选项来跳过批处理日志，不过，如果批处理失败，可能会造成批处理中的任务部分成功部分失败，请谨慎选择。\nCOUNTER batches使用 COUNTER 选项进行批量计数器更新。\n与 Cassandra 中的其他更新不同，计数器更新不是幂等的。\n参考资源Apache Cassandra Documentation\n","categories":["个人笔记"],"tags":["Cassandra"]},{"title":"Apereo CAS | Rest API 实践","url":"/p/2526/","content":"本文整理了单点登录系统 CAS 提供的 Rest API，并给出了示例。\n\n本文基于 CAS 6.3.4，并基于 cas-overlay-template 搭建的 cas 项目\n\n\n\n","categories":["个人笔记"],"tags":["SSO","CAS"]},{"title":"Apereo CAS | 修改登录页样式","url":"/p/63354/","content":"本文介绍单点登录系统 CAS 自定义登录页样式。\n\n本文基于 CAS 6.3.4，并基于 cas-overlay-template 搭建的 cas 项目\n\n\n\n1、下载源码去官网下载 cas 源码\n\n国内的朋友，可以试试这款 Github加速插件\n2、解压源码，找到 cas-server-support-thymeleaf$ cd cas-master$ lsapi/          core/  gradle/            gradlew.bat    NOTICE       settings.gradle  testcas.sh*build.gradle  docs/  gradle.properties  LICENSE        README.md    style/           webapp/ci/           etc/   gradlew*           lombok.config  release.sh*  support/$ cd support/cas-server-support-thymeleaf/src/main/resources/cas-theme-default.properties --&gt; cas主题配置messages_fr.properties --&gt; ...META-INF/ static/ --&gt; 静态内容templates/ --&gt; thymeleaf模板$ cd staticcss/  favicon.ico  images/  js/$ cd ../templates/...layout.htmlerror.html\n\n\n3、复制到项目，修改\n修改的时候，遵循以下原则\n\n只复制需要修改的文件到 src/main/resources/，避免项目冗余和向上兼容\n尽量不更改模板文件结构，例如 id，class 元素\n不使用的部分，用注释代替删除，方便还原\n\n\n一般来说，我们只需要修改以下几个文件即可\n\ncas-master/support/cas-server-support-thymeleafsrc/main/resources/static/css/cas.css\\\ncas-master/support/cas-server-support-thymeleaf/src/main/resources/templates/layout.html\ncas-master/support/cas-server-support-thymeleaf/src/main/resources/templates/login/casLoginView.html\n\n注意，虽然 login/casLoginView.html 位于 login 目录下，但是复制到 cas-overlay 时，需要位于 src/main/resources\n最终 cas-overlay 的目录结构\n\n相关资料\nUser-Interface-Customization\nThymeleaf\n\n","categories":["个人笔记"],"tags":["SSO","CAS","Java"]},{"title":"Dubbo 使用 Kryo 序列化协议的思考","url":"/p/13982/","content":"本文是使用 Dubbo 过程中的一些思考，不足以作为参考，只为留存记录。\n\n\n前言Dubbo 官方推荐使用 kryo 或者 fst 作为 RPC 序列化协议，原因是这两款序列化协议，性能显著优于其他序列化协议。虽然高效，但其实 Dubbo 对这两款的支持却不是那么理想，导致使用过程中出现了一些问题\n多服务下 kryo 序列化问题Q1、无法使用 kryo 类索引红利\n在多模块协作的系统中，无法保证服务提供方和消费方 DTO 数量以及注册顺序的一致，可能 A 作为服务提供方，提供了 DTO，而 B 作为消费方，可能同时消费 A、C 的服务，同时自身也对外提供服务，在使用 Kryo 序列化时，需要同时注册 A、B、C 服务的 DTO。\n\n例如 A 系统的 SerializationOptimizer 实现\npublic class ASerializationOptimizerImpl implements SerializationOptimizer {    @Override    public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() {        return A1.class;        return A2.class;        return A3.class;        return A4.class;        return C1.class;        return C2.class;    }}\n\nB 系统的 SerializationOptimizer 实现\npublic class BSerializationOptimizerImpl implements SerializationOptimizer {    @Override    public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() {        return A1.class;        return A2.class;        return A3.class;        return A4.class;        return B1.class;        return B2.class;        return C1.class;        return C2.class;    }}\n\n这里需要说明的是，kryo 之所以高效，不仅仅是因为其二进制序列化，更由于其可以预先为待序列化的类指定索引，在 RPC 传输过程中，只需要使用索引代替全类名，在类使用非常频繁的情况下，可以节省大量字节，从而大大提升传输效率。\n而 Dubbo 似乎也意识到了这个问题，所以将一些使用频繁的类进行了预处理，见 org.apache.dubbo.common.serialize.kryo.utils.AbstractKryoFactory#create\n但是其中很重要的一处\nSerializableClassRegistry.getRegisteredClasses().forEach((clazz, ser) -&gt; {    if (ser == null) {        kryo.register(clazz);    } else {        kryo.register(clazz, (Serializer) ser);    }});\n\n使用的是 kryo.register(clazz)，无法指定 kryo class index，这里我分析了一下，可能是为了兼容老的序列化协议，比如 json 等，而且 SerializableClassRegistry 是很早就有的接口，并没有考虑到类索引的问题。\n所以这里要注册的话，只能将项目内所有类都写到同一个 jar 中，然后写一个统一的 SerializationOptimizer 实现。\n这在微服务系统中是无法实现的，因为每个服务必然会提供自己的 dto.jar 和 dubbo-service.jar，而且服务之间也不可能依赖其他所有的服务，所以这一条无法满足。\n这样一来，类的注册顺序不能保证，类的数量也无法保证，所以 Dubbo 的 kryo 序列化只能说在一定程度上提升了效率，但是并没有完全发挥出 kryo 的性能优势。\nQ2、社区不活跃\n虽然阿里重启了 Dubbo，并且加入了 Apache 进行孵化，但是社区相较于 Spring Cloud，还是不够活跃，而且阿里的开源产品，多多少少带了点阿里内部的味道，不如 Spring Cloud 通用和考虑周全。一些功能我们不需要（比如 dubbo 注册时的一堆参数，很多都是需要阿里的一套开发体系才能使用到），我们需要的又迟迟不添加（kryo 的 ClassId 支持）。\n\n所以目前项目已经全面切换到 Spring Cloud Kubernetes，有时间也会总结一下 Dubbo 切换到 Spring Cloud 的经验，以及在云原生时代 Spring Cloud 所做出的努力。\n相关资料\nDubbo Issue\nKryo 和 FST 序列化\nKryo 序列化协议\n\n","categories":["个人笔记"],"tags":["Dubbo"]},{"title":"Docker 常用命令","url":"/p/7569/","content":"操作 Container启动容器并启动 bash（交互方式）\n\ndocker run -i -t &lt;image_name/continar_id&gt; /bin/bash\n\n启动容器以后台方式运行 (更通用的方式）\n这里的 image_name 包含了 tag，例如 hello.demo.kdemo:v1.0\n\ndocker run -d -it  image_name\n\n附着到正在运行的容器docker attach &lt;id、container_name&gt;\n\n进入正在运行的容器内部，同时运行 bash (比 attach 更好用)\n这里的 bash 也可以换成具体的命令，例如 ping 127.0.0.1\n\ndocker exec -t -i &lt;id/container_name&gt; /bin/bash\n\ndocker exec 是如此的有用，以至于我们通常是将其封装为一个脚本，放到全局可调用的地方，比如，可以写成一个 indocker.sh\n$ cat indocker.shdocker exec -t -i $1 /bin/bash# 查看需要附着的容器id$ docker ps | less -SCONTAINER ID        IMAGE                                                 9cf7b563f689        hello.demo.kdemo:v160525.202747$ ./indocker.sh 9cf7b563f689\n\n查看容器日志docker logs &lt;id/container_name&gt;\n\n实时查看日志输出docker logs -f &lt;id/container_name&gt; # 类似 tail -f [-t]带上时间戳\n\n列出当前所有正在运行的 containerdocker ps\n\n用一行列出所有正在运行的 container\n容器多的时候非常清晰\n\ndocker ps | less -S\n\n列出所有的 containerdocker ps -a\n\n列出最近一次启动的 containerdocker ps -l\n\n显示一个运行的容器里面的进程信息docker top &lt;id/container_name&gt;\n\n查看容器内部详情细节docker inspect &lt;id/container_name&gt;\n\n在容器中安装新的程序docker run &lt;id/container_name&gt; apt-get install -y app_name\n\n从容器里面拷贝文件 / 目录到本地一个路径docker cp &lt;id/container_name&gt;:/container_path to_path\n\n保存对容器的修改（commit）\n当你对某一个容器做了修改之后（通过在容器中运行某一个命令），可以把对容器的修改保存下来，这样下次可以从保存后的最新状态运行该容器\n\ndocker commit &lt;id/container_name&gt; new_image_name\n\n删除单个容器docker rm &lt;id/container_name&gt;\n\n删除所有容器docker rm `docker ps -a -q`# 停止所有的容器docker stop $(docker ps -aq)# 删除所有的容器docker rm $(docker ps -aq)\n\n停止、启动、杀死、重启一个容器docker stop &lt;id/container_name&gt;docker start &lt;id/container_name&gt;docker kill &lt;id/container_name&gt;docker restart &lt;id/container_name&gt;\n\n操作 Image列出镜像docker image ls\n\n从 dockerhub 检索 imagedocker search &lt;image_name&gt;\n\n下载 imagedocker pull &lt;image_name&gt;\n\n删除一个或者多个镜像docker rmi &lt;image_name&gt;\n\n显示一个镜像的历史docker history &lt;image_name&gt;\n\n发布 docker 镜像docker push &lt;new_image_name&gt;\n\n要发布到私有 Registry 中的镜像，在镜像命名中需要带上 Registry 的域名（如果非 80 端口，同时需要带上端口号）\ndocker push dockerhub.yourdomain.com:443/hello.demo.kdemo:v1.0\n\n拉取 docker 镜像docker pull &lt;image_name&gt;\n\n网络操作查看 docker0 的网络 (宿主机上操作)ip a show docker0\n\n查看容器的 IP 地址docker inspect -f '{{ .NetworkSettings.IPAddress }}' &lt;id/container_name&gt;\n\n或者附着到容器内部查看其内部 ip\ndocker exec -it &lt;id/container_name&gt; ip a show eth0\n\ndocker 信息查看 docker 版本$ docker versionClient: Docker Engine - Community Version:           20.10.6 API version:       1.41 Go version:        go1.13.15 Git commit:        370c289 Built:             Fri Apr  9 22:46:01 2021 OS/Arch:           linux/amd64 Context:           default Experimental:      trueServer: Docker Engine - Communit# ...\n\n查看 docker 系统的信息$ docker infoClient: Context:    default Debug Mode: false Plugins:  app: Docker App (Docker Inc., v0.9.1-beta3)  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)  scan: Docker Scan (Docker Inc., v0.7.0)Server: Containers: 83  Running: 61  Paused: 0  Stopped: 22# ...\n\n高级技巧docker 批量删除无用的容器或镜像# 删除异常停止的docker容器docker rm `docker ps -a | grep Exited | awk '{print $1}'`# 删除名称或标签为none的镜像docker rmi -f  `docker images | grep '&lt;none&gt;' | awk '{print $3}'`\n\n清理所有停止运行的容器docker container prune # 或者 docker rm $(docker ps -aq)\n\n清理所有悬挂（）镜像docker image prune# 或者docker rmi $(docker images -qf \"dangling=true\")\n\n清理所有无用数据卷docker volume prune\n\n清理所有无用镜像\n清理后再次使用需要重新下载\n\ndocker image prune -a\n\n按需批量清理容器docker ps -a --filter 'exited=0'\n\n目前支持的过滤器（–filter）有\nid (container’s id)\nlabel (label= or label==)\nname (container’s name)\nexited (int - the code of exited containers. Only useful with –all)\nstatus (created|restarting|running|removing|paused|exited|dead)\nancestor ([:],  or &lt;image@digest&gt;) - filters containers that were created from the given image or a descendant.\nbefore (container’s id or name) - filters containers created before given id or name\nsince (container’s id or name) - filters containers created since given id or name\nisolation (default|process|hyperv) (Windows daemon only)\nvolume (volume name or mount point) - filters containers that mount volumes.\nnetwork (network id or name) - filters containers connected to the provided network\nhealth (starting|healthy|unhealthy|none) - filters containers based on healthcheck status\n\n按需批量清理镜像 # 列出所有悬挂（dangling）的镜像，也就是显示为&lt;none&gt;的那些docker images --filter \"dangling=true\" # 清理所有悬挂（dangling）的镜像，也就是显示为&lt;none&gt;的那些docker rmi $(docker images -qf \"dangling=true\") # 清理，同上docker image prune --filter 'exited=0'\n\n目前支持的过滤器（–filter）有\ndangling (boolean - true or false)\nlabel (label= or label==)\nbefore ([:],  or &lt;image@digest&gt;) - filter images created before given id or references\nsince ([:],  or &lt;image@digest&gt;) - filter images created since given id or references\nreference (pattern of an image reference) - filter images whose reference matches the specified pattern\n\n查看 docker 日志docker logs -f `docker ps | grep qq- | awk '{print $1}'`\n\n","categories":["个人笔记"],"tags":["docker"]},{"title":"FastDFS 单机部署指南","url":"/p/56864/","content":"简介FastDFS 是一个开源的分布式文件系统，官方介绍有详细的介绍，不多赘述。本文主要是 FastDFS 的搭建及采坑指南。\n\n\nStep By Step Guide系统\n阿里云 ECS Ubuntu 16.04\n\n编译环境按需安装，这里是针对新的 ubuntu 系统\n$ apt-get install git gcc gcc-c++ make automake autoconf libtool pcre pcre-devel zlib zlib-devel openssl-devel wget vim\n\n磁盘目录\n\n\n说明\n位置\n\n\n\n所有安装包\n /usr/local/src\n\n\n 数据存储位置\n /data/dfs/\n\n\n$ mkdir /data/dfs #创建数据存储目录（对于阿里云ECS，最好建立在数据盘上，是用来存放文件的）$ cd /usr/local/src #切换到安装目录准备下载安装包\n\n安装 libfatscommon$ wget https://github.com/happyfish100/libfastcommon/archive/master.zip$ unzip master.zip$ cd libfastcommon-1.0.39/$ ./make.sh &amp;&amp; ./make.sh install #编译安装\n\n安装 FastDFS$ cd ../ #返回上一级目录$ wget https://github.com/happyfish100/fastdfs/archive/master.zip$ unzip master.zip$ cd fastdfs-master/$ ./make.sh &amp;&amp; ./make.sh install #编译安装#配置文件准备$ cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf$ cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf$ cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf #客户端文件，测试用$ cp /usr/local/src/fastdfs-master/conf/http.conf /etc/fdfs/ #供nginx访问使用$ cp /etc/nginx/mime.types /etc/fdfs/ #供nginx访问使用\n\n安装 fastdfs-nginx-module官网的文档，是针对没有安装过 Nginx 的机器，重新编译了一遍 Nginx，把 module 直接编译进 Nginx 了。但是针对已经安装 Nginx 的服务器来说，显然是不好的。\n根据 Nginx 官方文档 - 编译第三方动态模块，编译了 fastdfs-nginx-module，以供已存在的 Nginx 使用。\n我已经编译好了 fastdfs-nginx-module，可以直接下载，并跳到加载并使用模块，如果想知其所以然，可以往下看。\n准备 fastdfs-nginx-module 源码包$ cd ../ #返回上一级目录$ wget https://github.com/happyfish100/fastdfs-nginx-module/archive/master.zip$ unzip master.zip\n\n获取对应版本的 Nginx 源码包$ nginx -v # 确认服务器的Nginx版本nginx version: nginx/1.14.2$ wget http://nginx.org/download/nginx-1.14.2.tar.gz$ tar -xzvf nginx-1.14.2.tar.gz\n\n编译动态模块$ cd nginx-1.14.2/$ ./configure --with-compat --add-dynamic-module=/usr/local/src/fastdfs-nginx-module-master/src$ make modules\n\n将模块库（.so 文件）复制到 /etc/nginx/modules$ cp ngx_http_fastdfs_module.so /etc/nginx/modules/\n\n加载并使用模块Tips: 要将模块加载到 Nginx, 在 nginx.conf 文件开头添加 load_module 命令\n$ vim /etc/nginx/nginx.conf# 添加如下命令load_module modules/ngx_http_fastdfs_module.so;# 保存退出\n\n添加 FastDFS 配置使模块生效$ vim /etc/nginx/conf.d/fastdfs.conf# 添加如下配置server {    listen 8888;    ## 该端口为storage.conf中的http.server_port相同    server_name {your_domain};    location ~/group[0-9]/ {        ngx_fastdfs_module;    }    error_page 500 502 503 504 /50x.html;    location = /50x.html {        root html;    }}\n\n单机部署这里只描述下单机环境的部署方式，集群在官方文档有，没有实际使用过。\ntracker 配置$ vim /etc/fdfs/tracker.conf# 建议修改以下内容bind_addr={你的内网IP}base_path=/data/dfs # 建议修改为数据盘位置# 可选修改port=22122  # tracker服务器端口\n\nstorage 配置$ vim /etc/fdfs/storage.conf# 建议修改base_path=/data/dfs  # 数据和日志文件存储根目录（建议修改为数据盘位置）store_path0=/data/dfs  # 第一个存储目录（建议修改为数据盘位置）tracker_server={tracker.bind_addr}:{tracker.port}  # tracker服务器IP和端口http.server_port=8888  # http访问文件的端口（默认8888,看情况修改,和nginx中保持一致）# 可选修改port=23000  # storage服务端口（默认23000,一般不修改）\n\nclient 测试$ vim /etc/fdfs/client.conf# 建议修改base_path=/data/dfstracker_server={tracker.bind_addr}:{tracker.port}  # tracker服务器IP和端口# 保存后测试$ fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/nginx-1.14.2.tar.gzgroup1/M00/00/00/CgoKvlyUmi-AMVKDAA9-WL9wzEw.tar.gz # 下载时通过该ID下载# 返回ID表示成功 如：group1/M00/00/00/xx.tar.gz\n\n配置 nginx 访问vim /etc/fdfs/mod_fastdfs.conf# 建议修改tracker_server={tracker.bind_addr}:{tracker.port}  # tracker服务器IP和端口url_have_group_name=truestore_path0=/data/dfs# 修改完保存$ nginx -s reloadngx_http_fastdfs_set pid=8364 # 看见这条消息说明nginx模块启动成功了$ lsof -i:8888 # 查看Nginx下载端口是否正常启动nginx   31061  root   10u  IPv4 20389985      0t0  TCP *:8888 (LISTEN)\n\n测试下载在浏览器输入\nhttp://{IP}:8888/group1/M00/00/00/CgoKvlyUmi-AMVKDAA9-WL9wzEw.tar.gz?filename=nginx-1.14.2.tar.gz //刚才上传返回的ID\n\n弹出下载文件框，说明部署成功！\n\n相关命令防火墙$ sudo ufw enable|disable\n\ntracker$ /etc/init.d/fdfs_trackerd start # 启动tracker服务$ /etc/init.d/fdfs_trackerd restart # 重启动tracker服务$ /etc/init.d/fdfs_trackerd stop # 停止tracker服务$ update-rc.d fdfs_trackerd enable # 自启动tracker服务\n\nstorage$ /etc/init.d/fdfs_storaged start # 启动storage服务$ /etc/init.d/fdfs_storaged restart # 重动storage服务$ /etc/init.d/fdfs_storaged stop # 停止动storage服务$ update-rc.d fdfs_storaged enable # 自启动storage服务\n\nnginx$ service nginx start # 启动nginx$ nginx -s reload # 重启nginx$ nginx -s stop # 停止nginx\n\n问题执行 nginx -s reload 后，访问 502# 查看nginx日志$ vim /var/log/nginx/error.log\n\n如果发现错误日志：include file \"http.conf\" not exists, line: \"#include http.conf\"，fastdfs nginx 模块缺少配置文件，执行以下命令补全配置文件即可。\n$ cp /usr/local/src/fastdfs-master/conf/http.conf /etc/fdfs/ #供nginx访问使用$ cp /etc/nginx/mime.types /etc/fdfs/ #供nginx访问使用\n\n\n\n\n\n\n\n\n\n\n\n","categories":["个人笔记"],"tags":["FastDFS"]},{"title":"EntityManager 的 Clear 方法的使用","url":"/p/12153/","content":"在日常开发中，如果使用 hibernate 的话，常常会被 hibernate 的事务搞得焦头烂额。今天解决了之前项目中一直存在的问题，记录一下。\n问题描述有一张表 TemplateCopy, 如下\npublic class TemplateCopy {    @Id    @GeneratedValue(strategy = GenerationType.IDENTITY)    private Integer id;    private String name;    private String description;    @OneToMany(mappedBy = \"template\")    private Set&lt;SubDomainWeightsCopy&gt; subDomainWeights;    @OneToMany(mappedBy = \"template\")    private Set&lt;QuestionWeightsCopy&gt; questionWeights;}\n\n关联了两张表:\npublic class SubDomainWeightsCopy {    @JsonIgnore    @Id    @ManyToOne    @JoinColumn(name = \"template_id\")    private TemplateCopy template;    @Id    @ManyToOne    @JoinColumn(name = \"sub_domain_id\")    private SubDomainCopy subDomain;    private BigDecimal weights; //权重    private BigDecimal score;    @Data    public static class RelationId implements Serializable {        private Integer template;        private Integer subDomain;    }}\n\npublic class QuestionWeightsCopy implements IWeightsValue {    @JsonIgnore    @Id    @ManyToOne    @JoinColumn(name = \"template_id\")    private TemplateCopy template;    @Id    @ManyToOne    @JoinColumn(name = \"question_id\")    private QuestionCopy question;    private BigDecimal weights;    private BigDecimal score;    @Data    public static class RelationId implements Serializable {        private Integer template;        private Integer question;    }}\n\n简单的看一下，TemplateCopy 中有一堆 SubDomainWeightsCopy，和一堆 QuestionWeightsCopy，我们在保存 TemplateCopy 的时候，通常按照如下来保存\n1. templateCopy = save(TemplateCopy)2. QuestionWeightsCopy.setTemplateCopy(templateCopy)3. save(QuestionWeightsCopy)4. SubDomainWeightsCopy.setTemplateCopy(templateCopy)5. save(SubDomainWeightsCopy)\n\n到这就好了，数据库已经保存了关联关系。但是，这时候如果返回 save 好的 templateCopy，subDomainWeights 和 questionWeights 将会是 null。\n问题解决使用 EntityManager 的 clear 方法\n保存完毕后，执行 entityManager.clear ();\n 然后再次查询该对象，即可完整返回该对象。\n\nEntityManager clear 的作用？EntityManager clear 方法会清空其关联的缓存，从而强制在事务中稍后执行新的数据库查询。\n什么时候使用 EntityManager clear\n在进行批处理时，为了避免巨大的缓存占用内存并因长时间的脏检查而增加刷新的时间\n在进行 DML 或 SQL 查询时，它将完全绕过实体管理器缓存。在这种情况下，由于缓存，将不会实际去数据库查，会直接将缓存返回。所以造成了数据库已经保存了，但是查出来还是未保存的状态。这时候需要清除缓存以避免这种不一致。(本案例就是这种情况的实际例子)\n\n参考StackOverFlow 大神回答\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Java 安全笔记","url":"/p/10700/","content":"前言后端接口开发中，涉及到用户私密信息（用户名、密码）等，我们不能传输明文，必须使用加密方式传输。这次政府项目中，安全测试组提出了明文传输漏洞，抽空研究了下 Java 加解密相关知识，记录下。\n\n\n散列函数Java 提供了一个名为 MessageDigest 的类，它属于 java.security 包。 此类支持诸如 SHA-1，SHA 256，MD5 之类的算法，以将任意长度的消息转换为信息摘要。\n散列函数返回的值称为信息摘要或简称散列值。 下图说明了散列函数。\n\n要使用散列函数加密数据，我们通常按照以下步骤执行：\n创建 MessageDigest 对象MessageDigest md = MessageDigest.getInstance(\"MD5\");\n\n\nMessageDigest 提供了 getInstance 静态方法来获得 MessageDigest 实例，支持的类型可参考 Wiki-SHA 家族\n\n将数据传递给创建的 MessageDigest 对象md.update(\"gcdd1993\".getBytes());\n\n生成消息摘要byte[] digest = md.digest();\n\n通常我们会将其转换为 Hex 字符串StringBuffer hexString = new StringBuffer();for (byte aDigest : digest) {    hexString.append(Integer.toHexString(0xFF &amp; aDigest));}System.out.println(\"Hex format : \" + hexString.toString());\n\n消息认证码\nMAC (消息认证码) 算法是一种对称密钥加密技术，用于提供消息认证。要建立 MAC 过程，发送方和接收方共享对称密钥 K。\n\n实质上，MAC 是在基础消息上生成的加密校验和，它与消息一起发送以确保消息验证。\n使用 MAC 进行身份验证的过程如下图所示\n\n在 Java 中，javax.crypto 包的 Mac 类提供了消息认证代码的功能。按照以下步骤使用此类创建消息身份验证代码。\n创建 KeyGenerator 对象KeyGenerator keyGen = KeyGenerator.getInstance(\"DES\");\n\n\nKeyGenerator 支持以下类型：\n\nAES (128)\nDES (56)\nDESede (168)\nHmacSHA1\nHmacSHA256\n\n\n创建 SecureRandom 对象SecureRandom secureRandom = new SecureRandom();\n\n初始化 KeyGeneratorkeyGen.init(secureRandom);\n\n生成密钥Key key = keyGen.generateKey();\n\n使用密钥初始化 Mac 对象Mac mac = Mac.getInstance(\"HmacMD5\");mac.init(key);\n\n\nMac 支持以下类型：\n\nHmacMD5\nHmacSHA1\nHmacSHA256\n\n\n完成 mac 操作String msg = \"gcdd1993\";byte[] bytes = msg.getBytes();byte[] macResult = mac.doFinal(bytes);\n\n数字签名\n数字签名允许验证签名的作者，日期和时间，验证消息内容。 它还包括用于其他功能的身份验证功能。\n\n\n优点\n认证\n\n数字签名有助于验证消息来源。 \n\n\n完整性\n\n邮件签名后，邮件中的任何更改都将使签名无效。\n\n\n不可否认\n\n通过此属性，任何已签署某些信息的实体都不能在以后拒绝签名。\n\n\n\n创建数字签名创建 KeyPairGenerator 对象\nKeyPairGenerator 类提供 getInstance() 方法，该方法接受表示所需密钥生成算法的 String 变量，并返回生成密钥的 KeyPairGenerator 对象。\n\nKeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(\"DSA\");\n\n初始化 KeyPairGenerator 对象\nKeyPairGenerator 类提供了一个名为 initialize() 的方法，该方法用于初始化密钥对生成器。 此方法接受表示密钥大小的整数值。\n\nkeyPairGen.initialize(2048);\n\n生成 KeyPair\n使用 generateKeyPair() 方法生成密钥对\n\nKeyPair pair = keyPairGen.generateKeyPair();\n\n从密钥对中获取私钥PrivateKey privateKey = pair.getPrivate();\n\n创建签名对象\nSignature 类的 getInstance() 方法接受表示所需签名算法的字符串参数，并返回相应的 Signature 对象。\nSignature 支持以下类型：\n\nSHA1withDSA\nSHA1withRSA\nSHA256withRSA\n\n\nSignature sign = Signature.getInstance(\"SHA256withDSA\");\n\n初始化签名对象sign.initSign(privateKey);\n\n将数据添加到 Signature 对象String msg = \"gcdd1993\";sign.update(msg.getBytes());\n\n计算签名byte[] signature = sign.sign();\n\n验证签名\n我们创建签名后，通常可以将私钥发送到客户端，以进行签名操作。服务端保存公钥，以进行签名验证\n\n初始化签名对象以进行验证\n使用公钥初始化签名对象\n\nsign.initVerify(pair.getPublic());\n\n更新要验证的数据sign.update(msg.getBytes());\n\n验证签名boolean verify = sign.verify(signature);Assert.assertTrue(verify);\n\n公私钥加解密数据\n可以使用 javax.crypto 包的 Cipher 类加密给定数据。 \n\n获取公私钥的步骤，与签名类似\nKeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(\"RSA\");keyPairGen.initialize(2048);KeyPair pair = keyPairGen.generateKeyPair();PublicKey publicKey = pair.getPublic();\n\n加密数据创建一个 Cipher 对象\nCipher 类的 getInstance() 方法接受表示所需转换的 String 变量，并返回实现给定转换的 Cipher 对象。\nCipher 支持以下类型：\n\nAES/CBC/NoPadding (128)\nAES/CBC/PKCS5Padding (128)\nAES/ECB/NoPadding (128)\nAES/ECB/PKCS5Padding (128)\nDES/CBC/NoPadding (56)\nDES/CBC/PKCS5Padding (56)\nDES/ECB/NoPadding (56)\nDES/ECB/PKCS5Padding (56)\nDESede/CBC/NoPadding (168)\nDESede/CBC/PKCS5Padding (168)\nDESede/ECB/NoPadding (168)\nDESede/ECB/PKCS5Padding (168)\nRSA/ECB/PKCS1Padding (1024, 2048)\nRSA/ECB/OAEPWithSHA-1AndMGF1Padding (1024, 2048)\nRSA/ECB/OAEPWithSHA-256AndMGF1Padding (1024, 2048)\n\n\nCipher cipher = Cipher.getInstance(\"RSA/ECB/PKCS1Padding\");\n\n使用公钥初始化 Cipher 对象\nCipher 类的 init() 方法接受两个参数，一个表示操作模式的整数参数 (加密 / 解密) 和一个表示公钥的 Key 对象。\n\ncipher.init(Cipher.ENCRYPT_MODE, publicKey);\n\n将数据添加到 Cipher 对象\nCipher 类的 update() 方法接受表示要加密的数据的字节数组，并使用给定的数据更新当前对象。\n\nString msg = \"gcdd1993\";cipher.update(msg.getBytes());\n\n加密数据byte[] cipherText = cipher.doFinal();\n\n解密数据使用私钥初始化 Cipher 对象cipher.init(Cipher.DECRYPT_MODE, pair.getPrivate());\n\n解密数据byte[] decipheredText = cipher.doFinal(cipherText);Assert.assertEquals(msg, new String(decipheredText));\n\n第三方类库\n前后端适用且应用广泛的是 Crypto-JS, 使用 Crypto-JS 可以非常方便地在 JavaScript 进行 MD5、SHA1、SHA2、SHA3、RIPEMD-160 哈希散列，进行 AES、DES、Rabbit、RC4、Triple DES 加解密。\n\nAES 加密\n高级加密标准（英语：Advanced Encryption Standard，缩写：AES），在密码学中又称 Rijndael 加密法，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的 DES，已经被多方分析且广为全世界所使用。\n\n一般来说，我们可以在服务端随机生成密钥，然后将密钥发送给客户端进行加密，上传密文到服务端，服务端进行解密。\n本文只讨论 Java 的 AES 加解密方式。\n引入 Jar 包compile group: 'org.webjars.npm', name: 'crypto-js', version: '3.1.8'\n\n生成密钥Random random = new Random();byte[] key = new byte[16];random.nextBytes(key);SecretKeySpec keySpec = new SecretKeySpec(key, \"AES\");\n\n生成偏移量byte[] iv = new byte[16];random.nextBytes(iv);IvParameterSpec ivSpec = new IvParameterSpec(iv);\n\n创建 Cipher 对象Cipher cipher = Cipher.getInstance(\"AES/CBC/PKCS5Padding\");\n\n初始化 Cipher 为加密工作过程cipher.init(Cipher.ENCRYPT_MODE, keySpec, ivSpec);\n\n加密byte[] original = cipher.doFinal(encrypted1);\n\nAES 解密初始化 Cipher 为解密工作过程cipher.init(Cipher.DECRYPT_MODE, keySpec, ivSpec);\n\n解密byte[] bytes = cipher.doFinal(original);Assert.assertEquals(data, new String(bytes, StandardCharsets.UTF_8));\n\nAES 加解密总结实际项目中，可以按照以下方式实现对称加密\n\n服务端提供一个接口，该接口负责随机生成 key（密码）和 iv（偏移量），并将其存入 redis（设置超时时间）\n客户端调用接口，获得 key 和 iv 以及一个 redis_key，进行数据加密，将加密后的数据以及 redis_key 传到服务端\n服务端使用 redis_key 获得 key 和 iv，进行解密\n\n总结在 Java EE 安全里，主要是进行客户端加密，以及服务端解密的过程来实现数据安全传输的目的。在这个过程中，特别要注意以下几点：\n\n随机性：加密方式不可单一，可通过更换 Cipher.getInstance() 的 String 值来随机生成加密工人进行加密。\n保密性：加密使用的密钥或者偏移量等，需要使用超时、模糊目的等手段进行隐藏，加大破解成本。\n\n没有完全有效的加密，但是只要做到破解成本大于加密成本，就是有效的加密。这样，我们可以不断地更换加密方式达到我们想要的效果。\n👉 代码仓库\n","categories":["个人笔记"],"tags":["Java","安全"]},{"title":"Gradle 配置片段","url":"/p/9306/","content":"本文介绍 Gradle 常用配置，本人很早就开始使用 Gradle 代替 Maven 作为项目构建工具，Gradle 相较于 Maven 繁琐的 XML 配置来说，确实更为先进，依托于 Groovy 脚本的强大，也更加灵活。\n\n\n1、Jetbrains IDEA 设置 Gradle 不自动创建模块\n在 IDEA 某个版本之后，创建或导入 Gradle 项目的时候，无法再取消勾选 Create separate module per source set，多少有点强加的意思，因为自动为每个资源文件夹创建一个目录，模块多了以后，会出现意料之外的错误，比如某个依赖无法加载等问题\n\n\n编辑.idea/gradle.xml，加上一行 &lt;option name=\"resolveModulePerSourceSet\" value=\"false\" /&gt; 就行\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project version=\"4\"&gt;  &lt;component name=\"GradleMigrationSettings\" migrationVersion=\"1\" /&gt;  &lt;component name=\"GradleSettings\"&gt;    &lt;option name=\"linkedExternalProjectsSettings\"&gt;      &lt;GradleProjectSettings&gt;        &lt;option name=\"delegatedBuild\" value=\"true\" /&gt;        &lt;option name=\"testRunner\" value=\"GRADLE\" /&gt;        &lt;option name=\"distributionType\" value=\"LOCAL\" /&gt;        &lt;option name=\"externalProjectPath\" value=\"$PROJECT_DIR$\" /&gt;        &lt;option name=\"gradleHome\" value=\"$PROJECT_DIR$/../../../../DevTools/Gradle/gradle-6.8.3\" /&gt;        &lt;option name=\"gradleJvm\" value=\"#JAVA_HOME\" /&gt;        &lt;option name=\"resolveModulePerSourceSet\" value=\"false\" /&gt;      &lt;/GradleProjectSettings&gt;    &lt;/option&gt;  &lt;/component&gt;&lt;/project&gt;\n\n然后重新导入 Gradle 项目\n2、使用阿里云 Maven 仓库加速依赖下载\nGradle 6.8.3 即以上适用\n\n编辑 settings.gradle\npluginManagement {    repositories {        mavenLocal()        repositories {            maven { url 'https://maven.aliyun.com/repository/google' }            maven { url 'https://maven.aliyun.com/repository/gradle-plugin' }            maven { url 'https://maven.aliyun.com/repository/public/' }        }        mavenCentral()        gradlePluginPortal()    }}dependencyResolutionManagement {    repositories {        mavenLocal()        maven { url = uri(\"https://maven.aliyun.com/repository/central\") } // central        maven { url = uri(\"https://maven.aliyun.com/repository/public\") } // jcenter &amp; public        maven { url = uri(\"https://maven.aliyun.com/repository/google\") } // google        maven { url = uri(\"https://maven.aliyun.com/repository/spring\") } // spring        maven { url = uri(\"https://maven.aliyun.com/repository/spring-plugin\") } // spring plugin        maven { url = uri(\"https://maven.aliyun.com/repository/grails-core\") } // spring plugin    }}rootProject.name = 'my-prject'\n\n3、maven-publish.gradle// jar添加以下插件//plugins {//    id 'java-library'//    id 'maven-publish'//}// bom添加以下插件//plugins {//    id 'java-platform'//    id 'maven-publish'//}def artifactory = 'https://packages.aliyun.com/maven/repository/'def releasePath = \"xxx\"def snapshotsPath = \"xxx\"def mavenName = 'my-project'afterEvaluate { Project project -&gt;    if (project.plugins.hasPlugin(\"maven-publish\")) {        if (project.plugins.hasPlugin(\"java\")) {            java {                withSourcesJar()            }        }        publishing {            repositories {                maven {                    name \"${mavenName}\"                    url = \"${artifactory}${(version.endsWith('SNAPSHOT') ? snapshotsPath : releasePath)}\"                    credentials {                        username \"${maven_username}\"                        password \"${maven_password}\"                    }                    authentication {                        basic(BasicAuthentication)                    }                }            }            publications {                omac(MavenPublication) {                    from components.java                    versionMapping {                        usage('java-api') {                            fromResolutionOf('runtimeClasspath')                        }                        usage('java-runtime') {                            fromResolutionResult()                        }                    }                }            }        }    }}\n\n在 build.gradle 引入即可\napply from: \"${rootDir}/gradle/maven-publish.gradle\"\n\n4、docker.gradleafterEvaluate {    if (pluginManager.hasPlugin(\"application\")) {        task archiveDeps(type: Tar) { task -&gt;            def dependencies = task.project.configurations.runtimeClasspath.fileCollection { true }            from dependencies.sort()            compression = Compression.GZIP            archiveFileName = \"dep-libs.tar.gz\"            from jar.archiveFile            destinationDirectory = file(\"$buildDir/docker\")        }        task copyDeps(type: Copy) {            dependsOn(archiveDeps)            from tarTree(resources.gzip(\"$buildDir/docker/dep-libs.tar.gz\"))            into \"$buildDir/docker/libs/\"        }        task copyApp(type: Copy) {            dependsOn(jar, startScripts)            from \"$buildDir/scripts/\"            from jar.outputs            into \"$buildDir/docker/\"        }        task prepare() {            dependsOn(copyApp, copyDeps)        }    }}\n\n添加插件 application\nplugins {    id \"application\"}\n\n并配合以下 Dockerfile\n# First stage: complete build environmentFROM registry.cn-shanghai.aliyuncs.com/halmawork/gradle:6.8.3-jdk11-openj9 AS builderARG MAVEN_USERNAMEARG MAVEN_PASSWORDARG MODULERUN echo \"org.gradle.daemon=false\" &gt;&gt; ~/.gradle/gradle.properties \\    &amp;&amp; echo \"org.gradle.parallel=true\" &gt;&gt; ~/.gradle/gradle.properties \\    &amp;&amp; echo \"maven_username=$MAVEN_USERNAME\" &gt;&gt; ~/.gradle/gradle.properties \\    &amp;&amp; echo \"maven_password=$MAVEN_PASSWORD\" &gt;&gt; ~/.gradle/gradle.propertiesWORKDIR /builderCOPY . /builder# package jarRUN gradle :$MODULE:clean :$MODULE:prepare# Second stage: minimal runtime environmentFROM adoptopenjdk/openjdk11:jreENV MODULE \"\"ENV TZ=Asia/ShanghaiLABEL name=$MODULERUN ln -fs /usr/share/zoneinfo/$TZ /etc/localtime \\    &amp;&amp; dpkg-reconfigure -f noninteractive tzdataWORKDIR /appCOPY --from=builder /builder/build/docker/libs/*.jar /app/lib/COPY --from=builder /builder/build/docker/$MODULE /app/bin/runCOPY --from=builder /builder/build/docker/$MODULE-*.jar /app/lib/EXPOSE 8080CMD [\"/app/bin/run\"]\n\n打包 docker 镜像\n$ docker build -f gradle/Dockerfile -t gcdd1993/demo:latest-snapshot .\n\n5、支持 kotlingradle.properties\nkotlin_version=1.5.10\n\nbuild.gradle\nplugins {    id \"org.jetbrains.kotlin.jvm\" version \"${kotlin_version}\" apply false // jvm 插件    id \"org.jetbrains.kotlin.plugin.spring\" version \"${kotlin_version}\" apply false // spring 插件，allopen 插件    id \"org.jetbrains.kotlin.kapt\" version \"${kotlin_version}\" apply false // kapt，代替annotationProcessor    id \"org.jetbrains.kotlin.plugin.noarg\" version \"${kotlin_version}\" apply false // 用于自定义注解生成no arg constructor}apply plugin: \"org.jetbrains.kotlin.jvm\"apply plugin: \"org.jetbrains.kotlin.plugin.spring\"apply plugin: \"org.jetbrains.kotlin.kapt\"apply plugin: \"org.jetbrains.kotlin.plugin.noarg\"compileKotlin {    kotlinOptions {        freeCompilerArgs = [\"-Xjsr305=warn\"]        jvmTarget = \"11\"    }}noArg {    annotation(\"io.github.gcdd1993.NoArg\") // 该注解注释的class，会生成NoArg Constructor}\n\n6、限制项目 JDK 版本\n对于多人协同时，很有用，避免因为 JDK 版本不一导致的各种问题\n\ndef javaVersion = System.getProperty(\"java.version\")if (!javaVersion.startsWith(\"11\")) {    throw new RuntimeException(\"Incompatible JRE version: \" + javaVersion + \". Use JRE 11 instead.\")}\n\n7、为项目编译 jar 文件添加项目编译信息plugins {    id \"org.springframework.boot\" version \"${spring_boot_version}\" apply false    id \"com.gorylenko.gradle-git-properties\" version \"2.2.4\"}apply plugin: \"io.spring.dependency-management\"apply plugin: \"org.springframework.boot\"jar {    enabled = true    manifest {        attributes(            \"Implementation-Title\": project.name,            \"Implementation-Version\": project.version,            \"Built-By\": System.properties[\"user.name\"],            \"Build-Timestamp\": new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\").format(new Date()),            \"Created-By\": \"Gradle ${gradle.gradleVersion}\",            \"Build-Jdk\": \"${System.properties[\"java.version\"]} (${System.properties[\"java.vendor\"]} ${System.properties[\"java.vm.version\"]})\",            \"Build-OS\": \"${System.properties[\"os.name\"]} ${System.properties[\"os.arch\"]} ${System.properties[\"os.version\"]}\"        )    }}springBoot {    buildInfo()}\n\n如果是 Spring Boot 项目，还可以通过以下方法，在程序启动时，打印出编译信息\n/** * 在应用启动时，打印应用基本信息 * * @author gcdd1993 * @date 2021/2/20 * @since 1.0.0 */class AppInfoPreviewAutoConfiguration {    private val log = LoggerFactory.getLogger(javaClass)    @Autowired(required = false)    private val gitProperties: GitProperties? = null    @Autowired(required = false)    private val buildProperties: BuildProperties? = null    @Value(\"\\${spring.application.name}\")    private val name: String? = null    @EventListener(ApplicationStartedEvent::class)    fun onBootUp(event: ApplicationStartedEvent?) {        log.info(\"{} Started.\", name)        if (buildProperties != null) {            log.info(\"build.name : {}\", buildProperties.name)            log.info(\"build.artifact : {}\", buildProperties.artifact)            log.info(\"build.group : {}\", buildProperties.group)            log.info(\"build.version : {}\", buildProperties.version)            log.info(\"build.time : {}\", buildProperties.time.atZone(ZoneId.systemDefault()).toLocalDateTime())        } else {            log.warn(\"cannot find any build properties file from this project. please reference: https://stackoverflow.com/questions/47283048/how-to-capture-build-info-using-gradle-and-spring-boot.\")        }        if (gitProperties != null) {            log.info(\"commit.branch : {}\", gitProperties.branch)            log.info(\"commit.commit.id : {}\", gitProperties.commitId)            log.info(\"commit.commit.time : {}\", gitProperties.commitTime.atZone(ZoneId.systemDefault()).toLocalDateTime())        } else {            log.warn(\"cannot find any git properties file from this project. please add gradle plugin: https://plugins.gradle.org/plugin/com.gorylenko.gradle-git-properties.\")        }    }}\n\n未完待续。。。\n","categories":["个人笔记"],"tags":["Gradle"]},{"title":"Jackson 使用指南","url":"/p/56384/","content":"从事 JAVA 开发工作以来，一直都离不开 Jackson 的序列化反序列化，对于 Jackson 的使用也一直处于够用但不深入的状态，下面是日常使用过程中对 Jackson 的总结。\n\n\nJackson 常用注解序列化注解@JsonAnyGetter\n像普通属性一样序列化 Map\n\npublic class ExtendableBean {    public String name;    private Map&lt;String, String&gt; properties;     @JsonAnyGetter    public Map&lt;String, String&gt; getProperties() {        return properties;    }}\n\n序列化示例：\n{    \"name\":\"My bean\",    \"attr2\":\"val2\",    \"attr1\":\"val1\"}\n\n@JsonGetter\n将指定的方法标记为 getter 方法。可以用来代替 @JsonProperty\n\npublic class MyBean {    public int id;    private String name;     @JsonGetter(\"name\")    public String getTheName() {        return name;    }}\n\n序列化示例：\n{    \"id\": 1,    \"name\":\"My bean\"}\n\n@JsonPropertyOrder\n用在类上，在序列化的时候自定义属性输出顺序\n\n@JsonPropertyOrder({ \"name\", \"id\" })public class MyBean {    public int id;    public String name;}\n\n序列化示例：\n{    \"name\":\"My bean\",    \"id\": 1}\n\n@JsonRawValue\n完全按照原样序列化属性的值\n\npublic class RawBean {    public String name;     @JsonRawValue    public String json;}\n\n例如：\nRawBean bean = new RawBean(\"My bean\", \"{\\\"attr\\\":false}\");\n\n将序列化为：\n{    \"name\":\"My bean\",    \"json\":{        \"attr\":false    }}\n\n而不是：\n{    \"name\":\"My bean\",    \"json\":\"{\\\"attr\\\":false}\"}\n\n@JsonValue\n定义整个实体的序列化方法，Jackson 将会使用该方法的输出作为序列化输出。\n\npublic enum TypeEnumWithValue {    TYPE1(1, \"Type A\"), TYPE2(2, \"Type 2\");     private Integer id;    private String name;     // standard constructors     @JsonValue    public String getName() {        return name;    }}\n\n序列化示例：\n{  \"name\": \"Type 2\"}\n\n@JsonRootName\n如果需要将实体包装一层，可以使用 @JsonRootName 来指定根包装器的名称\n\n@JsonRootName(value = \"user\")public class UserWithRoot {    public int id;    public String name;}\n\n序列化示例：\n{    \"user\": {        \"id\": 1,        \"name\": \"John\"    }}\n\n如果不用该注解，将会序列化为：\n{    \"id\": 1,    \"name\": \"John\"}\n\n@JsonSerialize\n用于指定自定义序列化器来序列化实体\n\npublic class Event {    public String name;     @JsonSerialize(using = CustomDateSerializer.class)    public Date eventDate;}\n\n自定义序列化器如下：\npublic class CustomDateSerializer extends StdSerializer&lt;Date&gt; {     private static SimpleDateFormat formatter       = new SimpleDateFormat(\"dd-MM-yyyy hh:mm:ss\");     public CustomDateSerializer() {         this(null);     }      public CustomDateSerializer(Class&lt;Date&gt; t) {        super(t);     }     @Override    public void serialize(      Date value, JsonGenerator gen, SerializerProvider arg2)       throws IOException, JsonProcessingException {        gen.writeString(formatter.format(value));    }}\n\n输出示例：\n{  \"name\": \"test\",  \"eventDate\": \"20-12-2014 02:30:00\"}\n\n反序列化注解@JsonCreator\n指定反序列化使用的构造函数或方法\n\n待反序列化 Json 示例：\n{    \"id\":1,    \"theName\":\"My bean\"}\n\npublic class BeanWithCreator {    public int id;    public String name;     @JsonCreator    public BeanWithCreator(@JsonProperty(\"id\") int id, @JsonProperty(\"theName\") String name) {        this.id = id;        this.name = name;    }}\n\n@JacksonInject\n指定某个字段从注入赋值，而不是从 Json\n\npublic class BeanWithInject {    @JacksonInject    public int id;         public String name;}\n\n示例用法：\nString json = \"{\\\"name\\\":\\\"My bean\\\"}\"; InjectableValues inject = new InjectableValues.Std()  .addValue(int.class, 1);BeanWithInject bean = new ObjectMapper().reader(inject)  .forType(BeanWithInject.class)  .readValue(json);\n\n@JsonAnySetter\n在反序列化时，将 Map 当成普通属性\n\n待反序列化 Json：\n{    \"name\":\"My bean\",    \"attr2\":\"val2\",    \"attr1\":\"val1\"}\n\npublic class ExtendableBean {    public String name;    private Map&lt;String, String&gt; properties;     @JsonAnySetter    public void add(String key, String value) {        properties.put(key, value);    }}\n\nproperties 字段的值将会是由 attr2 -&gt; val2,attr1 -&gt; val1 组成的键值对。\n@JsonSetter\n将方法标记为 setter 方法，可以指定属性名称\n\npublic class MyBean {    public int id;    private String name;     @JsonSetter(\"name\")    public void setTheName(String name) {        this.name = name;    }}\n\n@JsonDeserialize\n用于指定自定义反序列化器来反序列化实体\n\npublic class Event {    public String name;     @JsonDeserialize(using = CustomDateDeserializer.class)    public Date eventDate;}\n\n对应的反序列化器：\npublic class CustomDateDeserializer  extends StdDeserializer&lt;Date&gt; {     private static SimpleDateFormat formatter      = new SimpleDateFormat(\"dd-MM-yyyy hh:mm:ss\");     public CustomDateDeserializer() {         this(null);     }      public CustomDateDeserializer(Class&lt;?&gt; vc) {         super(vc);     }     @Override    public Date deserialize(      JsonParser jsonparser, DeserializationContext context)       throws IOException {                 String date = jsonparser.getText();        try {            return formatter.parse(date);        } catch (ParseException e) {            throw new RuntimeException(e);        }    }}\n\nJackson 设置属性是否参与序列化@JsonIgnoreProperties\n在类上指定要忽略的属性\n\n@JsonIgnoreProperties({ \"id\" })public class BeanWithIgnore {    public int id;    public String name;}\n\n@JsonIgnore\n在具体属性上忽略，使其不参与序列化过程\n\npublic class BeanWithIgnore {    @JsonIgnore    public int id;     public String name;}\n\n与 @JsonIgnoreProperties 是等效的。\n@JsonIgnoreType\n用在类上，将忽略该类所有属性\n\npublic class User {    public int id;    public Name name;     @JsonIgnoreType    public static class Name {        public String firstName;        public String lastName;    }}\n\n@JsonInclude\n用于排除值为 empty/null/default 的属性\n\n@JsonInclude(Include.NON_NULL)public class MyBean {    public int id;    public String name;}\n\n@JsonAutoDetect\n强制序列化私有属性，不管它有没有 getter 方法\n\n@JsonAutoDetect(fieldVisibility = Visibility.ANY)public class PrivateBean {    private int id;    private String name;}\n\nJackson 处理多态一般都是组合起来使用，有下面三个注解：\n\n@JsonTypeInfo\n指定序列化中包含的类型信息的详细信息\n\n\n@JsonSubTypes\n指定带注释类型的子类型\n\n\n@JsonTypeName\n指定用于带注释的类的逻辑类型名称\n\n\n\npublic class Zoo {    public Animal animal;     @JsonTypeInfo(      use = JsonTypeInfo.Id.NAME,       include = As.PROPERTY,       property = \"type\")    @JsonSubTypes({        @JsonSubTypes.Type(value = Dog.class, name = \"dog\"),        @JsonSubTypes.Type(value = Cat.class, name = \"cat\")    })    public static class Animal {        public String name;    }     @JsonTypeName(\"dog\")    public static class Dog extends Animal {        public double barkVolume;    }     @JsonTypeName(\"cat\")    public static class Cat extends Animal {        boolean likesCream;        public int lives;    }}\n\n上述例子中，指定属性 type 为判断具体子类的依据，例如：type=dog，将被序列化为 Dog 类型。\nJackson 通用注解（序列化反序列化都生效）@JsonProperty\n指定 JSON 中的属性名称\n\npublic class MyBean {    public int id;    private String name;     @JsonProperty(\"name\")    public void setTheName(String name) {        this.name = name;    }     @JsonProperty(\"name\")    public String getTheName() {        return name;    }}\n\n@JsonFormat\n用于在序列化日期 / 时间值时指定格式。\n\npublic class Event {    public String name;     @JsonFormat(      shape = JsonFormat.Shape.STRING,      pattern = \"dd-MM-yyyy hh:mm:ss\")    public Date eventDate;}\n\n@JsonUnwrapped\n将对象中所有的属性与当前平级，不太好描述，简单说就是拆开包装。\n\npublic class UnwrappedUser {    public int id;     @JsonUnwrapped    public Name name;     public static class Name {        public String firstName;        public String lastName;    }}\n\n序列化示例：\n{    \"id\":1,    \"firstName\":\"John\",    \"lastName\":\"Doe\"}\n\n如果不加 @JsonUnwrapped 注解，将被序列化为：\n{    \"id\":1,    \"name\": {        \"firstName\":\"John\",        \"lastName\":\"Doe\"    }}\n\n@JsonView\n指定视图，类似分组进行序列化 / 反序列化\n\n定义视图：\npublic class Views {    public static class Public {}    public static class Internal extends Public {}}\n\n定义实体：\npublic class Item {    @JsonView(Views.Public.class)    public int id;     @JsonView(Views.Public.class)    public String itemName;     @JsonView(Views.Internal.class)    public String ownerName;}\n\n序列化示例：\nString result = new ObjectMapper()  .writerWithView(Views.Public.class)  .writeValueAsString(item);\n\n这时，将只会序列化 id 和 itemName 字段\n@JsonManagedReference, @JsonBackReference\n@JsonManagedReference 和 @JsonBackReference 注释用于处理父 / 子关系并解决循环问题。\n\n例如，有两个相互引用的类：\npublic class ItemWithRef {    public int id;    public String itemName;     @JsonManagedReference    public UserWithRef owner;}\n\npublic class UserWithRef {    public int id;    public String name;     @JsonBackReference    public List&lt;ItemWithRef&gt; userItems;}\n\n不加注解，会循环调用，导致内存溢出，这时候可以使用 @JsonManagedReference 和 @JsonBackReference 来避免内存溢出。\n@JsonIdentityInfo\n用于指定在序列化 / 反序列化值时使用对象标识，例如，处理无限递归类型的问题。\n\n@JsonIdentityInfo(  generator = ObjectIdGenerators.PropertyGenerator.class,  property = \"id\")public class ItemWithIdentity {    public int id;    public String itemName;    public UserWithIdentity owner;}\n\n@JsonFilter\n指定序列化期间要使用的过滤器。\n\n@JsonFilter(\"myFilter\")public class BeanWithFilter {    public int id;    public String name;}\n\n示例代码：\nBeanWithFilter bean = new BeanWithFilter(1, \"My bean\");FilterProvider filters   = new SimpleFilterProvider().addFilter(    \"myFilter\",     SimpleBeanPropertyFilter.filterOutAllExcept(\"name\"));String result = new ObjectMapper()  .writer(filters)  .writeValueAsString(bean);\n\n自定义 Jackson 注解可以使用 @JacksonAnnotationsInside 来开发自定义注解\n@Retention(RetentionPolicy.RUNTIME)    @JacksonAnnotationsInside    @JsonInclude(Include.NON_NULL)    @JsonPropertyOrder({ \"name\", \"id\", \"dateCreated\" })    public @interface CustomAnnotation {}\n\n如何使用自定义注解：\n@CustomAnnotationpublic class BeanWithCustomAnnotation {    public int id;    public String name;    public Date dateCreated;}\n\n自定义注解可以增强代码复用，把一些通用的 Jackson 注解组合起来，形成一个新注解，新注解可以代替组合的注解。\nJackson MixIn 注解\n动态地为某些类型增加统一的 Jackson 注解\n\n实体：\npublic class Item {    public int id;    public String itemName;    public User owner;}\n\nMixIn 类：\n@JsonIgnoreTypepublic class MyMixInForIgnoreType {}\n\n我们可以动态地让 User 类型不参与序列化：\nItem item = new Item(1, \"book\", null);ObjectMapper mapper = new ObjectMapper();mapper.addMixIn(User.class, MyMixInForIgnoreType.class);result = mapper.writeValueAsString(item);\n\n禁用 Jackson 注解假设我们有一个带 Jackson 注解的实体：\n@JsonInclude(Include.NON_NULL)@JsonPropertyOrder({ \"name\", \"id\" })public class MyBean {    public int id;    public String name;}\n\n我们可以这样来禁用该实体上的所有 Jackson 注解：\nMyBean bean = new MyBean(1, null);ObjectMapper mapper = new ObjectMapper();mapper.disable(MapperFeature.USE_ANNOTATIONS);\n\nJackson 的 ObjectMapper 用法java 类 转换为 json可以直接序列化为 Json 字符串：\nobjectMapper.writeValueAsString(car);\n\n或者，可以序列化到文件，文件内容是 Json 字符串：\nobjectMapper.writeValue(new File(\"target/car.json\"), car);\n\njson 转换为 java 类从字符串：\nString json = \"{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }\";objectMapper.readValue(json, Car.class); \n\n从文件：\nobjectMapper.readValue(new File(\"target/json_car.json\"), Car.class);\n\n从 URL：\nobjectMapper.readValue(new URL(\"target/json_car.json\"), Car.class);\n\njson 转换为 Jackson JsonNodeString json = \"{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"FIAT\\\" }\";JsonNode jsonNode = objectMapper.readTree(json);String color = jsonNode.get(\"color\").asText();// Output: color -&gt; Black\n\njson 转换为 java 集合String jsonCarArray =   \"[{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }, { \\\"color\\\" : \\\"Red\\\", \\\"type\\\" : \\\"FIAT\\\" }]\";List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;(){});\n\njson 转换为 MapString json = \"{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }\";Map&lt;String, Object&gt; map = objectMapper.readValue(json, new TypeReference&lt;Map&lt;String,Object&gt;&gt;(){});\nObjectMapper 的常用配置忽略不识别的字段（json 属性与目标实体存在属性上的差异）：\nobjectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n\n允许原始值为 null：\nobjectMapper.configure(DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES, false);\n\n允许将枚举序列化 / 反序列化为数字：\nobjectMapper.configure(DeserializationFeature.FAIL_ON_NUMBERS_FOR_ENUMS, false);\n\n配置自定义序列化 / 反序列化器假设有一个序列化器：\npublic class CustomCarSerializer extends StdSerializer&lt;Car&gt; {         public CustomCarSerializer() {        this(null);    }     public CustomCarSerializer(Class&lt;Car&gt; t) {        super(t);    }     @Override    public void serialize(      Car car, JsonGenerator jsonGenerator, SerializerProvider serializer) {        jsonGenerator.writeStartObject();        jsonGenerator.writeStringField(\"car_brand\", car.getType());        jsonGenerator.writeEndObject();    }}\n\n一个反序列化器：\npublic class CustomCarDeserializer extends StdDeserializer&lt;Car&gt; {         public CustomCarDeserializer() {        this(null);    }     public CustomCarDeserializer(Class&lt;?&gt; vc) {        super(vc);    }     @Override    public Car deserialize(JsonParser parser, DeserializationContext deserializer) {        Car car = new Car();        ObjectCodec codec = parser.getCodec();        JsonNode node = codec.readTree(parser);                 // try catch block        JsonNode colorNode = node.get(\"color\");        String color = colorNode.asText();        car.setColor(color);        return car;    }}\n\n用 ObjectMapper 使用他们：\n//添加自定义序列化器module.addSerializer(Car.class, new CustomCarSerializer());//添加自定义反序列化器module.addDeserializer(Car.class, new CustomCarDeserializer());\n\n处理日期格式化ObjectMapper objectMapper = new ObjectMapper();DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm a z\");objectMapper.setDateFormat(df);\n\n处理集合反序列化为数组：\nString jsonCarArray =   \"[{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }, { \\\"color\\\" : \\\"Red\\\", \\\"type\\\" : \\\"FIAT\\\" }]\";ObjectMapper objectMapper = new ObjectMapper();objectMapper.configure(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY, true);Car[] cars = objectMapper.readValue(jsonCarArray, Car[].class);\n\n反序列化为集合：\nString jsonCarArray =   \"[{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }, { \\\"color\\\" : \\\"Red\\\", \\\"type\\\" : \\\"FIAT\\\" }]\";ObjectMapper objectMapper = new ObjectMapper();List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;(){});\n\nObjectMapper 的基本用法ObjectMapper 可以通过 configure 方法设置全局序列化 / 反序列化行为，例如：objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n\n常用的一些设置：\n\nDeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES：忽略不识别的字段\nDeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES：允许使用属性的默认值进行反序列化\nDeserializationFeature.FAIL_ON_NUMBERS_FOR_ENUMS：允许将枚举值序列化 / 反序列化为数字\n\n注册自定义序列化 / 反序列化程序//创建一个模块SimpleModule module = new SimpleModule(\"CustomCarSerializer\", new Version(1, 0, 0, null, null, null));//将自定义序列化/反序列化程序注册到模块module.addSerializer(Car.class, new CustomCarSerializer());//module.addDeserializer(Car.class, new CustomCarDeserializer());//注册模块mapper.registerModule(module);\n\n处理日期格式DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm a z\");mapper.setDateFormat(df);\n\n处理集合处理数组String jsonCarArray = \"[{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }, { \\\"color\\\" : \\\"Red\\\", \\\"type\\\" : \\\"FIAT\\\" }]\";ObjectMapper objectMapper = new ObjectMapper();objectMapper.configure(DeserializationFeature.USE_JAVA_ARRAY_FOR_JSON_ARRAY, true);Car[] cars = objectMapper.readValue(jsonCarArray, Car[].class);\n\n处理集合String jsonCarArray = \"[{ \\\"color\\\" : \\\"Black\\\", \\\"type\\\" : \\\"BMW\\\" }, { \\\"color\\\" : \\\"Red\\\", \\\"type\\\" : \\\"FIAT\\\" }]\";ObjectMapper objectMapper = new ObjectMapper();List&lt;Car&gt; listCar = objectMapper.readValue(jsonCarArray, new TypeReference&lt;List&lt;Car&gt;&gt;(){});\n\nJackson 注解扩展@JsonIdentityReference\n使用指定的标识来序列化 Java 对象，而不是序列化整个对象\n\n例如：\n@JsonIdentityInfo(generator = ObjectIdGenerators.PropertyGenerator.class, property = \"id\")@JsonIdentityReference(alwaysAsId = true)public class BeanWithoutIdentityReference {    private int id;    private String name;}\n\n将被序列化为：\n1\n\n@JsonAppend\n运行在序列化时添加额外的属性\n\n@JsonAppend(attrs = {   @JsonAppend.Attr(value = \"version\") })public class BeanWithAppend {    private int id;    private String name;     // constructor, getters and setters}\n\n例如，我们在序列化时手动增加 version = 1.0 的属性\nBeanWithAppend bean = new BeanWithAppend(2, \"Bean With Append Annotation\");ObjectWriter writer = mapper.writerFor(BeanWithAppend.class).withAttribute(\"version\", \"1.0\");String jsonString = writer.writeValueAsString(bean);\n\n序列化结果：\n{    \"id\": 2,    \"name\": \"Bean With Append Annotation\",    \"version\": \"1.0\"}\n\n@JsonNaming\n指定序列化的时候属性命名方式\n\n有四种选项：\n\nKEBAB_CASE\n由连字符分割，例如：kebab-case\n\n\nLOWER_CASE\n所有的字母都转换为小写，例如：lowercase\n\n\nSNAKE_CASE\n所有的字母都转换为小写，并且由下划线分割，例如：snake_case\n\n\nUPPER_CAMEL_CASE\n所有名称元素，包括第一个元素，都以大写字母开头，后跟小写字母，并且没有分隔符，例如：UpperCamelCase\n\n\n\n使用举例：\n@JsonNaming(PropertyNamingStrategy.SnakeCaseStrategy.class)public class NamingBean {    private int id;    private String beanName;}\n\n@JsonPropertyDescription\n用于生成字段的描述信息\n\n例如，有下面一个实体：\npublic class PropertyDescriptionBean {    private int id;    @JsonPropertyDescription(\"This is a description of the name property\")    private String name;}\n\n我们可以输出该类的信息：\nSchemaFactoryWrapper wrapper = new SchemaFactoryWrapper();mapper.acceptJsonFormatVisitor(PropertyDescriptionBean.class, wrapper);JsonSchema jsonSchema = wrapper.finalSchema();String jsonString = mapper.writeValueAsString(jsonSchema);\n\n结果如下：\n{    \"type\": \"object\",    \"id\": \"urn:jsonschema:com:baeldung:jackson:annotation:extra:PropertyDescriptionBean\",    \"properties\":     {        \"name\":         {            \"type\": \"string\",            \"description\": \"This is a description of the name property\"        },         \"id\":         {            \"type\": \"integer\"        }    }}\n\n@JsonPOJOBuilder\n自定义生成器类，来控制 json 的反序列化行为\n\n@JsonPOJOBuilder 有两个属性：\n\nbuildMethodName\n将 JSON 字段绑定到 bean 的属性后，用于实例化预期 bean 的无参构造的名称。默认名称为 build。\n\n\nwithPrefix\n用于自动检测 JSON 和 bean 属性之间匹配的名称前缀。默认前缀为 with。\n\n\n\n假设我们要反序列化的 json 如下：\n{    \"id\": 5,    \"name\": \"POJO Builder Bean\"}\n\n对应的 pojo：\n@JsonDeserialize(builder = BeanBuilder.class)public class POJOBuilderBean {    private int identity;    private String beanName;     // constructor, getters and setters}\n\n对应的生成器：\n@JsonPOJOBuilder(buildMethodName = \"createBean\", withPrefix = \"construct\")public class BeanBuilder {    private int idValue;    private String nameValue;     public BeanBuilder constructId(int id) {        idValue = id;        return this;    }     public BeanBuilder constructName(String name) {        nameValue = name;        return this;    }     public POJOBuilderBean createBean() {        return new POJOBuilderBean(idValue, nameValue);    }}\n\n使用 ObjectMapper 反序列化：\nString jsonString = \"{\\\"id\\\":5,\\\"name\\\":\\\"POJO Builder Bean\\\"}\";POJOBuilderBean bean = mapper.readValue(jsonString, POJOBuilderBean.class);\n\n\n👉 代码仓库👉 Jackson JSON Tutorial\n","categories":["个人笔记"],"tags":["Jackson"]},{"title":"Java8 新特性一览表","url":"/p/55630/","content":"总览\nforEach () method in Iterable interface (Iterable 接口中的 forEach () 方法)\ndefault and static methods in Interfaces (接口中的默认和静态方法)\nFunctional Interfaces and Lambda Expressions (function 接口和 Lambda 表达式)\nJava Stream API for Bulk Data Operations on Collections (用于集合上的批量数据操作的 Java Stream API)\nJava Time API\nCollection API improvements\nConcurrency API improvements\nJava IO improvements\n\n\n\n1.forEach () method in Iterable interface (Iterable 接口中的 forEach () 方法)每当我们需要遍历 Collection 时，我们需要创建一个 Iterator，其目的是迭代，然后我们在循环中为 Collection 中的每个元素提供业务逻辑。如果没有正确使用迭代器，会抛出异常 ConcurrentModificationException。\nJava 8 在 java.lang.Iterable 接口中引入了 forEach 方法，这样在编写代码时我们只关注业务逻辑。 forEach 方法将 java.util.function.Consumer 对象作为参数，因此它有助于将我们的业务逻辑放在我们可以重用的单独位置。让我们通过简单的例子看看每个用法。\nList&lt;IntegermyList = new ArrayList&lt;Integer&gt;();for(int i=0; i&lt;10; i++) myList.add(i);//使用iteratorIterator&lt;Integeriterator = myList.iterator();while (iterator.hasNext()) {    Integer next = iterator.next();    System.out.println(\"Iterator Value::\" + next);}//foreach + 匿名类myList.forEach(new Consumer&lt;Integer&gt;() {    public void accept(Integer t) {        System.out.println(\"forEach anonymous class Value::\"+t);    }});//使用consumer 接口MyConsumer action = new MyConsumer();myList.forEach(action);//使用lambda表达式myList.forEach(System.out::println);\n\n2.default and static methods in Interfaces (接口中的默认和静态方法)jdk8 之前，interface 方法不能有实现，但是从 Java 8 开始，接口被增强为具有实现方法。我们可以使用 default 和 static 关键字来创建具有方法实现的接口。例如 Iterable 接口中的 forEach 方法实现是\ndefault void forEach(Consumer&lt;? super Taction) {    Objects.requireNonNull(action);    for (T t : this) {        action.accept(t);    }}\n\n示例代码创建一个接口\npublic interface MyInterface {    void show();    default void showA() {        System.out.println(\"我是接口默认方法\");    }    static void showB() {        System.out.println(\"我是接口静态方法\");    }}\n\n创建该接口实现类\npublic class MyClass implements MyInterface {    @Override    public void show() {        System.out.println(\"我是实现方法\");    }    //默认方法支持重写,不覆盖则执行接口的默认方法    @Override    public void showA() {        System.out.println(\"我覆盖了接口的默认方法\");    }    //静态方法不可以重写}\n\n测试\npublic class Test {    public static void main(String[] args) {        MyClass myClass = new MyClass();        myClass.show();        myClass.showA();        //通过类名.方法名调用接口静态方法        MyInterface.showB();    }}\n\n3.Functional Interfaces and Lambda Expressions（function 接口和 Lambda 表达式）如果你注意到上面的接口代码，你会注意到 @FunctionalInterface 注释。功能接口是 Java 8 中引入的新概念。只有一个抽象方法的接口就变成了功能接口。我们不需要使用 @FunctionalInterface 注释将接口标记为功能接口。 @FunctionalInterface 注释是一种避免在功能界面中意外添加抽象方法的工具。您可以将其视为 @Override 注释，并且最佳实践是使用它。实例：java8 的 runnable run 接口，带有一个抽象方法:\n@FunctionalInterfacepublic interface Runnable {    public abstract void run();}\n\n功能接口的主要好处之一是可以使用 lambda 表达式来实例化它们。我们可以使用匿名类实例化一个接口，但代码看起来很笨重。\n//使用匿名类实例化Runnable runnable = new Runnable() {    @Override    public void run() {        System.out.println(\"My Runnable\");    }};\n\n由于功能接口只有一个方法，因此 lambda 表达式可以很容易地提供方法实现。我们只需要提供方法参数和业务逻辑。例如，我们可以使用 lambda 表达式将上面的实现编写为：\n//使用lambda表达式Runnable runnable1 = () -System.out.println(\"My Runnable\");runnable.run();runnable1.run();\n\n如果在方法实现中有单个语句，我们也不需要花括号。例如，上面的 Interface1 匿名类可以使用 lambda 实例化，如下所示：\nInterface1 interface1 = (s) -System.out.println(s);interface1.method1(\"interface1 method\");\n\nlambda 表达式扩展Java 中的 Lambda 表达式通常使用 (argument) -(body) 语法书写，例如：(arg1, arg2...) -&gt; { body }(type1 arg1, type2 arg2...) -&gt; { body }\n\nLambda 表达式的结构\n一个 Lambda 表达式可以有零个或多个参数\n参数的类型既可以明确声明，也可以根据上下文来推断。例如：(int a) 与 (a) 效果相同\n所有参数需包含在圆括号内，参数之间用逗号相隔。例如：(a, b) 或 (int a, int b) 或 (String a, int b, float c)\n空圆括号代表参数集为空。例如：() -&gt; 42\n当只有一个参数，且其类型可推导时，圆括号（）可省略。例如：a -&gt; return a * a\nLambda 表达式的主体可包含零条或多条语句\n如果 Lambda 表达式的主体只有一条语句，花括号 {} 可省略。匿名函数的返回类型与该主体表达式一致\n如果 Lambda 表达式的主体包含一条以上语句，则表达式必须包含在花括号 {} 中（形成代码块）。匿名函数的返回类型与代码块的返回类型一致，若没有返回则为空\n\n函数式接口扩展函数式接口是只包含一个抽象方法声明的接口，可以使用 @FunctionalInterface 标记\nJDK8 之前已有的函数式接口\njava.lang.Runnable\njava.util.concurrent.Callable\njava.security.PrivilegedAction\njava.util.Comparator\njava.io.FileFilter\njava.nio.file.PathMatcher\njava.lang.reflect.InvocationHandler\njava.beans.PropertyChangeListener\njava.awt.event.ActionListener\njavax.swing.event.ChangeListener\n\n新定义的函数式接口java.util.function 中定义了几组类型的函数式接口以及针对基本数据类型的子接口。\n\nPredicate: 传入一个参数，返回一个 bool 结果，方法为 boolean test (T t)\nConsumer: 传入一个参数，无返回值，纯消费。方法为 void accept (T t)\nFunction: 传入一个参数，返回一个结果，方法为 R apply (T t)\nSupplier: 无参数传入，返回一个结果，方法为 T get ()\nUnaryOperator: 一元操作符，继承 Function, 传入参数的类型和返回类型相同。\nBinaryOperator: 二元操作符，传入的两个参数的类型和返回类型相同，继承 BiFunction。\n\n【示例】\nPredicate&lt;Integer&gt; predicate = (i) -&gt; i &gt; 0;Consumer&lt;Integer&gt; consumer = (i) -&gt; System.out.println(\"consumer : \" + i);Function&lt;Integer,Boolean&gt; function = (i) -&gt; i &gt; 0;Supplier&lt;Integer&gt; supplier = () -&gt; 1;UnaryOperator&lt;Integer&gt; unaryOperator = (i) -&gt; i * i;BinaryOperator&lt;Integer&gt; binaryOperator = (i1,i2) -&gt; i1 * i2;System.out.println(predicate.test(10));consumer.accept(10);System.out.println(function.apply(10));System.out.println(supplier.get());System.out.println(unaryOperator.apply(100));System.out.println(binaryOperator.apply(100,200));\n\n4.Java Stream API for Bulk Data Operations on Collections (用于集合上的批量数据操作的 Java Stream API)示例代码//从 Collection 和数组List&lt;Integerlist = new ArrayList&lt;&gt;();for(int i=0;i&lt;100;i++) {    list.add(i);}Stream&lt;Integerstream = list.stream(); //串行流Stream&lt;Integerstream1 = list.parallelStream(); //并行流Stream&lt;Integerstream2 = Arrays.stream(list.toArray(new Integer[0]));Stream&lt;Integerstream3 = Stream.of(list.toArray(new Integer[0]));//从 BufferedReaderBufferedReader bufferedReader = new BufferedReader(new FileReader(new File(\"path\")));Stream&lt;Stringstream4 = bufferedReader.lines();//静态工厂IntStream stream5 = IntStream.rangeClosed(1, 100);//生成1-100 的int streamStream&lt;Pathstream6 = Files.walk(Paths.get(\"path\"), 100);//自己构建 通过StreamSupport辅助类从spliterator产生流Stream&lt;Integerstream7 = StreamSupport.stream(list.spliterator(), false);//其它Random random = new Random();IntStream stream8 = random.ints();BitSet bitSet = BitSet.valueOf(new long[]{1L, 2L, 3L});IntStream stream9 = bitSet.stream();Pattern pattern = Pattern.compile(\"\\\\d+\");Stream&lt;Stringstream10 = pattern.splitAsStream(\"111sda123sda\");JarFile jarFile = new JarFile(\"xxx.jar\");Stream&lt;JarEntrystream11 = jarFile.stream();\n\n5.Java Time API (Java 时间 API)Java 8 通过发布新的 Date-Time API (JSR 310) 来进一步加强对日期与时间的处理。对日期与时间的操作一直是 Java 程序员最痛苦的地方之一。标准的 java.util.Date 以及后来的 java.util.Calendar 一点没有改善这种情况（可以这么说，它们一定程度上更加复杂）。\nClock 类它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock 可以替换 System.currentTimeMillis () 与 TimeZone.getDefault ()。\n// Get the system clock as UTC offset final Clock clock = Clock.systemUTC();System.out.println(clock.instant());System.out.println(clock.millis());\n\n下面是程序在控制台上的输出:\n2019-01-09T14:52:50.111Z1547045570335\n\nLocaleDate 与 LocalTimeLocaleDate 只持有 ISO-8601 格式且无时区信息的日期部分。相应的，LocaleTime 只持有 ISO-8601 格式且无时区信息的时间部分。LocaleDate 与 LocalTime 都可以从 Clock 中得到。\n// Get the local date and local timefinal LocalDate date = LocalDate.now();final LocalDate dateFromClock = LocalDate.now(clock);System.out.println(date);System.out.println(dateFromClock);// Get the local date and local timefinal LocalTime time = LocalTime.now();final LocalTime timeFromClock = LocalTime.now(clock);System.out.println(time);System.out.println(timeFromClock);\n\n下面是程序在控制台上的输出：\n2019-01-092019-01-0922:52:50.38314:52:50.383\n\nLocaleDateTimeLocaleDateTime 把 LocaleDate 与 LocaleTime 的功能合并起来，它持有的是 ISO-8601 格式无时区信息的日期与时间。\n// Get the local date/timefinal LocalDateTime datetime = LocalDateTime.now();final LocalDateTime datetimeFromClock = LocalDateTime.now(clock);System.out.println(datetime);System.out.println(datetimeFromClock);\n\n下面是程序在控制台上的输出:\n2019-01-09T22:55:05.1942019-01-09T14:55:05.194\n\nZonedDateTime如果你需要特定时区的日期 / 时间，那么 ZonedDateTime 是你的选择。它持有 ISO-8601 格式具具有时区信息的日期与时间。\n// Get the zoned date/timefinal ZonedDateTime zonedDatetime = ZonedDateTime.now();final ZonedDateTime zonedDatetimeFromClock = ZonedDateTime.now(clock);final ZonedDateTime zonedDatetimeFromZone = ZonedDateTime.now(ZoneId.of(\"America/Los_Angeles\"));System.out.println(zonedDatetime);System.out.println(zonedDatetimeFromClock);System.out.println(zonedDatetimeFromZone);\n\n下面是程序在控制台上的输出：\n2019-01-09T22:56:34.033+08:00[Asia/Shanghai]2019-01-09T14:56:34.033Z2019-01-09T06:56:34.035-08:00[America/Los_Angeles]\n\nDuration在秒与纳秒级别上的一段时间。Duration 使计算两个日期间的不同变的十分简单。\n// Get duration between two datesfinal LocalDateTime from = LocalDateTime.of(2018, Month.APRIL, 16, 0, 0, 0);final LocalDateTime to = LocalDateTime.of(2019, Month.APRIL, 16, 23, 59, 59);final Duration duration = Duration.between(from, to);System.out.println(\"Duration in days: \" + duration.toDays());System.out.println(\"Duration in hours: \" + duration.toHours());\n\n上面的例子计算了两个日期 2018 年 4 月 16 号与 2019 年 4 月 16 号之间的过程。下面是程序在控制台上的输出：\nDuration in days: 365Duration in hours: 8783\n\nCollection API improvements (集合 API 改进)上面已经展示了 forEach () 方法和 Stream API 在集合上的使用。java8 的 Collection API 中添加了一些新方法：\nIterator default method forEachRemaining(Consumer action)为每个元素执行给定操作，直到所有元素都已处理或操作引发异常。\n源码default void forEachRemaining(Consumer&lt;? super E&gt; action) {\t//传入一个非空消费者    Objects.requireNonNull(action);\t//遍历执行消费者函数    while (hasNext())        action.accept(next());}\n\n示例代码List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);Iterator&lt;Integer&gt; iterator = list.iterator();//创建一个消费者Consumer&lt;Integer&gt; consumer = i -&gt; System.out.println(\"consumer print \" + i);//iterator的forEachRemaining将集合中的每个元素消费iterator.forEachRemaining(consumer);\n\n控制台输出consumer print 1consumer print 2consumer print 3...\n\nCollection default method removeIf(Predicate filter)删除满足给定条件的此集合的所有元素。\n源码default boolean removeIf(Predicate&lt;? super E&gt; filter) {\t//传入一个非空谓语    Objects.requireNonNull(filter);    boolean removed = false;    final Iterator&lt;E&gt; each = iterator();    while (each.hasNext()) {\t\t//遍历元素，执行谓语的校验，如果为真，则删除该元素        if (filter.test(each.next())) {            each.remove();            removed = true;        }    }    return removed;}\n\n示例代码List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(1);list.add(2);list.add(3);list.add(4);Predicate&lt;Integer&gt; predicate = i -&gt; i &gt; 1;list.removeIf(predicate);System.out.println(\"remove if left items : \" + list);\n\n控制台输出//2,3,4满足条件被删除了remove if left items : [1]\n\nCollection spliterator()返回 Spliterator 实例的方法，该实例可用于顺序或并行遍历元素。\n源码//该方法是接口默认方法default Spliterator&lt;E&gt; spliterator() {    return Spliterators.spliterator(this, Spliterator.ORDERED);}\n\n示例代码List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9);Spliterator&lt;Integer&gt; spliterator = list.spliterator();//创建顺序流Stream&lt;Integer&gt; stream = StreamSupport.stream(spliterator, false);//创建并行流Stream&lt;Integer&gt; parallelStream = StreamSupport.stream(spliterator, true);\n\nMap replaceAll(), compute(), merge() methodsreplaceAll()替换 Map 中所有 Entry 的 value 值，这个值由旧的 key 和 value 计算得出，接收参数 (K, V) -&gt; V\n源码public void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function) {    Node&lt;K,V&gt;[] tab;    if (function == null)        throw new NullPointerException();    if (size &gt; 0 &amp;&amp; (tab = table) != null) {        int mc = modCount;        for (int i = 0; i &lt; tab.length; ++i) {            for (Node&lt;K,V&gt; e = tab[i]; e != null; e = e.next) {\t\t\t\t//使用给定的函数替换原来的value值，key不变                e.value = function.apply(e.key, e.value);            }        }        if (modCount != mc)            throw new ConcurrentModificationException();    }}\n\n示例代码Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(\"1\", \"A\");map.put(\"2\", \"B\");map.put(\"3\", \"C\");map.put(\"4\", \"D\");map.put(\"5\", \"E\");//replaceAll方法map.replaceAll((s, s2) -&gt; s + s2);System.out.println(map);\n\n控制台输出//原来的value由key + value替换掉了{1=1A, 2=2B, 3=3C, 4=4D, 5=5E}\n\ncompute()是 computeIfPresent 和 computeIfAbsent 方法的组合体\n\ncomputeIfPresent: 如果指定的 key 不存在，则通过指定的 K -&gt; V 计算出新的值设置为 key 的值。\ncomputeIfPresent: 如果指定的 key 存在，则根据旧的 key 和 value 计算新的值 newValue, 如果 newValue 不为 null，则设置 key 新的值为 newValue, 如果 newValue 为 null, 则删除该 key 的值。\n\n示例代码Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(\"1\", \"A\");map.put(\"2\", \"B\");map.put(\"3\", \"C\");map.put(\"4\", \"D\");map.put(\"5\", \"E\");//key存在，根据旧的key和value计算新的值newValuemap.compute(\"1\", (k, v) -&gt; v + \" computed\");System.out.println(\"key存在\" + map.get(\"1\"));//key不存在，通过指定的K -&gt; V计算出新的值设置为key的值map.compute(\"6\", (k, v) -&gt; \"F\");System.out.println(\"key不存在\" + map.get(\"6\"));//key存在，如果newValue为null, 则删除该key的值map.compute(\"1\", (k, v) -&gt; null);System.out.println(\"key存在，设置为null \" + map.get(\"1\"));\n\n控制台输出key存在A computedkey不存在Fkey存在，设置为null null\n\nmerge()如果指定的 key 不存在，则设置指定的 value 值，否则根据 key 的旧的值 oldvalue，value 计算出新的值 newValue, 如果 newValue 为 null, 则删除该 key，否则设置 key 的新值 newValue。\n示例代码Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(\"1\", \"A\");map.put(\"2\", \"B\");map.put(\"3\", \"C\");map.put(\"4\", \"D\");map.put(\"5\", \"E\");//存在key为1,输出 AmergeSystem.out.println(map.merge(\"1\", \"merge\", (k, v) -&gt; k + v));//新值为null，删除key，输出 nullSystem.out.println(map.merge(\"1\", \"merge\", (k, v) -&gt; null));//不存在key为6，输出 \"merge\"System.out.println(map.merge(\"6\", \"merge\", (k, v) -&gt; k + v));\n\n控制台输出Amergenullmerge\nPerformance Improvement for HashMap class with Key Collisions具有键冲突的 HashMap 类的性能改进\nConcurrency API improvements (并发 API 改进)ConcurrentHashMapJDK8 提供的并发友好的 HashMap\nCompletableFuture提供了非常强大的 Future 的扩展功能，可以帮助我们简化异步编程的复杂性，并且提供了函数式编程的能力，可以通过回调的方式处理计算结果，也提供了转换和组合 CompletableFuture 的方法。\nExecutors newWorkStealingPool()创建持有足够线程的线程池来支持给定的并行级别，并通过使用多个队列，减少竞争，它需要传一个并行级别的参数，如果不传，则被设定为默认的 CPU 数量。\nJava IO improvements (Java IO API 的改进)Files.list(Path dir)返回一个延迟填充的 Stream，其中的元素是目录中的条目。\n//返回目录下的元素集合流Stream&lt;Path&gt; list = Files.list(new File(\"C:\\\\Users\\\\Administrator\\\\Desktop\").toPath());list.forEach(System.out::println);\n\nFiles.lines(Path path)从文件中读取所有行作为流。\n//返回文件中的所有行数Stream&lt;String&gt; lines = Files.lines(new File(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\new 3.txt\").toPath());lines.forEach(System.out::println);\n\nFiles.find()通过搜索以给定起始文件为根的文件树中的文件，返回使用 Path 延迟填充的 Stream。\n//返回符合判断条件的Path流Stream&lt;Path&gt; stream = Files.find(new File(\"C:\\\\Users\\\\Administrator\\\\Desktop\").toPath(),        1,        (path, basicFileAttributes) -&gt; basicFileAttributes.isDirectory());stream.forEach(System.out::println);\n\nBufferedReader.lines()返回一个 Stream，其元素是从这个 BufferedReader 读取的行。\n//返回文件中的所有行数,类似Files.lines()BufferedReader br = new BufferedReader(new FileReader(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\new 3.txt\"));Stream&lt;String&gt; stringStream = br.lines();stringStream.forEach(System.out::println);\n\n\n参考资源\nJava 8 Features with Examples\n为并发而生的 ConcurrentHashMap（Java 8）\n通过实例理解 JDK8 的 CompletableFuture\n\n","categories":["个人笔记"],"tags":["Java"]},{"title":"Java 设计模式（一）UML 总结","url":"/p/63230/","content":"定义统一建模语言 (英语: Unified Modeling Language ，缩写 UML) 是非专利的第三代建模和规约语言。\n\n\nUML 特点\nUML 是一种开放的方法\n用于说明、可视化、构建和编写一个正在开发的面向对象的、软件密集系统的制品的开放方法。\nUML 展现了一系列最佳工程实践，这些最佳实践在对大规模，复杂系统进行建模方面，特别是在软件架构层次已经被验证有效。\n\nUML2.2 分类UML2.2 中一共定义了 14 种图示，分类如下:\n\n结构式图形：强调的是系统式的建模\n静态图 (类图，对象图，包图)\n 实现图 (组件图，部署图)\n 剖面图\n复合结构图\n\n\n行为式图形：强调系统模型中触发的事件\n活动图\n状态图\n用例图\n\n\n交互式图形：属于行为式图形子集合，强调系统模型中的资料流程\n通信图\n交互概述图 (UML2.0)\n 时序图 (UML2.0)\n 时间图 (UML2.0)\n\n\n\nUML 类图定义Class Diagram: 用于表示类、接口、实例等之间相互的静态关系。虽然名字叫类图，但类图中并不只有类（也包括权限，属性，方法等）。\n记忆技巧箭头方向\n定义子类时需要通过 extends 关键字指定父类\n子类一定是知道父类定义的，但父类并不知道子类的定义\n只有知道对方信息时才能指向对方\n所以箭头方向是从子类指向父类\n\n实线 - 继承 | 虚线 - 实现\n\n空心三角箭头：继承或实现\n实线继承， is a 关系，扩展目的，不虚，很结实\n虚线 - 实现，虚线代表 “虚”，无实体\n\n实线 - 关联 | 虚线 - 依赖\n\n实线 - 关联关系：关系稳定，实打实的关系，铁哥们\n表示一个类对象和另一个类对象有关联\n通常是一个类中有另一个类对象做为属性\n\n\n\n虚线 - 依赖关系：临时用一下，若即若离，虚无缥缈，若有若无\n表示一种使用关系，一个类需要借助另一个类来实现功能\n一般是一个类使用另一个类做为参数使用，或作为返回值\n\n空心菱形 - 聚合 | 实心菱形 - 组合\n菱形就是一个盛东西的器皿 (例如盘子)\n 聚合：代表空器皿里可以放很多相同东西，聚在一起 (箭头方向所指的类)\n 组合：代表满器皿里已经有实体结构的存在，生死与共\n\n\n\n整体和局部的关系，两者有着独立的生命周期，是 has a 的关系\n弱关系\n消极的词：弱 - 空\n\n\n\n整体与局部的关系，和聚合的关系相比，关系更加强烈\n两者有相同的生命周期， contains-a 的关系\n强关系\n积极的词：强 - 满\n\n常见数字表达及含义，假设有 A 类和 B 类，数字标记在 A 类侧\n0..1:0 或 1 个实例.\n0..*:0 或多个实例.\n1..1:1 个实例.\n1: 只能有一个实例.\n1..*: 至少有一个实例.\n\n类图详解\n类图从上到下包含：\n\n类名：抽象类使用斜体表示，接口用 &lt;&gt; 表示\n属性：访问权限 + 属性名：属性类型\n+：public\n-：private\n#：protected\n~：default\n下横线表示 static\n\n\n 方法：访问权限 + 方法名：返回值类型\n+：public\n-：private\n#：protected\n~：default\n下横线表示 static\n 斜体表示抽象方法\n\n\n\n典型的类图表示：\n\nUML 时序图Sequence Diagram : 是显示对象之间交互的图，这些对象是按时间顺序排列的。\n时序图中包括的建模元素主要有:\n\n对象 (Actor)\n 生命线 (Lifeline)\n 控制焦点 (Focus of control)\n 消息 (Message) 等\n\n典型的一个时序图如下：\n\n","categories":["个人笔记"],"tags":["设计模式","UML"]},{"title":"Java 设计模式（三）简单工厂模式","url":"/p/21710/","content":"定义与类型\n定义：由一个工厂对象决定创建出哪一种产品类的实例\n类型：创建型，但不属于 GOF23 种设计模式\n\n\n\n适用场景\n工厂类负责创建的对象比较少\n客户端 (应用层) 只知道传入工厂类的参数，对于如何创建对象 (逻辑) 不关心\n\n优点只需要传入一个正确的参数，就可以获取你所需要的对象，而无须知道其创建细节\n缺点工厂类的职责相对过重，增加新的产品，需要修改工厂类的判断逻辑，违背开闭原则\nCoding创建一个抽象产品类\npublic abstract class Video {    public abstract void produce();}\n\n产品实现类\npublic class JavaVideo extends Video {    @Override    public void produce() {        System.out.println(\"录制Java课程\");    }}public class PythonVideo extends Video {    @Override    public void produce() {        System.out.println(\"录制Python视频\");    }}\n\n创建产品对应的简单工厂，通过产品类型来创建产品，应用方无需知道创建产品的细节\npublic class VideoFactory {    public Video getVideo(String type) {        if (\"java\".equalsIgnoreCase(type)) {            return new JavaVideo();        } else if (\"python\".equalsIgnoreCase(type)) {            return new PythonVideo();        } else {            return null;        }    }}\n\n测试类\npublic class Test {    public static void main(String[] args) {        VideoFactory videoFactory = new VideoFactory();        Video video = videoFactory.getVideo(\"Java\");        video.produce();    }}\n\n控制台输出\n录制Java课程\n\n如果增加产品，我们不仅需要修改产品对应的产品类，还需要修改工厂类，违反了开闭原则。\n我们可以通过反射来优化下我们的工厂类\npublic class VideoFactory {    public Video getVideo(Class&lt;? extends Video&gt; clazz) {        try {            return clazz.newInstance();        } catch (InstantiationException | IllegalAccessException e) {            e.printStackTrace();        }        return null;    }}\n\n这样一来，添加产品的时候不用再修改我们的工厂类，而是直接添加产品即可。\n最终的 UML 类图\n\n源码解析JDK 源码在 JDK 中，使用简单工厂模式的例子如 java.util.Calendar，一组 getInstance 的重载方法，提供了创建 Calendar 产品的简单工厂方法。\npublic static Calendar getInstance()public static Calendar getInstance(TimeZone zone)public static Calendar getInstance(Locale aLocale)public static Calendar getInstance(TimeZone zone，Locale aLocale)\n\n核心方法为\nprivate static Calendar createCalendar(TimeZone zone，Locale aLocale)\n\n源码较长，不贴了，有兴趣的可以去看下源码。\nCalendar 的 UML 类图如下\n\nLogback 源码logback 类中的简单工厂模式主要体现在 ch.qos.logback.classic.LoggerContext#getLogger(String)\n@Overridepublic final Logger getLogger(final String name) {    if (name == null) {        throw new IllegalArgumentException(\"name argument cannot be null\");    }    // 判断log类型返回root节点的logger    if (Logger.ROOT_LOGGER_NAME.equalsIgnoreCase(name)) {        return root;    }    int i = 0;    Logger logger = root;    // 如果缓存中已经存在的指定的logger，直接返回childLogger    Logger childLogger = (Logger) loggerCache.get(name);    // if we have the child， then let us return it without wasting time    if (childLogger != null) {        return childLogger;    }    // 以下是创建logger的逻辑    String childName;    while (true) {        int h = LoggerNameUtil.getSeparatorIndexOf(name， i);        if (h == -1) {            childName = name;        } else {            childName = name.substring(0， h);        }        // move i left of the last point        i = h + 1;        synchronized (logger) {            childLogger = logger.getChildByName(childName);            if (childLogger == null) {                childLogger = logger.createChildByName(childName);                loggerCache.put(childName， childLogger);                incSize();            }        }        logger = childLogger;        if (h == -1) {            return childLogger;        }    }}\n\n是一个典型的简单工厂方法\n","categories":["个人笔记"],"tags":["设计模式"]},{"title":"Java 设计模式（四）工厂方法模式","url":"/p/30626/","content":"定义与类型\n定义：定义一个创建对象的接口，但让实现这个接口的类来决定实例化哪个类，工厂方法让类的实例化推迟到子类中进行。\n类型：创建型\n\n\n\n适用场景\n创建对象需要大量重复的代码\n客户端 (应用层) 不依赖于产品类实例如何被创建、实现等细节\n一个类通过其子类来指定创建哪个对象\n\n优点\n用户只需要关心所需产品对应的工厂，无须关心创建细节\n加入新产品符合开闭原则，提高可扩展性\n\n缺点\n类的个数容易过多，增加复杂度\n增加了系统的抽象性和理解难度\n\nCoding工厂方法模式从一定意义上讲是从简单工厂模式衍生过来的，创建产品抽象类\npublic abstract class Video {    public abstract void produce();}\n\n创建具体产品\npublic class JavaVideo extends Video {    @Override    public void produce() {        System.out.println(\"录制Java课程\");    }}public class PythonVideo extends Video {    @Override    public void produce() {        System.out.println(\"录制Python视频\");    }}\n\n创建产品工厂方法抽象类\npublic abstract class VideoFactory {    public abstract Video getVideo();}\n\n创建产品工厂方法实现类（每个产品都有对应的实现类）\npublic class JavaVideoFactory extends VideoFactory {    @Override    public Video getVideo() {        return new JavaVideo();    }}public class PythonVideoFactory extends VideoFactory {    @Override    public Video getVideo() {        return new PythonVideo();    }}\n\n测试类\npublic class Test {    public static void main(String[] args) {        VideoFactory javaVideoFactory = new JavaVideoFactory();        VideoFactory pythonVideoFactory = new PythonVideoFactory();        Video video = javaVideoFactory.getVideo();        video.produce();        video = pythonVideoFactory.getVideo();        video.produce();    }}\n\n控制台输出\n录制Java课程录制Python视频\n\n如果我们现在新增一个产品–前端课程，我们需要创建产品类，产品工厂类，但是无需改动其他代码，做到了对扩展开放，对修改关闭，符合开闭原则。\npublic class FEVideo extends Video {    @Override    public void produce() {        System.out.println(\"录制前端课程\");    }}public class FEVideoFactory extends VideoFactory {    @Override    public Video getVideo() {        return new FEVideo();    }}\n\n但是，我们也不难看出工厂方法模式的缺点–类的个数容易过多，增加复杂度。\n因为一旦我们需要现在产品，就需要创建产品对应的产品实现类，以及产品工厂方法类，无疑增加了类的个数和系统的复杂度。\n完整的 UML 类图\n\n源码解析Collection 源码jdk 中典型的工厂方法模式体现为 java.util.Collection\n抽象产品为 java.util.Iterator\npublic interface Iterator&lt;E&gt; {\t...}\n\n抽象工厂定义了创建产品族的方法 java.util.Collection.#iterator\nIterator&lt;E&gt; iterator();\n\n由子类来定义具体创建产品的逻辑，如 java.util.ArrayList.#iterator\npublic Iterator&lt;E&gt; iterator() {    return new Itr();}\n\n而具体的产品定义为 java.util.ArrayList$Itr\nprivate class Itr implements Iterator&lt;E&gt; {    ...}\n\nUML 类图\n\nURLStreamHandlerFactory 源码再来看一个典型例子，java.net.URLStreamHandlerFactory 作为工厂方法抽象类，定义了创建产品的抽象方法\npublic interface URLStreamHandlerFactory {    URLStreamHandler createURLStreamHandler(String protocol);}\n\n产品抽象类就是 java.net.URLStreamHandler\npublic abstract class URLStreamHandler {    ...}\n\n产品的工厂方法实现类为 sun.misc.Launcher$Factory\nprivate static class Factory implements URLStreamHandlerFactory {    ...    public URLStreamHandler createURLStreamHandler(String var1) {        private static String PREFIX = \"sun.net.www.protocol\";        private Factory() {        }        public URLStreamHandler createURLStreamHandler(String var1) {            String var2 = PREFIX + \".\" + var1 + \".Handler\";            try {                // 通过反射创建指定类型的产品                Class var3 = Class.forName(var2);                return (URLStreamHandler)var3.newInstance();            } catch (ReflectiveOperationException var4) {                throw new InternalError(\"could not load \" + var1 + \"system protocol handler\"， var4);            }        }    }}\n\n可以发现，工厂实现类通过反射类创建具体的产品实现类，而产品实现类非常多\n\n这样满足了开闭原则，也没有过多的增加类的数量，值得我们学习。\n","categories":["个人笔记"],"tags":["设计模式"]},{"title":"Java 设计模式（二）设计模式原则","url":"/p/64818/","content":"学习 Java 设计模式之前，有必要先了解设计模式原则。\n开闭原则定义\n一个软件实体如类、模块和函数应该对扩展开放，对修改关闭\n\n用抽象构建框架，用实现扩展细节\n\n优点：提高软件系统的可复用性及可维护性\n\n\nCoding创建接口\npublic interface ICourse {    Integer getId();    String getName();    Double getPrice();}\n\n创建实现类\n@ToString@AllArgsConstructorpublic class JavaCourse implements ICourse {    @Getter    private Integer id;    @Getter    private String name;    @Getter    private Double price;}\n\n测试类\npublic class Test {    public static void main(String[] args) {        ICourse iCourse = new JavaCourse(96， \"我的Java课程\"， 348d);        System.out.println(\"课程ID: \" + iCourse.getId() + \" 课程名称： \" + iCourse.getName() + \"课程价格： \" + iCourse.getPrice());    }}\n\n控制台输出\n课程ID: 96 课程名称： 我的Java课程课程价格： 348.0\n\n如果现在要打折出售课程，按照开闭原则来设计，对扩展开放，对修改关闭。\n创建打折类\npublic class JavaDiscountCourse extends JavaCourse {    public JavaDiscountCourse(Integer id， String name， Double price) {        super(id， name， price);    }    public Double getOriginPrice() {        return super.getPrice();    }    @Override    public Double getPrice() {        return super.getPrice() * 0.8;    }}\n\n修改应用类\npublic class Test {    public static void main(String[] args) {        ICourse javaCourse = new JavaDiscountCourse(96， \"我的Java课程\"， 348d);        JavaDiscountCourse iCourse = (JavaDiscountCourse) javaCourse;        System.out.println(\"课程ID: \" + iCourse.getId() + \" 课程名称： \" + iCourse.getName() + \"课程原价： \" + iCourse.getOriginPrice() + \" 课程折后价格： \" + iCourse.getPrice());    }}\n\n控制台输出\n课程ID: 96 课程名称： 我的Java课程课程原价： 348.0 课程折后价格： 278.40000000000003\n\n这里有个要注意的地方，Double * 0.8 后输出的浮点数精度有丢失的情况，可以使用 BigDecimal 的 String 构造器 public BigDecimal(String val) 来解决。\n修改 JavaDiscountCourse\npublic class JavaDiscountCourse extends JavaCourse {    public JavaDiscountCourse(Integer id， String name， Double price) {        super(id， name， price);    }    public Double getOriginPrice() {        return super.getPrice();    }    @Override    public Double getPrice() {        return new BigDecimal(super.getPrice().toString()).multiply(new BigDecimal(\"0.8\")).doubleValue();    }}\n\n控制台输出\n课程ID: 96 课程名称： 我的Java课程课程原价： 348.0 课程折后价格： 278.4\n\n依赖倒置原则定义\n高层模块不应该依赖低层模块，二者都应该依赖其抽象\n抽象不应该依赖细节；细节应该依赖抽象\n针对接口编程，不要针对实现编程\n优点：可以减少类间的耦合性、提高系统稳定性，提高代码可读性和可维护性，可降低修改程序所造成的风险\n\nCoding反例创建类\npublic class Geely {    public void studyJavaCourse() {        System.out.println(\"Geely在学习Java课程\");    }    public void studyFECourse() {        System.out.println(\"Geely在学习FE课程\");    }    public void studyPythonCourse() {        System.out.println(\"Geely在学习Python课程\");    }}\n\n测试类\npublic class Test {    // v1    public static void main(String[] args) {        Geely geely = new Geely();        geely.studyFECourse();        geely.studyJavaCourse();    }}\n\n控制台输出\nGeely在学习FE课程Geely在学习Java课程\n\n这时候，如果我们要让 Geely 学习 Ruby 课程，我们只能在 Geely 类中添加\npublic void studyRubyCourse() {    System.out.println(\"Geely在学习Ruby课程\");}\n\n然后，在 Test 类中添加\ngeely.studyRubyCourse();\n\n不符合依赖倒置原则\n正例创建接口\npublic interface ICourse {    void studyCourse();}\n\n创建类，带有成员变量 ICourse course\n@AllArgsConstructorpublic class Geely {    @Setter    private ICourse course;    public void studyImoocCourse() {        course.studyCourse();    }}\n\n创建实现类\npublic class FECourse implements ICourse {    @Override    public void studyCourse() {        System.out.println(\"Geely在学习FE课程\");    }}public class JavaCourse implements ICourse {    @Override    public void studyCourse() {        System.out.println(\"Geely在学习Java课程\");    }}public class PythonCourse implements ICourse {    @Override    public void studyCourse() {        System.out.println(\"Geely在学习Python课程\");    }}\n\n测试类\npublic class Test {    public static void main(String[] args) {        Geely geely = new Geely(new JavaCourse());        geely.studyImoocCourse();        geely.setCourse(new FECourse());        geely.studyImoocCourse();    }}\n\n控制台输出\nGeely在学习Java课程Geely在学习FE课程\n\n这样一来，如果要添加新的课程，只需要创建实现类即可。然后应用类设置实现类，无需改动其他代码，符合依赖倒置原则。\n单一职责原则定义\n不要存在多于一个导致类变更的原因\n一个类 / 接口 / 方法只负责一项职责\n优点：降低类的复杂度、提高类的可读性、提高系统的可维护性、降低变更引起的风险\n\nCoding反例创建类\npublic class Bird {    public void mainMoveMode(String birdName) {        System.out.println(birdName + \" 用翅膀飞\");    }}\n\n测试类\npublic class Test {    public static void main(String[] args) {        Bird bird = new Bird();        bird.mainMoveMode(\"大雁\");        bird.mainMoveMode(\"鸵鸟\");    }}\n\n控制台输出\n大雁 用翅膀飞鸵鸟 用翅膀飞\n\n鸵鸟是用脚走的，所以我们更改 Bird 类\npublic class Bird {    public void mainMoveMode(String birdName) {        if (\"鸵鸟\".equals(birdName)) {            System.out.println(birdName + \" 用脚走\");        } else {            System.out.println(birdName + \" 用翅膀飞\");        }    }}\n\n如果有更多的鸟类，我们还要写更多的 else 代码。\n正例我们修改下反例中的例子\npublic class FlyBird {    public void mainMoveMode(String birdName) {        System.out.println(birdName + \" 用翅膀飞\");    }}public class WalkBird {    public void mainMoveMode(String birdName) {        System.out.println(birdName + \" 用脚走\");    }}\n\n添加测试类\npublic class Test {    public static void main(String[] args) {        FlyBird flyBird = new FlyBird();        flyBird.mainMoveMode(\"大雁\");        WalkBird walkBird = new WalkBird();        walkBird.mainMoveMode(\"鸵鸟\");    }}\n\n控制台输出\n大雁 用翅膀飞鸵鸟 用脚走\n\n再举一个例子\n创建接口\n/** * 课程内容 * * @author gaochen * Created on 2019/7/27. */public interface ICourseContent {    String getCoursName();    byte[] getCourseVideo();}/** * 课程管理 * * @author gaochen * Created on 2019/7/27. */public interface ICourseManager {    void studyCourse();    void refundCourse();}\n\n创建实现类，有着课程内容和课程管理两种职能\npublic class CourseImpl implements ICourseContent， ICourseManager {    @Override    public String getCoursName() {        return null;    }    @Override    public byte[] getCourseVideo() {        return new byte[0];    }    @Override    public void studyCourse() {    }    @Override    public void refundCourse() {    }}\n\n接口隔离原则定义\n用多个专门的接口，而不使用单一的总接口，客户端不应该依赖它不需要的接口\n一个类对一个类的依赖应该建立在最小的接口上\n建立单一接口，不要建立庞大臃肿的接口\n尽量细化接口，接口中的方法尽量少\n优点：符合我们常说的高内聚低耦合的设计思想，从而使得类具有很好的可读性、可扩展性和可维护性。\n\n注意适度原则，一定要适度\nCoding反例创建接口\npublic interface IAnimalAction {    void eat();    void fly();    void swim();}\n\n创建实现类\npublic class Bird implements IAnimalAction {    @Override    public void eat() {        System.out.println(\"鸟 吃饭\");    }    @Override    public void fly() {        System.out.println(\"鸟 飞\");    }    @Override    public void swim() {        // 鸟不会游泳，空实现    }}public class Dog implements IAnimalAction {    @Override    public void eat() {        System.out.println(\"狗 吃饭\");    }    @Override    public void fly() {        // 狗不会飞，空实现    }    @Override    public void swim() {        System.out.println(\"狗 游泳\");    }}\n\n我们可以看出，鸟和狗实现了接口后，各自都有无用的接口，所以违反了接口隔离原则，只能采取空实现的方式。但是对于使用方来说，还是可以调用狗的 fly 方法，得到空的实现。\n正例将反例中的接口接口拆分为三个独立的接口\npublic interface IEatAnimalAction {    void eat();}public interface IFlyAnimalAction {    void fly();}public interface ISwimAnimalAction {    void swim();}\n\nDog 改为\npublic class Dog implements IEatAnimalAction，ISwimAnimalAction {    @Override    public void eat() {        System.out.println(\"狗 吃饭\");    }    @Override    public void swim() {        System.out.println(\"狗 游泳\");    }}\n\nBird 改为\npublic class Bird implements IEatAnimalAction，IFlyAnimalAction {    @Override    public void eat() {        System.out.println(\"鸟 吃饭\");    }    @Override    public void fly() {        System.out.println(\"鸟 飞\");    }}\n\n这样就成功的将一个大接口，优化为分摊职责的小接口，实现类可以根据需要实现多个职能接口。\n迪米特原则定义\n一个对象应该对其他对象保持最少的了解。又叫最少知道原则\n尽量降低类与类之间的耦合\n优点：降低类之间的耦合\n\nCoding反例创建课程类\npublic class Course {}\n\n创建项目经理类\npublic class TeamLeader {    public void checkNumberOfCourse(List&lt;Course&gt; courseList) {        System.out.println(\"在线课程的数量是 ：\" + courseList.size());    }}\n\n创建老板类\npublic class Boss {    public void commandCheckNumber(TeamLeader teamLeader) {        List&lt;Course&gt; courseList = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 20; i++) {            courseList.add(new Course());        }        teamLeader.checkNumberOfCourse(courseList);    }}\n\n测试类\n在线课程的数量是 ：20\n\n我们仔细分析一下，其实老板并不需要知道课程的细节，只需要问一下项目经理，有多少课程，项目经理直接告诉老板有 20 节在线课程。而不是老板将课程列出，让项目经理统计。\n我们看下 UML 类图\n\n正例项目经理类修改为\npublic class TeamLeader {        public void checkNumberOfCourse() {        List&lt;Course&gt; courseList = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; 20; i++) {            courseList.add(new Course());        }        System.out.println(\"在线课程的数量是 ：\" + courseList.size());    }}\n\n老板类\npublic class Boss {    public void commandCheckNumber(TeamLeader teamLeader) {        teamLeader.checkNumberOfCourse();    }}\n\n这时候运行一下，结果一样。但是从 UML 类图上来看，是有很大的优化的。\n\n","categories":["个人笔记"],"tags":["设计模式","UML"]},{"title":"Lombok 详解","url":"/p/12232/","content":"简介lombok 是一个编译级别的插件，它可以在项目编译的时候生成一些代码。通俗的说，lombok 可以通过注解来标示生成 getter settter 等代码。\n\n\n引入创建 gradle 项目\ncompile group: 'org.projectlombok', name: 'lombok', version: '1.16.20'\n\n注解@NonNull\n标记字段不可为 null\n\n@Setterpublic class Person {    @NonNull    private String name;    @NonNull    private Integer age;}\n\n对应的字节码文件：\npublic class Person {    @NonNull    private String name;    @NonNull    private Integer age;    public Person() {    }    public void setName(@NonNull String name) {        if (name == null) {            throw new NullPointerException(\"name\");        } else {            this.name = name;        }    }    public void setAge(@NonNull Integer age) {        if (age == null) {            throw new NullPointerException(\"age\");        } else {            this.age = age;        }    }}\n\n@Getter/@Setter\n自动生成 getter 和 setter 方法\n\npublic class Person {    @Getter    private String name;    @Setter    private Integer age;}\n\n对应的字节码文件：\npublic class Person {    private String name;    private Integer age;    public Person() {    }    public String getName() {        return this.name;    }    public void setAge(Integer age) {        this.age = age;    }}\n\n@Cleanup\n自动关闭流代码\n\n@CleanupInputStream in = new FileInputStream(args[0]);\n\n对应的字节码文件：\nInputStream in = new FileInputStream(args[0]);if (Collections.singletonList(in).get(0) != null) {    in.close();}\n\n@AllArgsConstructor/@NoArgsConstructor/@RequiredArgsConstructor\n自动生成全参构造函数和无参构造函数\n\n@AllArgsConstructor@NoArgsConstructorpublic class Person {    private String name;    private Integer age;}\n\n对应的字节码文件\npublic class Person {    private String name;    private Integer age;    public Person(String name, Integer age) {        this.name = name;        this.age = age;    }    public Person() {    }}\n\n@Builder\n自动生成建造者模式的 bean\n\n@Builderpublic class Person {    private String name;    private Integer age;}\n\n对应的字节码文件\npublic class Person {    private String name;    private Integer age;    Person(String name, Integer age) {        this.name = name;        this.age = age;    }    public static Person.PersonBuilder builder() {        return new Person.PersonBuilder();    }    public static class PersonBuilder {        private String name;        private Integer age;        PersonBuilder() {        }        public Person.PersonBuilder name(String name) {            this.name = name;            return this;        }        public Person.PersonBuilder age(Integer age) {            this.age = age;            return this;        }        public Person build() {            return new Person(this.name, this.age);        }        public String toString() {            return \"Person.PersonBuilder(name=\" + this.name + \", age=\" + this.age + \")\";        }    }}\n\n@EqualsAndHashCode\n自动生成 equals 和 hashcode 方法\n\n@EqualsAndHashCodepublic class Person {    private String name;    private Integer age;}\n\n对应的字节码文件\n\npublic class Person {    private String name;    private Integer age;    public Person() {    }    public boolean equals(Object o) {        if (o == this) {            return true;        } else if (!(o instanceof Person)) {            return false;        } else {            Person other = (Person)o;            if (!other.canEqual(this)) {                return false;            } else {                Object this$name = this.name;                Object other$name = other.name;                if (this$name == null) {                    if (other$name != null) {                        return false;                    }                } else if (!this$name.equals(other$name)) {                    return false;                }                Object this$age = this.age;                Object other$age = other.age;                if (this$age == null) {                    if (other$age != null) {                        return false;                    }                } else if (!this$age.equals(other$age)) {                    return false;                }                return true;            }        }    }    protected boolean canEqual(Object other) {        return other instanceof Person;    }    public int hashCode() {        int PRIME = true;        int result = 1;        Object $name = this.name;        int result = result * 59 + ($name == null ? 43 : $name.hashCode());        Object $age = this.age;        result = result * 59 + ($age == null ? 43 : $age.hashCode());        return result;    }}\n@ToString\n自动生成 toString () 方法\n\n@ToStringpublic class Person {    private String name;    private Integer age;}\n\n对应的字节码文件\npublic class Person {    private String name;    private Integer age;    public Person() {    }    public String toString() {        return \"Person(name=\" + this.name + \", age=\" + this.age + \")\";    }}\n\n@Value\n自动生成全参构造函数、Getter 方法、equals 方法、hashCode 法、toString 方法\n\n@Valuepublic class Person {    private String name;    private Integer age;}\n\n注意：@Value 不会生成 Setter 方法\n@Synchronized\n自动为被标记的方法添加 synchronized 锁\n\npublic class SynchronizedExample {  private final Object readLock = new Object();    @Synchronized  public static void hello() {    System.out.println(\"world\");  }    @Synchronized  public int answerToLife() {    return 42;  }    @Synchronized(\"readLock\")  public void foo() {    System.out.println(\"bar\");  }}\n\n对应的字节码文件\npublic class SynchronizedExample {    private static final Object $LOCK = new Object[0];    private final Object $lock = new Object[0];    private final Object readLock = new Object();    public static void hello() {        synchronized($LOCK) {            System.out.println(\"world\");        }    }    public int answerToLife() {        synchronized($lock) {            return 42;        }    }    public void foo() {        synchronized(readLock) {            System.out.println(\"bar\");        }    }}\n\n@Delegate\n为标记属性生成委托方法\n\npublic class DelegateExample {    public void show() {        System.out.println(\"show...\");    }}@AllArgsConstructorpublic class Demo {    @Delegate    private final DelegateExample delegateExample;}\n\n对应的字节码文件\npublic class DelegateExample {    public DelegateExample() {    }    public void show() {        System.out.println(\"show...\");    }}public class Demo {    private final DelegateExample delegateExample;    public Demo(DelegateExample delegateExample) {        this.delegateExample = delegateExample;    }    // 委托方法    public void show() {        this.delegateExample.show();    }}\n\n","categories":["个人笔记"],"tags":["Lombok"]},{"title":"MacOs 科学上网","url":"/p/709/","content":"前言之前使用 Windows 的时候，有非常优秀的全局代理软件 SSTap 用来翻墙，但是到了 MacOs 上，没有找到类似 SSTap 的全局翻墙神器。\n最终采取的方案是 ShadowsocksX-NG R8+Proxifier 的方式来实现。\n\n\n软件ShadowsocksX-NG R8\nShadowsocksX-NG 是 Mac 下的 SSR 工具，具有和 Windows 下同样的体验，使用起来也非常方便，支持服务器订阅。\n\n\n配置就不多说了，唯一要注意的是，Socks5 的监听地址是：127.0.0.1:1086，这个我们一会要用到。\n\nProxifier\nProxifier 是 Mac 下的全局代理工具，可以将流量统统都转到代理上，配合 ShadowsocksX-NG，我们很轻松的就可以实现全局翻墙\n\n软件本身是收费的，当然了，在 Xclient.info 上可以找到破解版：https://xclient.info/s/proxifier.html\n安装完毕之后，我们只需要简单的配置一下，就可以实现全局科学上网了！\n添加 Socks5 代理\n点击 Proxies\n\n\n\n 点击 Add，添加代理\n\n\n\n添加完毕后，回到主界面，点击 Rules，修改默认规则，将动作指向我们新添加的 Socks5 代理\n\n\n\n测试\n\n打开 ITerm2 测试下，在终端输入\n$ curl www.google.com\n\n\n同时，Proxifier 也打印出了 ITerm2 的流量信息\n\n另外，Proxifier 也支持指定软件走代理，具体步骤如下\n\n在 Rules 标签中点击 Add，新建一个规则\n\n\n\n添加完成后勾选使用，同时，不要忘记把 Default 规则的动作设置为 Direct，不然的话，还是全局都走代理的。\n\n\n相关软件\nShadowsocksX-NG：链接:https://pan.baidu.com/s/10i6PZZIParFRkvaVu5KhxA  密码:jvzx\nProxifier：链接:https://pan.baidu.com/s/1ymZZRDJrjrIXXFrYfIxV_w  密码:exns\n\n","categories":["个人笔记"],"tags":["科学上网"]},{"title":"Kubernetes 部署 csi-driver","url":"/p/20900/","content":"本文介绍在 k8s 集群中部署 nfs-csi-driver\n\n\n环境依赖\nDocker\nKubernetes（本文是基于 Rancher 的 RKE 集群）\n\n前言csi-driver 是非常丰富的，新搭建的集群，推荐使用阿里云的 NAS 作为存储后端（NAS 对比云盘，扩展性高、费用低、容错好），使用阿里云提供的 NAS CSI Plugin 进行 csi-driver 的安装。\n对于有云盘的 ECS 或者本地服务器来说，可以使用 NFS 作为存储后端，k8s 官方提供了 nfs-csi-driver\n如果是阿里的 ECS，还可以使用阿里提供的 disk-csi-driver，使用云盘作为存储后端（有单机故障的风险）\n部署 nfs-csi-driver1、安装 nfs-server# 选择一台主机，安装nfs-server$ sudo apt update$ sudo apt install nfs-kernel-server$ mkdir -p /data/nfs$ sudo vim /etc/fstab172.16.1.23:/data/nfs /data/nfs    nfs defaults,timeo=900,retrans=5,_netdev    0 0$ sudo vim /etc/exports/data/nfs    172.16.1.23/12(rw,sync,all_squash,anonuid=1001,anongid=1001,no_subtree_check) # 这里设置anonuid和anongid，是为了便于部署bitnami提供的helm charts$ sudo exportfs -ra# bitnami提供的helm-chart的fsGroup默认是1001，如果1001的用户被占用，可以修改用户的uid和gid，将1001空出来给bitnami使用# 其他主机，配置nfs-client$ apt install nfs-common$ sudo mount -t nfs -o vers=4 172.16.1.23:/data/nfs /data/nfs$ sudo vim /etc/fstab172.16.1.23:/data/nfs /data/nfs    nfs defaults,timeo=900,retrans=5,_netdev    0 0# 测试nfs访问# 在任意一台机器上创建、修改、删除文件，在其他机器上会对应列出修改\n\n2、安装 nfs-csi-driver这里有个小插曲，由于国内的网络环境，无法下载谷歌的 docker 镜像，所以只能一个个的把它们转到 dockerhub 上\ndocker pull k8s.gcr.io/sig-storage/livenessprobe:v2.1.0 \\    &amp;&amp; docker tag k8s.gcr.io/sig-storage/livenessprobe:v2.1.0 qq1398371419/gcr.sig-storage.livenessprobe:v2.1.0 \\    &amp;&amp; docker push qq1398371419/gcr.sig-storage.livenessprobe:v2.1.0docker pull k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 \\    &amp;&amp; docker tag k8s.gcr.io/sig-storage/csi-provisioner:v2.1.0 qq1398371419/gcr.sig-storage.csi-provisioner:v2.1.0 \\    &amp;&amp; docker push qq1398371419/gcr.sig-storage.csi-provisioner:v2.1.0    docker pull k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 \\    &amp;&amp; docker tag k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.1.0 qq1398371419/gcr.sig-storage.csi-node-driver-registrar:v2.1.0 \\    &amp;&amp; docker push qq1398371419/gcr.sig-storage.csi-node-driver-registrar:v2.1.0\n\n安装 nfs-csi-driver\ncurl -skSL https://gitee.com/qq1398371419/csi-driver-nfs/raw/master/deploy/install-driver.sh | bash -s master --\n\n\n3、安装 StorageClassnfs-sc.yaml\n---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: nfs-csiprovisioner: nfs.csi.k8s.ioparameters:  server: 172.16.1.23 # 这里改成自己的内网IP  share: /data/nfs # 这里改成自己的nfs共享目录reclaimPolicy: RetainvolumeBindingMode: ImmediatemountOptions:  - hard  - nfsvers=4.1\n\n$ kubectl create -f nfs-sc.yaml\n\n4、测试部署 Redis$ vim redis-test.yaml---master:  persistence:    storageClass: nfs-csi  service:    nodePort: 30001    type: NodePortreplica:  persistence:    storageClass: nfs-csiauth:  enabled: false$ helm repo add bitnami https://charts.bitnami.com/bitnami$ helm install my-release bitnami/redis -f redis-test.yaml\n\n\n问题\n前面提到的设置 anonuid 和 anongid，是因为部署 bitnami 提供的 helm charts 遇到了无法写入文件的问题，之所以设置为 1001，因为 bitnami 默认的 uid 是 1001\n\n\n容器对于 mount 的路径的访问权限问题，可以参考 https://www.cnblogs.com/sammyliu/p/10129670.html\n相关资料\nKubernetes CSI Developer Documentation\nalibaba-cloud-csi-driver\nKubernetes CSI\n\n","categories":["个人笔记"],"tags":["Kubernetes","Docker"]},{"title":"PG 数据库常用操作","url":"/p/59866/","content":"记录一下，在开发过程中接触到的一些 PG 数据库常用操作，以备不时之需。\n全量迁移\n备份数据 \n\n$ pg_dump -h 172.19.235.145 -U &lt;username&gt; -d &lt;database&gt; &gt; 20180704_dbpe.sql\n\n\n正式迁移\n\n首先要修改备份文件 *.sql 的 owner，防止权限出现错误。\n$ psql -h &lt;ip&gt; -U &lt;username&gt; -d &lt;database&gt; -f 20180704_dbpe.sql\n\n【注意点】该迁移操作会覆盖原来的数据库，所以最好创建一个新库。\n列出所有表名和数据库名select tablename from pg_tables where schemaname ='public';\n\nPostgreSQL 中 有时候想删除数据库（drop database swiftliveqaapi;），发现提示 “ERROR:  database “xxxxxx” is being accessed by other users DETAIL:  There are 30 other sessions using the database.”用psql 登录进入， 执行语句：SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='数据库名' AND pid&lt;&gt;pg_backend_pid();然后就可以删除数据库了\n\n修改表的序列为 id 最大值SELECT setval('表名_id_seq', (SELECT MAX(id) FROM 表名));\n\n查询表结构SELECT \tCOLUMN_NAME AS 列名,\tDATA_TYPE AS 字段类型,\tCHARACTER_MAXIMUM_LENGTH AS 长度,\tIS_NULLABLE AS 是否为空,\tCOLUMN_DEFAULT AS 默认值 FROM\tINFORMATION_SCHEMA.COLUMNS WHERE\ttable_schema = 'public' \tAND TABLE_NAME = '表名';\n\nPG 数据库状态，启动，停止$ pg_ctlcluster 9.5 main status$ pg_ctlcluster 9.5 main start$ pg_ctlcluster 9.5 main stop\n\n","categories":["个人笔记"],"tags":["数据库"]},{"title":"Nginx 配置 Https 指南","url":"/p/17023/","content":"前言本文是对 Nginx 配置 SSL 证书的总结。\n申请 SSL 证书\n你可以从任何证书提供商处申请证书，这里以阿里云为例。\n\n打开阿里云 SSL 证书控制台，点击购买证书\n选择免费型一年期的证书，点击立即购买注意，1 年到期后别忘记重新申请证书！\n\n支付放心大胆的支付吧，不用钱！\n\n验证 SSL 证书购买完成之后，返回 SSL 证书控制台，你应该会看到刚才购买的证书。我们点击申请\n填写域名（必须是你自己的或者有管理权的域名）和相关信息，完成后点击下一步。注意，免费型证书只支持单个域名！例如你要为 www.example.com 申请证书，你必须填写 www.example.com，而不能是 example.com。\n\n在 DNS 服务商处配置阿里云提供的验证信息。例如 DNSPod，填写主机记录，记录值和记录类型，然后点击保存。\n\n耐心等待 TTL 刷新（一般为 10 分钟，也可能花不了 10 分钟）。\n回到阿里云 SSL 证书申请页面，点击验证。\n\n签发域名验证通过后，证书提供商将会为你的域名颁发证书。在阿里云 SSL 证书控制台的已签发列表下可以找到你的域名对应的 SSL 证书。\n\n下载证书下载 Nginx 对应的 SSL 证书 xx_nginx.zip，准备配置 Nginx。\n\n配置 Nginx如果你还没有安装 Nginx，可以参考部署 Nginx\n上传证书$ sudo mkdir /etc/nginx/certs$ sudo cd /etc/nginx/certs## 上传你的证书至此目录$ sudo ls -ldrwxr-xr-x 2 root root 4096 Jul 24 17:15 ./drwxr-xr-x 7 root root 4096 Jul 24 17:15 ../-rw-r--r-- 1 root root 4053 Jul 24 16:49 xx_nginx.zip$ sudo unzip xx_nginx.zip$ sudo ls -l-rw-r--r-- 1 root root 1679 Jul 24 16:48 xx.key ## ssl cert key-rw-r--r-- 1 root root 3667 Jul 24 16:48 xx.pem ## ssl cert\n\n一切准备就绪后，可以开始修改我们的 Nginx 配置文件了。\n修改 Nginx 配置文件将 Http 修改为 Https 非常简单，只需要修改一处内容，并添加若干代码。\n\n将 listen 80; 修改为 listen 443;\n在 server 块中添加以下代码 \n\nssl on;ssl_certificate certs/xx.pem;ssl_certificate_key certs/xx.key;ssl_session_timeout 5m;\n\n修改完成后，重启 Nginx\n$ sudo service nginx reload$ sudo service nginx restart\n\n好了，使用 Https 访问你的网站吧。\nHttp 强制转向 Https注意，以上修改完成后，只能使用 Https 访问了，但是往往我们不希望用户使用 Http 访问的时候出现 404 的情况。那么，我们可以简单的将 80 端口的用户转发到 443 端口，来达到 Http 和 Https 共存的状态。\n在 Nginx 配置文件中添加\nserver {        listen 80;        server_name xx.xx.com;        return 301 https://$server_name$request_uri;}\n\n重启 Nginx\n","categories":["个人笔记"],"tags":["Nginx"]},{"title":"Redis 常用命令","url":"/p/1091/","content":"前言Redis 提供了丰富的命令（command）对数据库和各种数据类型进行操作，这些 command 可以在 Linux 终端使用。在编程时，比如各类语言包，这些命令都有对应的方法。下面将 Redis 提供的命令做一总结。\n \n\n键值相关命令keys\n返回满足给定 pattern 的所有 key\n\n127.0.0.1:6379&gt; keys * 1) \"mylist4\" 2) \"myset7\" 3) \"name1\" 4) \"myset3\" 5) \"myset2\" 6) \"mylist2\" 7) \"mylist6\" 8) \"name\" 9) \"myhash\"10) \"mylist7\"11) \"key1\"12) \"mylist5\"13) \"mylist8\"14) \"myzset2\"15) \"myzset3\"16) \"myzset\"17) \"myset5\"18) \"myset4\"19) \"mylist3\"20) \"myset\"21) \"myset6\"22) \"age\"23) \"mylist\"24) \"key2\"\n\n用表达式 *，代表取出所有的 key。\n127.0.0.1:6379&gt; keys mylist*1) \"mylist4\"2) \"mylist2\"3) \"mylist6\"4) \"mylist7\"5) \"mylist5\"6) \"mylist8\"7) \"mylist3\"8) \"mylist\"\n\n用表达式 mylist*，代表取出所有以 mylist 开头的 key。\nexists\n确认一个 key 是否存在\n\n127.0.0.1:6379&gt; exists HongWan(integer) 0127.0.0.1:6379&gt; exists age(integer) 1\n\n从结果来数据库中不存在 HongWan 这个 key，但是 age 这个 key 是存在的。\ndel\n删除一个 key\n\n127.0.0.1:6379&gt; del age(integer) 1127.0.0.1:6379&gt; exists age(integer) 0\n\nexpire\n设置一个 key 的过期时间 (单位：秒)\n\n127.0.0.1:6379&gt; exists addr(integer) 1127.0.0.1:6379&gt; ttl addr(integer) -1127.0.0.1:6379&gt; expire addr 10(integer) 1127.0.0.1:6379&gt; ttl addr(integer) 6127.0.0.1:6379&gt; ttl addr(integer) 5127.0.0.1:6379&gt; ttl addr(integer) 4127.0.0.1:6379&gt; ttl addr(integer) -2127.0.0.1:6379&gt; exists addr(integer) 0\n\n可以看到，未设置过期时间时，ttl 值为 - 1，设置 10s 过期后，不断地使用 ttl 获取 key 的有效时长，当值为 - 2 时，表示已过期并被删除。\nmove\n将当前数据库中的 key 转移到其它数据库中\n\n127.0.0.1:6379&gt; select 0OK127.0.0.1:6379&gt; set age 30OK127.0.0.1:6379&gt; get age\"30\"127.0.0.1:6379&gt; move age 1(integer) 1127.0.0.1:6379&gt; get age(nil)127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; get age\"30\"\n\n在本例中，我先显式的选择了数据库 0，然后在这个库中设置一个 key，接下来我们将这个 key 从数据库 0 移到数据库 1，之后我们确认在数据库 0 中无此 key 了，但在数据库 1 中存在这个 key，说明我们转移成功了 。\npersist\n移除给定 key 的过期时间\n\n127.0.0.1:6379[1]&gt; expire age 300(integer) 1127.0.0.1:6379[1]&gt; ttl age(integer) 296127.0.0.1:6379[1]&gt; persist age(integer) 1127.0.0.1:6379[1]&gt; ttl age(integer) -1\n\n在这个例子中，我们手动的将未到过期时间的 key，成功设置为过期。\nrandomkey\n随机返回 key 空间的一个 key\n\n127.0.0.1:6379&gt; randomkey\"mylist5\"127.0.0.1:6379&gt; randomkey\"myzset2\"\n\n通过结果可以看到取 key 的规则是随机的。\nrename\n重命名 key\n\n127.0.0.1:6379[1]&gt; keys *1) \"age\"127.0.0.1:6379[1]&gt; rename age age_newOK127.0.0.1:6379[1]&gt; keys *1) \"age_new\"\n\nage 成功的被我们改名为 age_new 了。\ntype\n返回值的类型\n\n127.0.0.1:6379&gt; type namestring127.0.0.1:6379&gt; type mysetset127.0.0.1:6379&gt; type myzsetzset\n\n服务器相关命令ping\n测试连接是否存活\n\n127.0.0.1:6379&gt; pingPONG// 执行下面命令之前，我们停止redis服务器127.0.0.1:6379&gt; pingCould not connect to Redis at 127.0.0.1:6379: Connection refused// 执行下面命令之前，我们启动redis服务器not connected&gt; pingPONG\n\necho\n在命令行打印一些内容\n\n127.0.0.1:6379&gt; echo HongWan\"HongWan\"\n\nselect\n选择数据库。Redis 数据库编号从 0~15，我们可以选择任意一个数据库来进行数据的存取\n\n127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; select 16(error) ERR invalid DB index\n\nquit\n退出连接\n\n127.0.0.1:6379&gt; quitroot@test01:~# \n\ndbsize\n返回当前数据库中 key 的数目\n\n127.0.0.1:6379&gt; dbsize(integer) 23\n\n结果说明此库中有 23 个 key。\ninfo\n获取服务器的信息和统计\n\n127.0.0.1:6379&gt; info# Serverredis_version:3.0.6redis_git_sha1:00000000redis_git_dirty:0redis_build_id:28b6715d3583bf8eredis_mode:standaloneos:Linux 4.4.0-148-generic x86_64arch_bits:64multiplexing_api:epollgcc_version:5.4.0...\n\n此结果用于说明服务器的基础信息，包括版本、启动时间等。\nmonitor\n实时转储收到的请求\n\n先在终端 1 输入 monitor 命令，将会进入等待状态\n127.0.0.1:6379&gt; monitorOK\n\n新建一个终端，输入一些 redis 命令\n127.0.0.1:6379&gt; keys * 1) \"myset3\" 2) \"myset2\" 3) \"mylist7\" 4) \"mylist4\" 5) \"key1\" 6) \"myset7\" 7) \"name1\" 8) \"mylist6\" 9) \"myzset\"10) \"mylist2\"11) \"myset\"12) \"mylist\"13) \"myhash\"14) \"myset4\"15) \"name\"16) \"myset5\"17) \"myzset3\"18) \"mylist3\"19) \"mylist5\"20) \"myzset2\"21) \"mylist8\"22) \"key2\"23) \"myset6\"127.0.0.1:6379&gt; get addr(nil)\n\n回到终端 1 中，我们将会看到打印出了刚才我们在终端 2 中敲入的 redis 命令\n127.0.0.1:6379&gt; monitorOK1558844434.297954 [0 127.0.0.1:34926] \"keys\" \"*\"1558844444.673315 [0 127.0.0.1:34926] \"get\" \"addr\"\n\nconfig get\n获取服务器配置信息\n\n127.0.0.1:6379&gt; config get dir1) \"dir\"2) \"/var/lib/redis\"\n\n本例中我们获取了 dir 这个参数配置的值，如果想获取全部参数据的配置值也很简单，只需执行”config get *” 即可将全部的值都显示出来。\nflushdb\n删除当前选择数据库中的所有 key\n\n127.0.0.1:6379&gt; dbsize(integer) 23127.0.0.1:6379&gt; flushdbOK127.0.0.1:6379&gt; dbsize(integer) 0\n\n在本例中我们将 0 号数据库中的 key 都清除了。\nflushall\n删除所有数据库中的所有 key\n\n127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; dbsize(integer) 1127.0.0.1:6379[1]&gt; select 0OK127.0.0.1:6379&gt; flushallOK127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; dbsize(integer) 0\n\n在本例中我们先查看了一个 1 号数据库中有一个 key，然后我切换到 0 号库执行 flushall 命令，结果 1 号库中的 key 也被清除了，说明此命令工作正常。\n数据相关命令👉Redis - 数据类型及操作\n","categories":["个人笔记"],"tags":["Redis","NoSql"]},{"title":"Scala 学习笔记","url":"/p/37757/","content":"写在前面Scala 是一门优秀的编程语言，它是一门纯面向对象的语言，且支持函数式编程。\nScala 运行于 Jvm，所有 Scala 的代码，都需要经过编译为字节码，然后交由 Java 虚拟机来运行。Scala 和 Java 是可以无缝互操作的。Scala 可以任意调用 Java 的代码。\n\n\n安装 Scala\n从 Scala 官方网站下载，http://www.scala-lang.org/download/，windows 版本的安装包是 `scala-2.11.7.msi`。\n\n使用下载下来的安装包安装 Scala。\n\n在 PATH 环境变量中，配置 $SCALA_HOME/bin 目录。\n\n\n\n\n在 windows 命令行内即可直接键入 scala，打开 scala 命令行，进行 scala 编程。\n\n$ scalaWelcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_231).Type in expressions for evaluation. Or try :help.scala&gt;\n\n基础语法Scala 解释器的使用REPL\nscala 解释器也被称为 REPL，会快速编译 scala 代码为字节码，然后交给 JVM 来执行。\n\nRead（取值）-&gt; Evaluation（求值）-&gt; Print（打印）-&gt; Loop（循环）。\n计算表达式\n在 scala &gt; 命令行内，键入 scala 代码，解释器会直接返回结果给你。如果你没有指定变量来存放这个值，那么值默认的名称为 res，而且会显示结果的数据类型，比如 Int、Double、String 等等。\n\n例如，输入 1 + 1，会看到 res0: Int = 2\nscala&gt; 1 + 1res0: Int = 2\n\n内置变量\n在后面可以继续使用 res 这个变量，以及它存放的值。\n\n例如，2.0 * res0，返回 res1: Double = 4.0\nscala&gt; 2.0 * res0res1: Double = 4.0\n\n例如，”Hi, “ + res0，返回 res2: String = Hi, 2\nscala&gt; \"Hi, \" + res0res2: String = Hi, 2\n\n自动补全\n在 scala &gt; 命令行内，可以使用 Tab 键进行自动补全。\n\n例如，输入 res2.to，敲击 Tab 键，解释器会显示出以下选项，toCharArray，toLowerCase，toString，toUpperCase。因为此时无法判定你需要补全的是哪一个，因此会提供给你所有的选项。\nscala&gt; res2.toto          toCharArray    toIterable    toMap ...\n\n例如，输入 res2.toU，敲击 Tab 键，直接会给你补全为 res2.toUpperCase。\n声明变量声明 val 变量\n可以声明 val 变量来存放表达式的计算结果。\n\n\n例如，val result = 1 + 1\n\nscala&gt; val result = 1 + 1result: Int = 2\n\n\n后续这些常量是可以继续使用的，例如，2 * result\n\nscala&gt; 2 * resultres6: Int = 4\n\n\n但是常量声明后，是无法改变它的值的，例如，result = 1，会返回 error: reassignment to val 的错误信息。\n\nscala&gt; result = 1&lt;console&gt;:12: error: reassignment to val       result = 1              ^\n\n声明 var 变量\n如果要声明值可以改变的引用，可以使用 var 变量。\n\n\n例如，val myresult = 1，myresult = 2\n\n但是在 Scala 程序中，通常建议使用 val，也就是常量，因此比如类似于 spark 的大型复杂系统中，需要大量的网络传输数据，如果使用 var，可能会担心值被错误的更改。\n\n在 Java 的大型复杂系统的设计和开发中，也使用了类似的特性，我们通常会将传递给其他模块 / 组件 / 服务的对象，设计成不可变类（Immutable Class）。在里面也会使用 Java 的常量定义，比如 final，阻止变量的值被改变。从而提高系统的健壮性（robust，鲁棒性），和安全性。\n\n\n指定类型\n无论声明 val 变量，还是声明 var 变量，都可以手动指定其类型，如果不指定的话，Scala 会自动根据值，进行类型的推断。\n\n\n例如，val name: String = null\n\n例如，val name: Any = \"leo\"\n\n\n","categories":["个人笔记"],"tags":["Scala"]},{"title":"Spring Event 事件驱动","url":"/p/18285/","content":"Spring 事件驱动模型，简单来说类似于 Message-Queue 消息队列中的 Pub/Sub 发布 / 订阅模式，也类似于 Java 设计模式中的观察者模式。\n\n\n自定义事件Spring 的事件接口位于 org.springframework.context.ApplicationEvent，源码如下：\npublic abstract class ApplicationEvent extends EventObject {\tprivate static final long serialVersionUID = 7099057708183571937L;\tprivate final long timestamp;\tpublic ApplicationEvent(Object source) {\t\tsuper(source);\t\tthis.timestamp = System.currentTimeMillis();\t}\tpublic final long getTimestamp() {\t\treturn this.timestamp;\t}}\n\n继承了 Java 的事件对象 EventObject，所以可以使用 getSource() 方法来获取到事件传播对象。\n自定义 Spring 事件public class CustomSpringEvent extends ApplicationEvent {    private String message;    public CustomSpringEvent(Object source, String message) {        super(source);        this.message = message;    }    public String getMessage() {        return message;    }}\n\n然后定义事件监听器，该监听器实际上等同于消费者，需要交给 Spring 容器管理。\n@Componentpublic class CustomSpringEventListener implements ApplicationListener&lt;CustomSpringEvent&gt; {    @Override    public void onApplicationEvent(CustomSpringEvent event) {        System.out.println(\"Received spring custom event - \" + event.getMessage());    }}\n\n最后定义事件发布者\n@Componentpublic class CustomSpringEventPublisher {    @Autowired    private ApplicationEventPublisher applicationEventPublisher;    public void doStuffAndPublishAnEvent(final String message) {        System.out.println(\"Publishing custom event. \");        CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message);        applicationEventPublisher.publishEvent(customSpringEvent);    }}\n\n创建测试类\n@RunWith(SpringRunner.class)@SpringBootTestpublic class CustomSpringEventPublisherTest {    @Autowired    private CustomSpringEventPublisher publisher;    @Test    public void publishStringEventTest() {        publisher.doStuffAndPublishAnEvent(\"111\");    }}\n\n运行测试类，可以看到控制台打印了两条重要信息\n//发布事件Publishing custom event. //监听器得到了事件，并相应处理Received spring custom event - 111\n\n由于 Spring 事件是发布 / 订阅的模式，而发布订阅模式有以下三种情况\n\n1 生产者 - 1 消费者\n 1 生产者 - 多消费者\n多生产者 - 多消费者\n\n上面举的例子是第一种情况，我们来试试其他两个情况\n继续创建一个事件监听器作为消费者：\n@Componentpublic class CustomSpringEventListener2 implements ApplicationListener&lt;CustomSpringEvent&gt; {    @Override    public void onApplicationEvent(CustomSpringEvent event) {        System.out.println(\"CustomSpringEventListener2 Received spring custom event - \" + event.getMessage());    }}\n\n运行测试类后，可以观察到，控制台顺序打印了两条消费信息：\nPublishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111\n\n说明，Spring 的发布订阅模式是广播模式，所有消费者都能接受到消息，并正常消费\n再试试第三种多生产者 - 多消费者的情况\n继续创建一个发布者，\n@Componentpublic class CustomSpringEventPublisher2 {    @Autowired    private ApplicationEventPublisher applicationEventPublisher;    public void doStuffAndPublishAnEvent(final String message) {        System.out.println(\"CustomSpringEventPublisher2 Publishing custom event. \");        CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message);        applicationEventPublisher.publishEvent(customSpringEvent);    }}\n\n控制台输出：\nCustomSpringEventPublisher Publishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111CustomSpringEventPublisher2 Publishing custom event. CustomSpringEventListener1 Received spring custom event - 222CustomSpringEventListener2 Received spring custom event - 222\n\n从以上输出内容，我们可以猜测到，Spring 的事件发布订阅机制是同步进行的，也就是说，事件必须被所有消费者消费完成之后，发布者的代码才能继续往下走，这显然不是我们想要的效果，那有没有异步执行的事件呢？\nSpring 中的异步事件要使用 Spring 的异步事件，我们需要自定义异步事件配置类\n@Configurationpublic class AsynchronousSpringEventsConfig {    @Bean(name = \"applicationEventMulticaster\")    public ApplicationEventMulticaster simpleApplicationEventMulticaster() {        SimpleApplicationEventMulticaster eventMulticaster                = new SimpleApplicationEventMulticaster();        eventMulticaster.setTaskExecutor(new SimpleAsyncTaskExecutor());        return eventMulticaster;    }}\n\n发布和订阅的代码不用变动，直接运行测试类，控制台将打印出：\nCustomSpringEventPublisher Publishing custom event. CustomSpringEventPublisher2 Publishing custom event. CustomSpringEventListener1 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 111CustomSpringEventListener2 Received spring custom event - 222CustomSpringEventListener1 Received spring custom event - 222\n\n可以看到，两个发布者几乎同时运行，证明监听器是异步执行的，没有阻塞住发布者的代码。准确的说，监听器将在一个单独的线程中异步处理事件。\nSpring 自带的事件类型事件驱动在 Spring 中是被广泛采用的，我们查看 ApplicationEvent 的子类可以发现许多 Event 事件，在此不赘述。\n\n注解驱动的监听器从 Spring 4.2 开始，事件监听器不需要是实现 ApplicationListener 接口的 bean，它可以通过 @EventListener 注解在任何被 Spring 容器管理的 bean 的公共方法上。\n@Componentpublic class AnnotationDrivenContextStartedListener {    @EventListener    public void handleContextStart(CustomSpringEvent cse) {        System.out.println(\"Handling Custom Spring Event.\");    }}\n\n控制台输出结果：\nCustomSpringEventPublisher Publishing custom event.Handling Custom Spring Event.CustomSpringEventPublisher2 Publishing custom event. Handling Custom Spring Event.\n\n同样的，我们可以看出，这个事件监听器是同步执行的，如果要改为异步监听器，在事件方法上加上 @Async，并且在 Spring 应用中开启异步支持 (在 SpringBootApplication 上添加 @EnableAsync)。\n@Componentpublic class AnnotationDrivenContextStartedListener {    @Async    @EventListener    public void handleContextStart(CustomSpringEvent cse) {        System.out.println(\"Handling Custom Spring Event.\");    }}\n\n再次运行测试类:\nCustomSpringEventPublisher Publishing custom event. CustomSpringEventPublisher2 Publishing custom event. Handling Custom Spring Event.Handling Custom Spring Event.\n\n泛型支持创建一个通用泛型事件模型\n@Datapublic class GenericSpringEvent&lt;T&gt; {    private T message;    protected boolean success;    public GenericSpringEvent(T what, boolean success) {        this.message = what;        this.success = success;    }}\n\n注意 GenericSpringEvent 和 CustomSpringEvent 之间的区别。我们现在可以灵活地发布任何任意事件，并且不再需要从 ApplicationEvent 扩展。\n这样的话，我们无法像之前一样，通过继承 ApplicationListener 的方式来定义一个监听器，因为 ApplicationListener 定义了事件必须是 ApplicationEvent 的子类。所以，我们只能使用注解驱动的监听器。\n通过在 @EventListener 注释上定义布尔 SpEL 表达式，也可以使事件监听器成为条件。在这种情况下，只会为成功的 String 的 GenericSpringEvent 调用事件处理程序：\n@Componentpublic class AnnotationDrivenEventListener {    @EventListener(condition = \"#event.success\")    public void handleSuccessful(GenericSpringEvent&lt;String&gt; event) {        System.out.println(\"Handling generic event (conditional).\");    }}\n\n定义具体类型的事件:\npublic class StringGenericSpringEvent extends GenericSpringEvent&lt;String&gt; {    public StringGenericSpringEvent(String message, boolean success) {        super(message, success);    }}\n\n定义发布者：\n@Componentpublic class StringGenericSpringEventPublisher {    @Autowired    private ApplicationEventPublisher applicationEventPublisher;    public void doStuffAndPublishAnEvent(final String message, final boolean success) {        System.out.println(\"CustomSpringEventPublisher Publishing custom event. \");        StringGenericSpringEvent springEvent = new StringGenericSpringEvent(message, success);        applicationEventPublisher.publishEvent(springEvent);    }}\n\n测试类：\n@RunWith(SpringRunner.class)@SpringBootTestpublic class CustomSpringEventPublisherTest {    @Autowired    private StringGenericSpringEventPublisher publisher;    @Test    public void publishStringEventTest() {        publisher.doStuffAndPublishAnEvent(\"success\", true);        publisher.doStuffAndPublishAnEvent(\"failed\", false);    }}\n\n运行结果：\nCustomSpringEventPublisher Publishing custom event. Handling generic event (conditional) successCustomSpringEventPublisher Publishing custom event. \n\n监听器只处理了成功的事件，成功忽略掉了失败的事件。这样的好处是，可以为同一个事件定义成功和失败不同的操作。\nSpring 事件的事务绑定从 Spring 4.2 开始，框架提供了一个新的 @TransactionalEventListener 注解，它是 @EventListener 的扩展，允许将事件的侦听器绑定到事务的一个阶段。绑定可以进行以下事务阶段：\n\nAFTER_COMMIT (默认的)：在事务成功后触发\n AFTER_ROLLBACK: 事务回滚时触发\n AFTER_COMPLETION：事务完成后触发，不论是否成功\n BEFORE_COMMIT：事务提交之前触发\n\n总结\nSpring 中处理事件的基础知识：创建一个简单的自定义事件，发布它，然后在监听器中处理它。\n在配置中启用事件的异步处理。\nSpring 4.2 中引入的改进，例如注释驱动的侦听器，更好的泛型支持以及绑定到事务阶段的事件。\n\n👉 本文代码地址\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Spring IoC Container 源码分析（二）-bean 初始化流程","url":"/p/60483/","content":"准备Person 实例\n@Datapublic class Person {    private String name;    private int age;}\n\n\nxml bean 配置\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:context=\"http://www.springframework.org/schema/context\"       xsi:schemaLocation=\"http://www.springframework.org/schema/beans                        http://www.springframework.org/schema/beans/spring-beans.xsd                        http://www.springframework.org/schema/context                        http://www.springframework.org/schema/context/spring-context.xsd\"&gt;    &lt;bean id=\"person\" class=\"com.gcdd1993.spring.framework.base.domain.Person\"/&gt;&lt;/beans&gt;\n\n入口\nAbstractApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"config.xml\");applicationContext.getBean(\"person\");\n\n使用 Debug 进入 ClassPathXmlApplicationContext 构造函数，源码如下\npublic ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent)        throws BeansException {    super(parent);    setConfigLocations(configLocations);    if (refresh) {        refresh();    }}\n\nsuper(parent)一步步向上调用父类构造函数，路径为\nClassPathXmlApplicationContext -&gt; AbstractXmlApplicationContext -&gt; AbstractRefreshableConfigApplicationContext -&gt; AbstractRefreshableApplicationContext -&gt; AbstractApplicationContext\n历经整个继承体系，最终到达 AbstractApplicationContext:\npublic AbstractApplicationContext(ApplicationContext parent) {    this();    setParent(parent);}\n\n最后会设置当前 ApplicationContext 的父级 ApplicationContext\nsetConfigLocations(configLocations)设置配置文件路径，解析的细节参照官方文档 Resource 一节，不是本文讨论的重点，在此略过。\npublic void setConfigLocations(String... locations) {    if (locations != null) {        Assert.noNullElements(locations, \"Config locations must not be null\");        this.configLocations = new String[locations.length];        for (int i = 0; i &lt; locations.length; i++) {            this.configLocations[i] = resolvePath(locations[i]).trim();        }    }    else {        this.configLocations = null;    }}\n\nrefresh()此方法是 Spring 容器的核心方法，源码 (精简了 try catch 部分) 如下：\npublic void refresh() throws BeansException, IllegalStateException {    // Prepare this context for refreshing.    prepareRefresh();    // Tell the subclass to refresh the internal bean factory.    ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();    // Prepare the bean factory for use in this context.    prepareBeanFactory(beanFactory);    // Allows post-processing of the bean factory in context subclasses.    postProcessBeanFactory(beanFactory);    // Invoke factory processors registered as beans in the context.    invokeBeanFactoryPostProcessors(beanFactory);    // Register bean processors that intercept bean creation.    registerBeanPostProcessors(beanFactory);    // Initialize message source for this context.    initMessageSource();    // Initialize event multicaster for this context.    initApplicationEventMulticaster();    // Initialize other special beans in specific context subclasses.    onRefresh();    // Check for listener beans and register them.    registerListeners();    // Instantiate all remaining (non-lazy-init) singletons.    finishBeanFactoryInitialization(beanFactory);    // Last step: publish corresponding event.    finishRefresh();}\n\n此处可以看到 Spring 编码方式近似于流程图的，重点部分都抽出为了单独的方法，流程清晰，易于理解。我们一步步看：\nprepareRefresh()\n上下文刷新前预热\n\nprotected void prepareRefresh() {    this.startupDate = System.currentTimeMillis();    this.closed.set(false);    this.active.set(true);    if (logger.isInfoEnabled()) {        logger.info(\"Refreshing \" + this);    }    // Initialize any placeholder property sources in the context environment    initPropertySources();    // Validate that all properties marked as required are resolvable    // see ConfigurablePropertyResolver#setRequiredProperties    getEnvironment().validateRequiredProperties();    // Allow for the collection of early ApplicationEvents,    // to be published once the multicaster is available...    this.earlyApplicationEvents = new LinkedHashSet&lt;ApplicationEvent&gt;();}\n\n\n设置上下文基本信息，如 startupDate (启动时刻)、closed (是否关闭)、active (是否存活) 等等。\n解析占位符资源，并验证标记为 required 的资源是否可用\n\nobtainFreshBeanFactory()\n初始化 beanFactory (bean 工厂，实际存放 bean 的就是它了)\n\nprotected ConfigurableListableBeanFactory obtainFreshBeanFactory() {    refreshBeanFactory();    ConfigurableListableBeanFactory beanFactory = getBeanFactory();    if (logger.isDebugEnabled()) {        logger.debug(\"Bean factory for \" + getDisplayName() + \": \" + beanFactory);    }    return beanFactory;}\n\n核心方法 refreshBeanFactory()\nprotected final void refreshBeanFactory() throws BeansException {    if (hasBeanFactory()) {        destroyBeans();        closeBeanFactory();    }    try {        DefaultListableBeanFactory beanFactory = createBeanFactory();        beanFactory.setSerializationId(getId());        customizeBeanFactory(beanFactory);        loadBeanDefinitions(beanFactory);        synchronized (this.beanFactoryMonitor) {            this.beanFactory = beanFactory;        }    }    catch (IOException ex) {        throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex);    }}\n\n\ncreateBeanFactory();\n 设置 beanFactory 属性\n loadBeanDefinitions(beanFactory);\n\nloadBeanDefinitions(beanFactory)\n解析 bean 定义，有几个 bean 就有几个 BeanDefinition。注意，Spring 并不是拿到配置就直接用反射实例化 bean，而是先将 bean 配置解析为 BeanDefinition。\n\nBeanDefinition 保存了实例化 bean 需要的一切信息，包括属性，依赖等。以 ConcurrentHashMap&lt;String, BeanDefinition&gt; 保存在 DefaultListableBeanFactory 的 beanDefinitionMap 里。\n\nprepareBeanFactory(beanFactory)\n设置 beanFactory 的其余属性\n\npostProcessBeanFactory(beanFactory)\n空实现，给子类一个机会，自定义 beanFactory 后置处理器\n\nBeanFactoryPostProcessor 定义：\npublic interface BeanFactoryPostProcessor {    void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;}\n\ninvokeBeanFactoryPostProcessors(beanFactory)\n执行上一步中的 beanFactory 后置处理器的回调方法 void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory)\n\nregisterBeanPostProcessors(beanFactory)\n注册 bean 后置处理器，实现 bean 初始化前后的自定义逻辑\n\nBeanPostProcessor 定义：\npublic interface BeanPostProcessor {    // 在bean实例化前调用    Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException;    // 在bean实例化后调用    Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;}\n\ninitMessageSource()\n注册国际化相关 bean\n\ninitApplicationEventMulticaster()\n初始化 Spring 事件发布相关 bean\n\nonRefresh()\n空实现，给子类一个机会，初始化特殊 bean\n\nregisterListeners()\n注册监听器\n\nfinishBeanFactoryInitialization(beanFactory)\n实例化所有非懒加载的 bean\n\n直到这里，才开始真正实例化 bean\nprotected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) {    // 1. 实例化bean的类型转换器    if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp;            beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) {        beanFactory.setConversionService(                beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class));    }    // 2. 实例化属性占位符解析器    if (!beanFactory.hasEmbeddedValueResolver()) {        beanFactory.addEmbeddedValueResolver(new StringValueResolver() {            @Override            public String resolveStringValue(String strVal) {                return getEnvironment().resolvePlaceholders(strVal);            }        });    }    // 3. 实例化LoadTimeWeaverAware    String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false);    for (String weaverAwareName : weaverAwareNames) {        getBean(weaverAwareName);    }    // 4. 停止使用临时ClassLoader进行类型匹配    beanFactory.setTempClassLoader(null);    // 5. 禁止再修改bean定义    beanFactory.freezeConfiguration();    // 6. 实例化所有非懒加载单例bean    beanFactory.preInstantiateSingletons();}\n\npreInstantiateSingletons()\n根据每一个 bean 定义，实例化 bean\n 为每一个实现 SmartInitializingSingleton 的 bean 执行回调方法\n\n实例化 bean 部分的代码：\nfor (String beanName : beanNames) {    // 获取bean定义    RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);    // 只有不是abstract、单例且不是懒加载的bean才在这里实例化    if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) {        // 如果是FactoryBean        if (isFactoryBean(beanName)) {            // 先实例化实例对应的FactoryBean            final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName);            boolean isEagerInit;            if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) {                isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() {                    @Override                    public Boolean run() {                        return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit();                    }                }, getAccessControlContext());            }            else {                isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp;                        ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit());            }            if (isEagerInit) {                // 使用FactoryBean的getObject()方法返回真正的实例                getBean(beanName);            }        }        else {            getBean(beanName);        }    }}\n\ngetBean(String name)该方法调用了一个 doGetBean，doGetBean 代码较长，而且有部分代码是为了解决并发场景下单例的生成，我们挑出重点的看：\n\n从父 BeanFactory 检查是否存在该 bean 的定义，如果存在，委托父 BeanFactory 来实例化 \n\nBeanFactory parentBeanFactory = getParentBeanFactory();if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) {    // Not found -&gt; check parent.    String nameToLookup = originalBeanName(name);    if (args != null) {        // Delegation to parent with explicit args.        return (T) parentBeanFactory.getBean(nameToLookup, args);    }    else {        // No args -&gt; delegate to standard getBean method.        return parentBeanFactory.getBean(nameToLookup, requiredType);    }}\n\n\n获得 bean 定义，如果存在依赖，先实例化每一个依赖 bean，注意：不允许循环依赖 \n\nfinal RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName);checkMergedBeanDefinition(mbd, beanName, args);// Guarantee initialization of beans that the current bean depends on.String[] dependsOn = mbd.getDependsOn();//如果存在依赖，先实例化每一个依赖beanif (dependsOn != null) {    // 实例化每一个依赖bean    for (String dep : dependsOn) {        // 检查循环依赖        if (isDependent(beanName, dep)) {            throw new BeanCreationException(mbd.getResourceDescription(), beanName,                    \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\");        }        // 实例化依赖bean        registerDependentBean(dep, beanName);        try {            getBean(dep);        }        catch (NoSuchBeanDefinitionException ex) {            throw new BeanCreationException(mbd.getResourceDescription(), beanName,                    \"'\" + beanName + \"' depends on missing bean '\" + dep + \"'\", ex);        }    }}\n\n\n实例化 bean\n\n方法调用流程：\ncreateBean &gt; doCreateBean &gt; populateBean\n其中 doCreateBean：\n\n从 BeanDefinition 生成 BeanWrapper\n 将 BeanWrapper 和 BeanDefinition.getPropertyValues () 传给 populateBean，实例化 bean\n\nfinishRefresh()protected void finishRefresh() {    // 初始化生命周期处理器    initLifecycleProcessor();    // 刷新生命周期处理器状态 running = true    getLifecycleProcessor().onRefresh();    // 发布上下文初始化完成事件ContextRefreshedEvent    publishEvent(new ContextRefreshedEvent(this));    // 如果处于活动状态，将自己注册到LiveBeans    LiveBeansView.registerApplicationContext(this);}\n\n总结Spring IoC Container 时序图\n\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Redis 数据类型及操作","url":"/p/38807/","content":"前言作为 Key-value 型数据库，Redis 也提供了键（Key）和键值（Value）的映射关系。但是，除了常规的数值或字符串，Redis 的键值还可以是以下形式之一：\n\n[Lists （可重复列表）](#Lists （可重复列表） )\n[Sets （不可重复集合）](#Sets （不可重复集合）)\n[Sorted sets （不可重复有序集合）](#Sorted sets （不可重复有序集合）)\n[Hashes （哈希表）](#Hashes （哈希表）)\n\n键值的数据类型决定了该键值支持的操作。Redis 支持诸如列表、集合或有序集合的交集、并集、查集等高级原子操作；同时，如果键值的类型是普通数字，Redis 则提供自增等原子操作。\nstrings（字符串）\nstring 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据，比如 jpg 图片或者序列化的对象。\n\nset\n设置 key 对应的值为 string 类型的 value。\n\n127.0.0.1:6379&gt; set name wwlOK\n\nsetnx\n设置 key 对应的值为 string 类型的 value。如果 key 已经存在，返回 0，nx 是 not exist 的意思。\n\n127.0.0.1:6379&gt; get name\"wwl\"127.0.0.1:6379&gt; setnx name HongWan_new(integer) 0127.0.0.1:6379&gt; get name \"HongWan\"\n\n由于原来 name 有一个对应的值，所以本次的修改不生效，且返回码是 0。\nsetex\n设置 key 对应的值为 string 类型的 value，并指定此键值对应的有效期。\n\n127.0.0.1:6379&gt; setex haircolor 10 red OK127.0.0.1:6379&gt; get haircolor\"red\"127.0.0.1:6379&gt; get haircolor\"red\"127.0.0.1:6379&gt; get haircolor(nil)\n\n可见由于最后一次的调用是 10 秒以后了，所以取不到 haicolor 这个键对应的值。\nsetrange\n设置指定 key 的 value 值的子字符串。\n\n127.0.0.1:6379&gt; set name 'HongWan@126.com'OK127.0.0.1:6379&gt; get name \"HongWan@126.com\"127.0.0.1:6379&gt;  setrange name 8 gmail.com (integer) 17127.0.0.1:6379&gt; get name \"HongWan@gmail.com\"\n\n其中的 8 是指从下标为 8（包含 8）的字符开始替换。\nmset\n一次设置多个 key 的值，成功返回 ok 表示所有的值都设置了，失败返回 0 表示没有任何值被设置。\n\n127.0.0.1:6379&gt; mset key1 HongWan1 key2 HongWan2 OK127.0.0.1:6379&gt; get key1 \"HongWan1\"127.0.0.1:6379&gt; get key2\"HongWan2\"\n\nmsetnx\n一次设置多个 key 的值，成功返回 ok 表示所有的值都设置了，失败返回 0 表示没有任何值被设置，但是不会覆盖已经存在的 key。\n\n127.0.0.1:6379&gt; get key1\"HongWan1\"127.0.0.1:6379&gt; get key2\"HongWan2\"127.0.0.1:6379&gt; msetnx key2 HongWan2_new key3 HongWan3(integer) 0127.0.0.1:6379&gt; get key2 \"HongWan2\"127.0.0.1:6379&gt; get key3(nil)\n\n可以看出如果这条命令返回 0，那么里面操作都会回滚，都不会被执行。\nget\n获取 key 对应的 string 值，如果 key 不存在返回 nil。 \n\n127.0.0.1:6379&gt; get name \"HongWan_new\"## 我们获取一个库中不存在的键name1，那么它会返回一个nil以表时无此键值对 redis 127.0.0.1:6379&gt; get name1 (nil) \n\ngetset\n设置 key 的值，并返回 key 的旧值。\n\n127.0.0.1:6379&gt; get name \"HongWan\"127.0.0.1:6379&gt; getset name HongWan_new \"HongWan\"127.0.0.1:6379&gt; get name\"HongWan_new\"\n\n如果 key 不存在，将返回 nil，并会设置新值。\nredis 127.0.0.1:6379&gt; getset name1 aaa(nil) 127.0.0.1:6379&gt; get name1\"aaa\"\n\ngetrange\n获取指定 key 的 value 值的子字符串。\n\n127.0.0.1:6379&gt; get name \"HongWan_new\"127.0.0.1:6379&gt; getrange name 0 6 \"HongWan\"## 字符串左面下标是从0开始的127.0.0.1:6379&gt; getrange name -7 -1\"Wan_new\"## 字符串右面下标是从-1开始的127.0.0.1:6379&gt; getrange name 7 100 \"_new\"## 当下标超出字符串长度时，将默认为是同方向的最大下标\n\nmget\n一次获取多个 key 的值，如果对应 key 不存在，则对应返回 nil。 \n\n127.0.0.1:6379&gt; mget key1 key2 key3 1) \"HongWan1\"2) \"HongWan2\"3) (nil)## key3由于没有这个键定义，所以返回nil。 \n\nincr\n对 key 的值做加加操作，并返回新的值。注意 incr 一个不是 int 的 value 会返回错误，incr 一个不存在的 key，则设置 key 为 1 。\n\n127.0.0.1:6379&gt; set age 20OK127.0.0.1:6379&gt; incr age (integer) 21127.0.0.1:6379&gt; get age\"21\"\n\nincrby\n同 incr 类似，加指定值 ，key 不存在时候会设置 key，并认为原来的 value 是 0 。\n\n127.0.0.1:6379&gt; get age \"21\"127.0.0.1:6379&gt; incrby age 5(integer) 26127.0.0.1:6379&gt; get name\"HongWan_new\"127.0.0.1:6379&gt; get age\"26\"\n\ndecr\n对 key 的值做的是减减操作，decr 一个不存在 key，则设置 key 为 - 1。\n\n127.0.0.1:6379&gt; get age\"26\"127.0.0.1:6379&gt; decr age(integer) 25127.0.0.1:6379&gt; get age\"25\"\n\ndecrby\n同 decr，减指定值。\n\n127.0.0.1:6379&gt; get age\"25\"127.0.0.1:6379&gt; decrby age 5(integer) 20127.0.0.1:6379&gt; get age\"20\"\n\ndecrby 完全是为了可读性，我们完全可以通过 incrby 一个负值来实现同样效果，反之一样。\n127.0.0.1:6379&gt; get age\"20\"127.0.0.1:6379&gt; incrby age -5(integer) 15127.0.0.1:6379&gt; get age\"15\"\n\nappend\n给指定 key 的字符串值追加 value, 返回新字符串值的长度。 \n\n127.0.0.1:6379&gt; get name\"HongWan_new\"127.0.0.1:6379&gt; append name @126.com(integer) 19127.0.0.1:6379&gt; get name\"HongWan_new@126.com\"\n\nstrlen\n取指定 key 的 value 值的长度。\n\n127.0.0.1:6379&gt; get name\"HongWan_new@126.com\"127.0.0.1:6379&gt; strlen name(integer) 19127.0.0.1:6379&gt; get age\"15\"127.0.0.1:6379&gt; strlen age(integer) 2\n\nLists （可重复列表）list 是一个链表结构，主要功能是 push、pop、获取一个范围的所有值等等，操作中 key 理解为链表的名字。\nRedis 的 list 类型其实就是一个每个子元素都是 string 类型的双向链表。链表的最大长度是 (2^32)。我们可以通过 push,pop 操作从链表的头部或者尾部添加删除元素。这使得 list 既可以用作栈，也可以用作队列。 \nlpush\n在 key 对应 list 的头部添加字符串元素。\n\n127.0.0.1:6379&gt; lpush mylist \"world\"(integer) 1127.0.0.1:6379&gt; lpush mylist \"hello\"(integer) 2127.0.0.1:6379&gt;  lrange mylist 0 -11) \"hello\"2) \"world\"\n\n在此处我们先插入了一个 world，然后在 world 的头部插入了一个 hello。其中 lrange 是用于获取 mylist 的内容。\nrpush\n在 key 对应 list 的尾部添加字符串元素。\n\n127.0.0.1:6379&gt; rpush mylist2 \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist2 \"world\"(integer) 2127.0.0.1:6379&gt; lrange mylist2 0 -11) \"hello\"2) \"world\"\n\n在此处我们先插入了一个 hello，然后在 hello 的尾部插入了一个 world。\nlinsert\n在 key 对应 list 的特定位置之前或之后添加字符串元素。\n\n127.0.0.1:6379&gt; rpush mylist3 \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist3 \"world\"(integer) 2127.0.0.1:6379&gt; linsert mylist3 before \"world\" \"there\"(integer) 3127.0.0.1:6379&gt; lrange mylist3 0 -11) \"hello\"2) \"there\"3) \"world\"\n\n在此处我们先插入了一个 hello，然后在 hello 的尾部插入了一个 world，然后又在 world 的前面插入了 there。\nlset\n设置 list 中指定下标的元素值 (下标从 0 开始) 。\n\n127.0.0.1:6379&gt; rpush mylist4 \"one\"(integer) 1127.0.0.1:6379&gt; rpush mylist4 \"two\"(integer) 2127.0.0.1:6379&gt; rpush mylist4 \"three\"(integer) 3127.0.0.1:6379&gt; lset mylist4 0 \"four\"OK127.0.0.1:6379&gt; lset mylist4 -2 \"five\"OK127.0.0.1:6379&gt; lrange mylist4 0 -11) \"four\"2) \"five\"3) \"three\"\n\n在此处我们依次插入了 one,two,three，然后将标是 0 的值设置为 four，再将下标是 - 2 的值设置为 five。\nlrem\n从 key 对应 list 中删除 count 个和 value 相同的元素。\n\ncount&gt;0 时，按从头到尾的顺序删除。\n127.0.0.1:6379&gt; rpush mylist5 \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist5 \"hello\"(integer) 2127.0.0.1:6379&gt; rpush mylist5 \"foo\"(integer) 3127.0.0.1:6379&gt; rpush mylist5 \"hello\"(integer) 4127.0.0.1:6379&gt; lrem mylist5 2 \"hello\"(integer) 2127.0.0.1:6379&gt; lrange mylist5 0 -11) \"foo\"2) \"hello\"\n\ncount&lt;0 时，按从尾到头的顺序删除。\n127.0.0.1:6379&gt; rpush mylist6 \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist6 \"hello\"(integer) 2127.0.0.1:6379&gt; rpush mylist6 \"foo\"(integer) 3127.0.0.1:6379&gt; rpush mylist6 \"hello\"(integer) 4127.0.0.1:6379&gt; lrem mylist6 -2 \"hello\"(integer) 2127.0.0.1:6379&gt; lrange mylist6 0 -11) \"hello\"2) \"foo\"\n\ncount=0 时，删除全部。\n127.0.0.1:6379&gt; rpush mylist7 \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist7 \"hello\"(integer) 2127.0.0.1:6379&gt; rpush mylist7 \"foo\"(integer) 3127.0.0.1:6379&gt; rpush mylist7 \"hello\"(integer) 4127.0.0.1:6379&gt; lrem mylist7 0 \"hello\"(integer) 3127.0.0.1:6379&gt; lrange mylist7 0 -11) \"foo\"\n\nltrim\n保留指定 key 的值范围内的数据。\n\n127.0.0.1:6379&gt; rpush mylist8 \"one\"(integer) 1127.0.0.1:6379&gt; rpush mylist8 \"two\"(integer) 2127.0.0.1:6379&gt; rpush mylist8 \"three\"(integer) 3127.0.0.1:6379&gt; rpush mylist8 \"four\"(integer) 4127.0.0.1:6379&gt; ltrim mylist8 1 -1OK127.0.0.1:6379&gt; lrange mylist8 0 -11) \"two\"2) \"three\"3) \"four\"\n\nlpop\n从 list 的头部删除元素，并返回删除元素。\n\n127.0.0.1:6379&gt; lrange mylist 0 -11) \"hello\"2) \"world\"127.0.0.1:6379&gt; lpop mylist\"hello\"127.0.0.1:6379&gt; lrange mylist 0 -11) \"world\"\n\nrpop\n从 list 的尾部删除元素，并返回删除元素。\n\n127.0.0.1:6379&gt; lrange mylist2 0 -11) \"hello\"2) \"world\"127.0.0.1:6379&gt; rpop mylist2\"world\"127.0.0.1:6379&gt; lrange mylist2 0 -11) \"hello\"\n\nrpoplpush\n从第一个 list 的尾部移除元素并添加到第二个 list 的头部，最后返回被移除的元素值，整个操作是原子的。如果第一个 list 是空或者不存在返回 nil。\n\n127.0.0.1:6379&gt; lrange mylist5 0 -11) \"foo\"2) \"hello\"127.0.0.1:6379&gt; lrange mylist6 0 -11) \"hello\"2) \"foo\"127.0.0.1:6379&gt; rpoplpush mylist5 mylist6\"hello\"127.0.0.1:6379&gt; lrange mylist5 0 -11) \"foo\"127.0.0.1:6379&gt; lrange mylist6 0 -11) \"hello\"2) \"hello\"3) \"foo\"\n\nlindex\n返回名称为 key 的 list 中 index 位置的元素。\n\n127.0.0.1:6379&gt; lrange mylist5 0 -11) \"three\"2) \"foo\"127.0.0.1:6379&gt; lindex mylist5 0\"three\"127.0.0.1:6379&gt; lindex mylist5 1\"foo\"\n\nllen\n返回 key 对应 list 的长度。\n\n127.0.0.1:6379&gt; llen mylist5(integer) 2\n\nSets （不可重复集合）Redis 的 set 是 string 类型的无序集合。set 元素最大可以包含 (2^32) 个元素。\nset 的是通过 hash table 实现的，所以添加、删除和查找的复杂度都是 O (1)。hash table 会随着添加或者删除自动的调整大小。\nsadd\n向名称为 key 的 set 中添加元素。\n\n127.0.0.1:6379&gt; sadd myset \"hello\"(integer) 1127.0.0.1:6379&gt; sadd myset \"world\"(integer) 1127.0.0.1:6379&gt; sadd myset \"world\"(integer) 0127.0.0.1:6379&gt; smembers myset1) \"world\"2) \"hello\"\n\n本例中，我们向 myset 中添加了三个元素，但由于第三个元素跟第二个元素是相同的，所以第三个元素没有添加成功，最后我们用 smembers 来查看 myset 中的所有元素。\nsrem\n删除名称为 key 的 set 中的元素 member。\n\n127.0.0.1:6379&gt; sadd myset2 \"one\"(integer) 1127.0.0.1:6379&gt; sadd myset2 \"two\"(integer) 1127.0.0.1:6379&gt; sadd myset2 \"three\"(integer) 1127.0.0.1:6379&gt; srem myset2 \"one\"(integer) 1127.0.0.1:6379&gt; srem myset2 \"four\"(integer) 0127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"\n\n本例中，我们向 myset2 中添加了三个元素后，再调用 srem 来删除 one 和 four，但由于元素中没有 four 所以，此条 srem 命令执行失败。\nspop\n随机返回并删除名称为 key 的 set 中一个元素。\n\n127.0.0.1:6379&gt; sadd myset3 \"one\"(integer) 1127.0.0.1:6379&gt; sadd myset3 \"two\"(integer) 1127.0.0.1:6379&gt; sadd myset3 \"three\"(integer) 1127.0.0.1:6379&gt; spop myset3\"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"\n\n本例中，我们向 myset3 中添加了三个元素后，再调用 spop 来随机删除一个元素，可以看到 three 元素被删除了。\nsdiff\n返回所有给定 key 与第一个 key 的差集。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sdiff myset2 myset31) \"two\"\n\n本例中，我们可以看到 myset2 中的元素与 myset3 中不同的只是 three，所以只有 three 被查出来了，而不是 three 和 one，因为 one 是 myset3 的元素。 \n我们也可以将 myset2 和 myset3 换个顺序来看一下结果：\n127.0.0.1:6379&gt; sdiff myset3 myset21) \"one\"\n\n这个结果中只显示了，myset3 中的元素与 myset2 中不同的元素。\nsdiffstore返回所有给定 key 与第一个 key 的差集，并将结果存为另一个 key。\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sdiffstore myset4 myset2 myset3(integer) 1127.0.0.1:6379&gt; smembers myset41) \"two\"\n\nsinter\n返回所有给定 key 的交集。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sinter myset2 myset31) \"three\"\n\n通过本例的结果可以看出，myset2 和 myset3 的交集 two 被查出来了。\nsinterstore\n返回所有给定 key 的交集，并将结果存为另一个 key。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sinterstore myset5 myset2 myset3(integer) 1127.0.0.1:6379&gt; smembers myset51) \"three\"\n\n通过本例的结果可以看出，myset2 和 myset3 的交集被保存到 myset5 中了。\nsunion\n返回所有给定 key 的并集。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sunion myset2 myset31) \"three\"2) \"one\"3) \"two\"\n\n通过本例的结果可以看出，myset2 和 myset3 的并集被查出来了。\nsunionstore\n返回所有给定 key 的并集，并将结果存为另一个 key。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; sunionstore myset6 myset2 myset3(integer) 3127.0.0.1:6379&gt; smembers myset61) \"three\"2) \"one\"3) \"two\"\n\n通过本例的结果可以看出，myset2 和 myset3 的并集被保存到 myset6 中了。\nsmove\n从第一个 key 对应的 set 中移除 member 并添加到第二个对应 set 中。\n\n127.0.0.1:6379&gt; smembers myset21) \"three\"2) \"two\"127.0.0.1:6379&gt; smove myset2 myset7 three(integer) 1127.0.0.1:6379&gt; smembers myset71) \"three\"\n\n通过本例可以看到，myset2 的 three 被移到 myset7 中了。\nscard\n返回名称为 key 的 set 的元素个数。\n\n127.0.0.1:6379&gt; scard myset2(integer) 1\n\nsismember\n测试 member 是否是名称为 key 的 set 的元素。\n\n127.0.0.1:6379&gt; smembers myset21) \"two\"127.0.0.1:6379&gt; sismember myset2 two(integer) 1127.0.0.1:6379&gt; sismember myset2 one(integer) 0\n\n通过本例可以看到，two 是 myset2 的成员，而 one 不是。\nsrandmember\n随机返回名称为 key 的 set 的一个元素，但是不删除元素。\n\n127.0.0.1:6379&gt; smembers myset31) \"three\"2) \"one\"127.0.0.1:6379&gt; srandmember myset3\"three\"127.0.0.1:6379&gt; srandmember myset3\"one\"127.0.0.1:6379&gt; srandmember myset3\"one\"\n\n通过本例可以看到，第二次返回了元素”one”，但是并没有删除”one” 元素。\nSorted sets （不可重复有序集合）sorted set 是 set 的一个升级版本，它在 set 的基础上增加了一个顺序属性，这一属性在添加修改元素的时候可以指定，每次指定后，zset 会自动重新按新的值调整顺序。可以理解为有两列的 mysql 表，一列存 value，一列存顺序。\n和 set 一样 sorted set 也是 string 类型元素的集合，不同的是每个元素都会关联一个 double 类型的 score。sorted set 的实现是 skip list 和 hash table 的混合体。\nzadd\n向名称为 key 的 zset 中添加元素 member，score 用于排序。如果该元素已经存在，则根据 score 更新该元素的顺序。\n\n127.0.0.1:6379&gt; zadd myzset 1 \"one\"(integer) 1127.0.0.1:6379&gt; zadd myzset 2 \"two\"(integer) 1127.0.0.1:6379&gt; zadd myzset 3 \"two\"(integer) 0127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"3\"\n\nzrem\n删除名称为 key 的 zset 中的元素 member。\n\n127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"3\"127.0.0.1:6379&gt; zrem myzset two(integer) 1127.0.0.1:6379&gt; zrange myzset 0 -1 withscores1) \"one\"2) \"1\"\n\n可以看到 two 被删除了。\nzincrby\n如果在名称为 key 的 zset 中已经存在元素 member，则该元素的 score 增加 increment；否则向集合中添加该元素，其 score 的值为 increment。\n\n127.0.0.1:6379&gt; zadd myzset2 1 \"one\"(integer) 1127.0.0.1:6379&gt; zadd myzset2 2 \"two\"(integer) 1127.0.0.1:6379&gt; zincrby myzset2 2 \"one\"\"3\"127.0.0.1:6379&gt; zrange myzset2 0 -1 withscores1) \"two\"2) \"2\"3) \"one\"4) \"3\"\n\n本例中将 one 的 score 从 1 增加了 2，增加到了 3。\nzrank\n返回名称为 key 的 zset 中 member 元素的排名 (按 score 从小到大排序) 即下标。\n\n127.0.0.1:6379&gt; zadd myzset3 1 \"one\"(integer) 1127.0.0.1:6379&gt; zadd myzset3 2 \"two\"(integer) 1127.0.0.1:6379&gt; zadd myzset3 3 \"three\"(integer) 1127.0.0.1:6379&gt; zadd myzset3 5 \"five\"(integer) 1127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"2\"5) \"three\"6) \"3\"7) \"five\"8) \"5\"127.0.0.1:6379&gt; zrank myzset3 two(integer) 1\n\nzrevrank\n返回名称为 key 的 zset 中 member 元素的排名 (按 score 从大到小排序) 即下标。\n\n127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"2\"5) \"three\"6) \"3\"7) \"five\"8) \"5\"127.0.0.1:6379&gt; zrevrank myzset3 two(integer) 2\n\n按从大到小排序的话 two 是第三个元素，下标是 2。\nzrevrange\n返回名称为 key 的 zset（按 score 从大到小排序）中的 index 从 start 到 end 的所有元素。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"five\"2) \"5\"3) \"three\"4) \"3\"5) \"two\"6) \"2\"7) \"one\"8) \"1\"\n\nzrangebyscore\n返回集合中 score 在给定区间的元素。\n\n127.0.0.1:6379&gt; zrange myzset3 0 -1 withscores1) \"one\"2) \"1\"3) \"two\"4) \"2\"5) \"three\"6) \"3\"7) \"five\"8) \"5\"127.0.0.1:6379&gt; zrangebyscore myzset3 2 3 withscores1) \"two\"2) \"2\"3) \"three\"4) \"3\"\n\nzcount\n返回集合中 score 在给定区间的数量。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"five\"2) \"5\"3) \"three\"4) \"3\"5) \"two\"6) \"2\"7) \"one\"8) \"1\"127.0.0.1:6379&gt; zcount myzset3 2 3(integer) 2\n\nzcard\n返回集合中元素个数。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"five\"2) \"5\"3) \"three\"4) \"3\"5) \"two\"6) \"2\"7) \"one\"8) \"1\"127.0.0.1:6379&gt; zcard myzset3(integer) 4\n\nzscore\n返回给定元素对应的 score。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"five\"2) \"5\"3) \"three\"4) \"3\"5) \"two\"6) \"2\"7) \"one\"8) \"1\"127.0.0.1:6379&gt;  zscore myzset3 two\"2\"\n\nzremrangebyrank\n删除集合中排名在给定区间的元素。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"five\"2) \"5\"3) \"three\"4) \"3\"5) \"two\"6) \"2\"7) \"one\"8) \"1\"127.0.0.1:6379&gt; zremrangebyrank myzset3 3 3(integer) 1127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"three\"2) \"3\"3) \"two\"4) \"2\"5) \"one\"6) \"1\"\n\n在本例中我们将 myzset3 中按从小到大排序结果的下标为 3 的元素删除了。\nzremrangebyscore\n删除集合中 score 在给定区间的元素。\n\n127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"three\"2) \"3\"3) \"two\"4) \"2\"5) \"one\"6) \"1\"127.0.0.1:6379&gt; zremrangebyscore myzset3 1 2(integer) 2127.0.0.1:6379&gt; zrevrange myzset3 0 -1 withscores1) \"three\"2) \"3\"\n\n在本例中我们将 myzset3 中按从小到大排序结果的 score 在 1~2 之间的元素删除了。\nHashes （哈希表）Redis hash 是一个 string 类型的 field 和 value 的映射表。它的添加、删除操作都是 O (1)（平均），hash 特别适合用于存储对象。\n相较于将对象的每个字段存成单个 string 类型，将一个对象存储在 hash 类型中会占用更少的内存，并且可以更方便的存取整个对象。\nhset\n设置 hash field 为指定值，如果 key 不存在，则先创建。\n\n127.0.0.1:6379&gt; hset myhash field1 Hello(integer) 1\n\nhsetnx\n设置 hash field 为指定值，如果 key 不存在，则先创建。如果 field 已经存在，返回 0，nx 是 not exist 的意思。\n\n127.0.0.1:6379&gt; hsetnx myhash field \"Hello\"(integer) 1127.0.0.1:6379&gt; hsetnx myhash field \"Hello\"(integer) 0\n\n第一次执行是成功的，但第二次执行相同的命令失败，原因是 field 已经存在了。 \nhmset\n同时设置 hash 的多个 field。\n\n127.0.0.1:6379&gt; hmset myhash field1 Hello field2 WorldOK\n\nhget\n获取指定的 hash field。\n\n127.0.0.1:6379&gt; hget myhash field1\"Hello\"127.0.0.1:6379&gt; hget myhash field2\"World\"127.0.0.1:6379&gt; hget myhash field3(nil)\n\n由于数据库没有 field3，所以取到的是一个空值 nil。\nhmget\n获取全部指定的 hash filed。\n\n127.0.0.1:6379&gt; hmget myhash field1 field2 field31) \"Hello\"2) \"World\"3) (nil)\n\n由于数据库没有 field3，所以取到的是一个空值 nil。\nhincrby\n给指定的 hash filed 加上给定值。\n\n127.0.0.1:6379&gt; hset myhash field3 20(integer) 1127.0.0.1:6379&gt; hget myhash field3 \"20\"127.0.0.1:6379&gt; hincrby myhash field3 -8(integer) 12127.0.0.1:6379&gt; hget myhash field3\"12\"\n\n在本例中我们将 field3 的值从 20 降到了 12，即做了一个减 8 的操作。\nhexists\n测试指定 field 是否存在。\n\n127.0.0.1:6379&gt; hexists myhash field1(integer) 1127.0.0.1:6379&gt; hexists myhash field9(integer) 0\n\n通过上例可以说明 field1 存在，但 field9 是不存在的。\nhlen\n返回指定 hash 的 field 数量。\n\n127.0.0.1:6379&gt; hlen myhash(integer) 4\n\n通过上例可以看到 myhash 中有 4 个 field。\nhdel\n删除指定 hash 的指定 field。\n\n127.0.0.1:6379&gt; hlen myhash(integer) 4127.0.0.1:6379&gt; hdel myhash field1(integer) 1127.0.0.1:6379&gt; hlen myhash(integer) 3\n\nhkeys\n返回 hash 的所有 field。\n\n127.0.0.1:6379&gt;  hkeys myhash1) \"field\"2) \"field2\"3) \"field3\"\n\nhvals\n返回 hash 的所有 value。\n\n127.0.0.1:6379&gt; hvals myhash1) \"Hello\"2) \"World\"3) \"12\"\n\nhgetall\n获取某个 hash 中全部的 filed 及 value。\n\n127.0.0.1:6379&gt; hgetall myhash1) \"field\"2) \"Hello\"3) \"field2\"4) \"World\"5) \"field3\"6) \"12\"\n\n一下子将 myhash 中所有的 field 及对应的 value 都取出来了。\n","categories":["个人笔记"],"tags":["Redis","NoSql"]},{"title":"Spring Boot Starter 开发指南","url":"/p/20136/","content":"Spring Boot Starter 是什么？依赖管理是任何复杂项目的关键部分。以手动的方式来实现依赖管理不太现实，你得花更多时间，同时你在项目的其他重要方面能付出的时间就会变得越少。\nSpring Boot starter 就是为了解决这个问题而诞生的。Starter POM 是一组方便的依赖描述符，您可以将其包含在应用程序中。您可以获得所需的所有 Spring 和相关技术的一站式服务，无需通过示例代码搜索和复制粘贴依赖。\n\n\n揭开 Spring Boot 自动装配的神秘面纱Auto Configuration 类当 Spring Boot 启动时，它会在类路径中查找名为 spring.factories 的文件。该文件位于 META-INF 目录中。让我们看一下 spring-boot-autoconfigure 项目中这个文件的片段：\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration\n\n此文件定义了一些 Spring Boot 将尝试运行的自动装配类。例如以上的代码片段，Spring Boot 将尝试运行 RabbitMQ，Cassandra，MongoDB 和 Hibernate 的所有配置类。这些类是否实际运行将取决于类路径上是否存在依赖类。例如，如果在类路径中找到 MongoDB 的类，则将运行 MongoAutoConfiguration，并初始化所有与 mongo 相关的 bean。此条件初始化由 @ConditionalOnClass 注释启用。让我们看一下 MongoAutoConfiguration 类的代码片段，看看它的用法：\n@Configuration@ConditionalOnClass(MongoClient.class)@EnableConfigurationProperties(MongoProperties.class)@ConditionalOnMissingBean(type = \"org.springframework.data.mongodb.MongoDbFactory\")public class MongoAutoConfiguration {    // configuration code}\n\n如果存在 MongoClient 类，将运行该自动装配类初始化 MongoClient 相关 bean。\n在 application.properties 自定义配置Spring Boot 使用一些预先配置的默认值初始化 bean。要覆盖这些默认值，我们通常会在 application.properties 文件中使用某个特定名称声明它们。Spring Boot 容器会自动获取这些属性。在 MongoAutoConfiguration 的代码片段中，@EnableConfigurationProperties(MongoProperties.class) 表示，使用 MongoProperties 类来声明自定义属性：\n@ConfigurationProperties(prefix = \"spring.data.mongodb\")public class MongoProperties {     private String host;     // other fields with standard getters and setters}\n\n@ConfigurationProperties(prefix = \"spring.data.mongodb\") 定义了配置前缀，我们可以在 application.properties 这样来使用它：\nspring.data.mongodb.host = localhost\n\n这样，初始化的时候，localhost 将被注入到 host 属性中\n自定义 Spring Boot StarterSpring Boot 自动装配虽然神奇，但是编写起来却异常简单，我们只需要按部就班的执行以下两个流程：\n\n编写属性容器 *Properties，并编写对应的 *AutoConfiguration 自动装配类\n一个 pom 文件，用于定义引入库和自动装配类的依赖项\n\n概念解析用于 *Properties 的注解\n@ConfigurationProperties(prefix = \"spring.data.mongodb\") ：用于指定配置前缀\n\n用于 *AutoConfiguration 的注解\n@Configuration：标记为配置类，由 Spring 容器初始化并接管\n @EnableConfigurationProperties：注入配置属性容器\n @ConditionalOnBean：条件装配\n\n重点说下条件装配，以 @ConditionalOnBean 为例，当 Spring 容器中存在指定 Bean 的时候装配\n@Target({ ElementType.TYPE, ElementType.METHOD })@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnBeanCondition.class)public @interface ConditionalOnBean{    //properties}\n\n@Conditional(OnBeanCondition.class) 指定了实现条件装配的逻辑代码\nOnBeanCondition 声明如下：\nclass OnBeanCondition extends SpringBootCondition implements ConfigurationCondition{}\n\n所以，我们自己也可以继承 SpringBootCondition 并实现 ConfigurationCondition 来自定义条件装配注解。\n比较常用的几个条件装配注解：\n\n@ConditionalOnBean：当 Spring 容器中存在指定 Bean 时装配\n @ConditionalOnClass：当存在指定 Class 时装配\n @ConditionalOnMissingBean：当 Spring 容器中不存在指定 Bean 时装配\n @ConditionalOnMissingClass：当不存在指定 Class 时装配\n\n小试牛刀\n我们将自动配置模块称为 greeter-spring-boot-autoconfigure。该模块将有两个主要类，即 GreeterProperties，它将通过 application.properties 文件和 GreeterAutoConfiguartion 设置自定义属性，并为 greeter 库创建 bean。\n\n准备，创建假想的一个第三方工程：Greet\npublic class Greeter {    private GreetingConfig greetingConfig;    public Greeter(GreetingConfig greetingConfig) {        this.greetingConfig = greetingConfig;    }    public String greet(LocalDateTime localDateTime) {        String name = greetingConfig.getProperty(USER_NAME);        int hourOfDay = localDateTime.getHour();        if (hourOfDay &gt;= 5 &amp;&amp; hourOfDay &lt; 12) {            return String.format(\"Hello %s, %s\", name, greetingConfig.get(MORNING_MESSAGE));        } else if (hourOfDay &gt;= 12 &amp;&amp; hourOfDay &lt; 17) {            return String.format(\"Hello %s, %s\", name, greetingConfig.get(AFTERNOON_MESSAGE));        } else if (hourOfDay &gt;= 17 &amp;&amp; hourOfDay &lt; 20) {            return String.format(\"Hello %s, %s\", name, greetingConfig.get(EVENING_MESSAGE));        } else {            return String.format(\"Hello %s, %s\", name, greetingConfig.get(NIGHT_MESSAGE));        }    }    public String greet() {        return greet(LocalDateTime.now());    }}public class GreeterConfigParams {    public static final String USER_NAME = \"user.name\";    public static final String MORNING_MESSAGE = \"morning.message\";    public static final String AFTERNOON_MESSAGE = \"afternoon.message\";    public static final String EVENING_MESSAGE = \"evening.message\";    public static final String NIGHT_MESSAGE = \"night.message\";}public class GreetingConfig extends Properties {    private static final long serialVersionUID = 5662570853707247891L;}\n\n编写 Properties 和 AutoConfiguration：\n@ConfigurationProperties(prefix = \"gcdd1993.greeter\")public class GreeterProperties {    private String userName;    private String morningMessage;    private String afternoonMessage;    private String eveningMessage;    private String nightMessage;    // getter and setter}@Configuration@ConditionalOnClass(Greeter.class)@EnableConfigurationProperties(GreeterProperties.class)public class GreeterAutoConfiguration {    @Autowired    private GreeterProperties greeterProperties;    @Bean    @ConditionalOnMissingBean    public GreetingConfig greeterConfig() {        String userName = greeterProperties.getUserName() == null                ? System.getProperty(\"user.name\")                : greeterProperties.getUserName();        GreetingConfig greetingConfig = new GreetingConfig();        greetingConfig.put(USER_NAME, userName);        if (greeterProperties.getMorningMessage() != null) {            greetingConfig.put(MORNING_MESSAGE, greeterProperties.getMorningMessage());        }        if (greeterProperties.getAfternoonMessage() != null) {            greetingConfig.put(AFTERNOON_MESSAGE, greeterProperties.getAfternoonMessage());        }        if (greeterProperties.getEveningMessage() != null) {            greetingConfig.put(EVENING_MESSAGE, greeterProperties.getEveningMessage());        }        if (greeterProperties.getNightMessage() != null) {            greetingConfig.put(NIGHT_MESSAGE, greeterProperties.getNightMessage());        }        return greetingConfig;    }    @Bean    @ConditionalOnMissingBean    public Greeter greeter(GreetingConfig greetingConfig) {        return new Greeter(greetingConfig);    }}\n\n然后在 src/main/resources/META-INF 目录下创建 spring.factories 文件\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\  com.gcdd.autoconfigure.GreeterAutoConfiguration\n\n测试一下：\n\n创建配置文件 application.properties：\n\ngcdd1993.greeter.userName=gcdd1993gcdd1993.greeter.eveningMessage=good evening\n\n\n使用 Greeter bean\n\n@SpringBootApplicationpublic class GreeterSampleApplication implements CommandLineRunner {    @Autowired    private Greeter greeter;    public static void main(String[] args) {        SpringApplication.run(GreeterSampleApplication.class, args);    }    @Override    public void run(String... args) throws Exception {        String message = greeter.greet();        System.out.println(message);    }}\n\n执行 main 方法，将会输出一行：\nHello gcdd1993, good evening\n\n为配置类添加提示我们知道，在 Idea 中，编写配置文件的时候，有智能提示\n\n其实这不是 Idea 搞的鬼，是由 META-INF/spring-configuration-metadata.json 文件配置好的，Idea 只是负责解析这个文件，提供我们智能化的提示信息。\n想要达到这个目的很简单，添加依赖 org.springframework.boot:spring-boot-configuration-processor 就行了。\nMaven\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;    &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt;\n\nGradle\ncompile group: 'org.springframework.boot', name: 'spring-boot-configuration-processor', version: '2.1.6.RELEASE'\n\n总结以上就是 Spring Boot Starter 的全部内容了，如果要发布到 maven 仓库，供别人使用，可以使用 mvn install 打包发布至 maven 仓库。\n👉 本文代码地址\n","categories":["个人笔记"],"tags":["Spring Boot"]},{"title":"Spring Mvc Http 400 Bad Request 问题排查","url":"/p/6604/","content":"如果遇到了 Spring MVC 报错 400，而且没有返回任何信息的情况下该如何排查问题？\n问题描述一直都没毛病的接口，今天测试的时候突然报错 400 Bad Request，而且 Response 没有返回任何信息。\n\n\n解决方案尝试了一下午，终于找到了排查这类问题的办法。\n我们知道，在 Spring MVC 里面，org.springframework.web.servlet.mvc.method.annotation.ResponseEntityExceptionHandler负责所有异常的统一处理。我们只要在方法 handleException 打上断点即可。\n\n点开发现，原来是 Lombok 的问题。报错如下\nCould not read JSON document: Can not construct instance of xxx: no suitable constructor found, can not deserialize from Object value (missing default constructor or creator, or perhaps need to add/enable type information?) at [Source: java.io.PushbackInputStream@12544acd; line: 2, column: 5]\n\nLombok 没有为我们自动生成类的构造函数。我们在目标类加上 @NoArgsConstructor 即可解决。\n刨根问底为什么 Lombok 自动生成的类，没有可供 Jackson 反序列化的构造函数呢？我看了一下生成的字节码文件，里面确实不存在无参构造和全参构造函数，唯一的构造函数是带一个参数的。\n目标类使用了 @Data 注解，而 @Data 注解的声明如下\n/** * Generates getters for all fields, a useful toString method, and hashCode and equals implementations that check * all non-transient fields. Will also generate setters for all non-final fields, as well as a constructor. * &lt;p&gt; * Equivalent to {@code @Getter @Setter @RequiredArgsConstructor @ToString @EqualsAndHashCode}. * &lt;p&gt; * Complete documentation is found at &lt;a href=\"https://projectlombok.org/features/Data\"&gt;the project lombok features page for &amp;#64;Data&lt;/a&gt;. *  * @see Getter * @see Setter * @see RequiredArgsConstructor * @see ToString * @see EqualsAndHashCode * @see lombok.Value */@Target(ElementType.TYPE)@Retention(RetentionPolicy.SOURCE)public @interface Data {\tString staticConstructor() default \"\";}\n\n简单来说，@Data 包含了以下注解的功能\n\n@Getter\n@Setter\n@RequiredArgsConstructor\n@ToString\n@EqualsAndHashCode\n\n而 “罪魁祸首” 就是 @RequiredArgsConstructor 了，它的作用是\n\n为每个需要特殊处理的字段（final 修饰的或者是 @NotNull 注释的字段）生成一个带有 1 个参数的构造函数。\n\n而目标类恰巧有一个字段就是 @NotNull 注解修饰的，所以生成了单参构造函数。\n参考\nLombok Features\nSpring MVC 自定义全局异常处理\n\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Spring-Security 无法正常捕捉到 UsernameNotFoundException 异常","url":"/p/31514/","content":"前言在 Web 应用开发中，安全一直是非常重要的一个方面。在庞大的 spring 生态圈中，权限校验框架也是非常完善的。其中，spring security 是非常好用的。今天记录一下在开发中遇到的一个 spring-security 相关的问题。\n问题描述使用 spring security 进行授权登录的时候，发现登录接口无法正常捕捉 UsernameNotFoundException 异常，捕捉到的一直是 BadCredentialsException 异常。我们的预期是：\n\nUsernameNotFoundException -&gt; 用户名错误\n BadCredentialsException -&gt; 密码错误\n\n贴几个比较重要的代码：\n1. 登录业务逻辑@Servicepublic class AuthServiceImpl implements AuthService {    @Autowired    private UserDetailsService userDetailsService;    @Autowired    private AuthenticationManager authenticationManager;    @Autowired    private JwtTokenUtil jwtTokenUtil;    @Override    public JwtAuthenticationResponse login(String username, String password) {\t\t//构造spring security需要的UsernamePasswordAuthenticationToken        UsernamePasswordAuthenticationToken upToken = new UsernamePasswordAuthenticationToken(username, password);\t\t//调用authenticationManager.authenticate(upToken)方法验证\t\t//该方法将会执行UserDetailsService的loadUserByUsername验证用户名\t\t//以及PasswordEncoder的matches方法验证密码        val authenticate = authenticationManager.authenticate(upToken);        JwtUser userDetails = (JwtUser) authenticate.getPrincipal();        val token = jwtTokenUtil.generateToken(userDetails);        return new JwtAuthenticationResponse(token, userDetails.getId(), userDetails.getUsername());    }}\n\n2. spring security 的 UserDetailsService 实现类@Servicepublic class JwtUserDetailsServiceImpl implements UserDetailsService {    @Autowired    private UserRepository userRepository;    @Override    public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException {        AbstractUser abstractUser = userRepository.findByUsername(username);\t\t//如果通过用户名找不到用户，则抛出UsernameNotFoundException异常        if (abstractUser == null) {            throw new UsernameNotFoundException(String.format(\"No abstractUser found with username '%s'.\", username));        } else {            return JwtUserFactory.create(abstractUser);        }    }}\n\n3. 登录接口try {    final JwtAuthenticationResponse jsonResponse = authService.login(authenticationRequest.getUsername(), authenticationRequest.getPassword());    //存入redis    redisService.setToken(jsonResponse.getToken());    return ok(jsonResponse);} catch (BadCredentialsException e) {\t//捕捉到BadCredentialsException，密码不正确    return forbidden(LOGIN_PASSWORD_ERROR, request);} catch (UsernameNotFoundException e) {\t//捕捉到UsernameNotFoundException，用户名不正确    return forbidden(LOGIN_USERNAME_ERROR, request);}\n\n在上述代码中，如果用户名错误，应该执行catch (UsernameNotFoundException e) {    return forbidden(LOGIN_USERNAME_ERROR, request);}\n\n如果密码错误，应该执行catch (BadCredentialsException e) {    return forbidden(LOGIN_PASSWORD_ERROR, request);}\n\n实际上，不管是抛出什么错，最后抓到的都是 BadCredentialsException\n问题定位debug 大法断点\n跟踪经过步进法跟踪代码，发现问题所在，位于\nAbstractUserDetailsAuthenticationProviderpublic Authentication authenticate(Authentication authentication)\n\n\n结论\nloadUserByUsername 方法确实抛出了 UsernameNotFoundException\n 走到 AbstractUserDetailsAuthenticationProvider 的 authenticate 方法的时候，如果 hideUserNotFoundExceptions = true，直接就覆盖了 UsernameNotFoundException 异常并抛出 BadCredentialsException 异常，这也就解释了，为什么总是捕捉到 BadCredentialsException 异常\n\n问题解决既然已经找到了是因为 hideUserNotFoundExceptions = true 导致的问题，那把 hideUserNotFoundExceptions = false 不就完事了吗？\n方案 1参考 stackoverflow 大神回答\n修改 WebSecurityConfig 配置，添加 AuthenticationProvider Bean@Beanpublic AuthenticationProvider daoAuthenticationProvider() {    DaoAuthenticationProvider daoAuthenticationProvider = new DaoAuthenticationProvider();    daoAuthenticationProvider.setUserDetailsService(userDetailsService);    daoAuthenticationProvider.setPasswordEncoder(passwordEncoder());    daoAuthenticationProvider.setHideUserNotFoundExceptions(false);    return daoAuthenticationProvider;}\n\n配置 AuthenticationProvider Bean@Autowiredpublic void configureAuthentication(AuthenticationManagerBuilder authenticationManagerBuilder) throws Exception {    authenticationManagerBuilder            .authenticationProvider(daoAuthenticationProvider());}\n\n方案 2由于以前项目中也是一样的技术栈，而且代码也差不多，登录这段逻辑可以说是完全相同，不过之前就一直都没有这个问题。反复查看之后发现，在 login 的代码有些不同\n在\nval authenticate = authenticationManager.authenticate(upToken);\n\n前面还有一个\n//执行UserDetailsService的loadUserByUsername验证用户名userDetailsService.loadUserByUsername(authenticationRequest.getUsername());\n\n该方法会直接抛出 UsernameNotFoundException，而不走 spring security 的 AbstractUserDetailsAuthenticationProvider，也就不存在被转换为 BadCredentialsException 了。\n但是这个方案有个缺点，\n如果验证用户名通过以后，再次调用\nval authenticate = authenticationManager.authenticate(upToken);\n\n还会再执行一遍\nuserDetailsService.loadUserByUsername(authenticationRequest.getUsername());\n\n该操作是冗余的，产生了不必要的数据库查询工作。\n推荐使用方案 1","categories":["个人笔记"],"tags":["Spring Security"]},{"title":"Spring Cloud feign 使用 okhttp3","url":"/p/7382/","content":"spring cloud feign 使用 okhttp3\n指南maven\n&lt;dependency&gt;    &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt;    &lt;artifactId&gt;feign-okhttp&lt;/artifactId&gt;&lt;/dependency&gt;\n\n配置文件\nfeign.httpclient.enabled=falsefeign.okhttp.enabled=true\n\n配置\n@Configuration@ConditionalOnClass(Feign.class)@AutoConfigureBefore(FeignAutoConfiguration.class)public class FeignOkHttpConfig {    @Autowired    OkHttpLoggingInterceptor okHttpLoggingInterceptor;    @Bean    public okhttp3.OkHttpClient okHttpClient(){        return new okhttp3.OkHttpClient.Builder()            .readTimeout(60, TimeUnit.SECONDS)             .connectTimeout(60, TimeUnit.SECONDS)             .writeTimeout(120, TimeUnit.SECONDS)             .connectionPool(new ConnectionPool())            // .addInterceptor();            .build();    }}\n\n实践不需要额外编写 FeignOkHttpConfig，feign 本身已经存在 FeignOkHttpAutoConfiguration 了，不需要额外配置。\n","categories":["个人笔记"],"tags":["Spring Cloud"]},{"title":"Spring-Framework - 官方文档阅读（一）Spring IoC Container","url":"/p/433/","content":"前言通读 Spring IoC 容器官方文档，对 IoC 容器有一个大致的了解。\n\n\n环境\nJDK1.8\nSpring Framework Version ：4.3.18.RELEASE\n\n容器概述\n接口 org.springframework.context.ApplicationContext 代表 Spring IoC 容器，负责实例化，配置和组装 bean。在独立应用程序中，通常会创建一个 ClassPathXmlApplicationContext 或者 FileSystemXmlApplicationContext 的实例。\n\n\n\nSpring 工作原理的高级视图\n1. 配置元数据创建 SimpleBean\npublic class SimpleBean {    public void send() {        System.out.println(\"Hello Spring Bean!\");    }}\n\nconfig.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:context=\"http://www.springframework.org/schema/context\"       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\t\t\t\t\t\thttp://www.springframework.org/schema/beans/spring-beans.xsd\t\t\t\t\t\thttp://www.springframework.org/schema/context\t\t\t\t\t\thttp://www.springframework.org/schema/context/spring-context.xsd\"&gt;    &lt;bean id=\"simple\" class=\"base.SimpleBeanFactoryBean\"/&gt;&lt;/beans&gt;\n\n2. 实例化容器\nApplicationContext context = new ClassPathXmlApplicationContext(\"config.xml\");\n\n3. 使用容器\n// 检索Spring容器中的beanSimpleBean simpleBean = context.getBean(SimpleBean.class);// 使用beansimpleBean.send();\n\n还有更灵活的方式来从配置文件获取 bean，使用 GenericApplicationContext 与 BeanDefinitionReader 结合，直接读取 bean 定义\nGenericApplicationContext context = new GenericApplicationContext();new XmlBeanDefinitionReader(context).loadBeanDefinitions(\"config.xml\");context.refresh();SimpleBean simpleBean = (SimpleBean) context.getBean(\"simple\");simpleBean.send();\n\nBean 概述\nSpring IoC 容器管理一个或多个 bean。这些 bean 是使用您提供给容器的配置元数据创建的，例如，以 XML &lt;bean/&gt; 定义的形式 。\n\n在容器本身内，这些 bean 定义表示为 BeanDefinition 对象。\n除了创建配置好的 bean 之外，ApplicationContext 还允许用户注册在容器外部创建的现有对象。通过 getBeanFactory() 获得 DefaultListableBeanFactory，然后使用 registerSingleton() 或者 registerBeanDefinition() 来注册 bean。\nDefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory();ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"config.xml\");User user = new User();user.setId(1L);user.setName(\"xiaoming\");beanFactory.registerSingleton(\"user\", user);User bean = (User) applicationContext.getBean(\"user\");System.out.println(bean);\n\n或者是以下做法：\nClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"config.xml\");DefaultListableBeanFactory beanFactory = (DefaultListableBeanFactory) applicationContext.getBeanFactory();BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(User.class);builder.addPropertyValue(\"id\", 1);builder.addPropertyValue(\"name\", \"xiaoming\");AbstractBeanDefinition beanDefinition = builder.getBeanDefinition();beanFactory.registerBeanDefinition(\"user\", beanDefinition);User bean = (User) applicationContext.getBean(\"user\");System.out.println(bean);\n\n命名 bean每个 bean 都有一个或多个标识符。这些标识符在托管 bean 的容器中必须是唯一的。bean 通常只有一个标识符，但如果它需要多个标识符，则额外的标识符可以被视为别名。\n在基于 XML 的配置元数据中，使用 id 和 / 或 name 属性指定 bean 标识符。\n实例化 bean1. 构造函数实例化\n&lt;bean id=\"exampleBean\" class=\"examples.ExampleBean\"/&gt;&lt;bean name=\"anotherExample\" class=\"examples.ExampleBeanTwo\"/&gt;\n\n2. 静态工厂方法实例化\n&lt;bean id=\"clientService\"    class=\"examples.ClientService\"    factory-method=\"createInstance\"/&gt;\n\npublic class ClientService {    private static ClientService clientService = new ClientService();    private ClientService() {}    public static ClientService createInstance() {        return clientService;    }}\n\n3. 实例工厂方法实例化\n&lt;!-- the factory bean, which contains a method called createInstance() --&gt;&lt;bean id=\"serviceLocator\" class=\"examples.DefaultServiceLocator\"&gt;    &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;!-- the bean to be created via the factory bean --&gt;&lt;bean id=\"clientService\"    factory-bean=\"serviceLocator\"    factory-method=\"createClientServiceInstance\"/&gt;\n\npublic class DefaultServiceLocator {    private static ClientService clientService = new ClientServiceImpl();    public ClientService createClientServiceInstance() {        return clientService;    }}\n\n一个工厂类也可以包含多个工厂方法:\n&lt;bean id=\"serviceLocator\" class=\"examples.DefaultServiceLocator\"&gt;    &lt;!-- inject any dependencies required by this locator bean --&gt;&lt;/bean&gt;&lt;bean id=\"clientService\"    factory-bean=\"serviceLocator\"    factory-method=\"createClientServiceInstance\"/&gt;&lt;bean id=\"accountService\"    factory-bean=\"serviceLocator\"    factory-method=\"createAccountServiceInstance\"/&gt;\n\npublic class DefaultServiceLocator {    private static ClientService clientService = new ClientServiceImpl();    private static AccountService accountService = new AccountServiceImpl();    public ClientService createClientServiceInstance() {        return clientService;    }    public AccountService createAccountServiceInstance() {        return accountService;    }}\n\n依赖注入构造器注入\n基于构造函数的 DI 由容器调用具有多个参数的构造函数来完成，每个参数表示一个依赖项。\n\npublic class SimpleMovieLister {    // SimpleMovieLister依赖于MovieFinder    private MovieFinder movieFinder;    // 一个构造函数，以便Spring容器可以注入一个MovieFinder    public SimpleMovieLister(MovieFinder movieFinder) {        this.movieFinder = movieFinder;    }    // business logic that actually uses the injected MovieFinder is omitted...}\n\n构造函数参数解析\npackage x.y;public class Foo {    public Foo(Bar bar, Baz baz) {        // ...    }}\n\n&lt;beans&gt;    &lt;bean id=\"foo\" class=\"x.y.Foo\"&gt;        &lt;constructor-arg ref=\"bar\"/&gt;        &lt;constructor-arg ref=\"baz\"/&gt;    &lt;/bean&gt;    &lt;bean id=\"bar\" class=\"x.y.Bar\"/&gt;    &lt;bean id=\"baz\" class=\"x.y.Baz\"/&gt;&lt;/beans&gt;\n\n显式指定构造函数参数的类型：\n&lt;bean id=\"exampleBean\" class=\"examples.ExampleBean\"&gt;    &lt;constructor-arg type=\"int\" value=\"7500000\"/&gt;    &lt;constructor-arg type=\"java.lang.String\" value=\"42\"/&gt;&lt;/bean&gt;\n\n使用 index 属性显式指定构造函数参数的索引：\n&lt;bean id=\"exampleBean\" class=\"examples.ExampleBean\"&gt;    &lt;constructor-arg index=\"0\" value=\"7500000\"/&gt;    &lt;constructor-arg index=\"1\" value=\"42\"/&gt;&lt;/bean&gt;\n\n或者指定构造函数参数名称：\n&lt;bean id=\"exampleBean\" class=\"examples.ExampleBean\"&gt;    &lt;constructor-arg name=\"years\" value=\"7500000\"/&gt;    &lt;constructor-arg name=\"ultimateAnswer\" value=\"42\"/&gt;&lt;/bean&gt;\n\nsetter 注入\n基于 setter 的 DI 是在调用无参数构造函数或无参数 static 工厂方法来实例化 bean 之后，通过容器调用 bean 上的 setter 方法来完成的。\n\npublic class SimpleMovieLister {    // SimpleMovieLister依赖于MovieFinder    private MovieFinder movieFinder;    // 一个setter方法，以便Spring容器可以注入一个MovieFinder    public void setMovieFinder(MovieFinder movieFinder) {        this.movieFinder = movieFinder;    }    // business logic that actually uses the injected MovieFinder is omitted...}\n\n小结ApplicationContext 的依赖注入支持构造器注入和 setter 注入两种方式。在通过构造函数方法注入了一些依赖项之后，它还支持基于 setter 的依赖注入。可以用 BeanDefinition 与 PropertyEditor 实例结合使用的方式来配置依赖项。 不过，我们一般不直接使用 BeanDefinition 与 PropertyEditor，而是用 XML 定义 bean 或者是注解方式（@Component， @Controller 等等），或者是直接编写 @Configuration 类。然后，这些类在内部转换为实例 BeanDefinition 并用于加载整个 Spring IoC 容器实例。\n解决循环依赖如果主要使用构造函数注入，则可能出现无法解析的循环依赖关系场景。例如：类 A 通过构造函数注入需要类 B 的实例，而类 B 通过构造函数注入类 A 的实例。如果将 A 类和 B 类的 bean 配置为相互注入，则 Spring IoC 容器会在运行时检测到此循环引用，并抛出 BeanCurrentlyInCreationException 异常。一种可行的解决方案是仅使用 setter 注入。与典型情况（没有循环依赖）不同，bean A 和 bean B 之间的循环依赖强制其中一个 bean 在完全初始化之前被注入另一个 bean（一个经典的鸡 / 蛋场景）。\n使用 depends-ondepends-on 可以在初始化 bean 之前，显式地强制初始化一个或多个 bean。下面的例子，在初始化 beanOne 之前，将强制初始化 manager\n&lt;bean id=\"beanOne\" class=\"ExampleBean\" depends-on=\"manager\"/&gt;&lt;bean id=\"manager\" class=\"ManagerBean\" /&gt;\n\n懒加载的 bean默认情况下，ApplicationContext 会立即配置并初始化所有单例 bean，但是我们可以使用 lazy-init=\"true\" 将其设置为按需加载。\n&lt;bean id=\"lazy\" class=\"com.foo.ExpensiveToCreateBean\" lazy-init=\"true\"/&gt;&lt;bean name=\"not.lazy\" class=\"com.foo.AnotherBean\"/&gt;\n\n注意：懒加载不要使用在数据库连接池上，因为无法立即获知数据库连接状态，将导致运行时创建连接池失败，不可预知的后果。\n自动装配协作者Spring 容器可以自动连接协作 bean 之间的关系。您可以通过检查 ApplicationContext 的内容，允许 Spring 自动为您的 bean 解析协作者（其他 bean）。\n自动装配模式\n\nno：无自动装配，必须使用 ref 来定义 Bean 引用。\nbyName：按属性名称自动装配。\nbyType：按属性类型自动装配，如果存在多个同类型 Bean，则抛出致命异常。\nconstructor：类似于 byType，如果容器中没有构造函数参数类型的一个 bean，则抛出致命异常。\n\nBean 作用域singleton\nSpring IoC 容器只创建该 bean 定义的对象的一个实例。此单个实例存储在此类单例 bean 的缓存中，并且该 Bean 的所有后续请求和引用都将返回缓存对象。\n\n\n&lt;bean id=\"accountService\" class=\"com.foo.DefaultAccountService\"/&gt;&lt;!-- the following is equivalent, though redundant (singleton scope is the default) --&gt;&lt;bean id=\"accountService\" class=\"com.foo.DefaultAccountService\" scope=\"singleton\"/&gt;\n\nprototype\n和单例对立，通常，对所有有状态 bean 使用原型范围，对无状态 bean 使用单例范围。\n\n\n&lt;bean id=\"accountService\" class=\"com.foo.DefaultAccountService\" scope=\"prototype\"/&gt;\n\nRequest, session, global session, application, and WebSocket\n在 web 程序中使用，对应于 HTTP 请求作用域\n\n自定义 bean 的性质生命周期回调初始化回调实现 org.springframework.beans.factory.InitializingBean 接口，可以为 bean 设置初始化方法，该接口定义了一个方法：\nvoid afterPropertiesSet() throws Exception;\n\n官方不建议使用该接口，因为会增加与 Spring 的耦合度。可以使用 @PostConstruct 或指定 bean 的初始化方法。\n\n使用 xml 配置文件 \n\n&lt;bean id=\"exampleInitBean\" class=\"examples.ExampleBean\" init-method=\"init\"/&gt;\n\n\n使用 Java @Bean 注解 \n\n@Bean(initMethod = \"init\")\n\n销毁回调实现 org.springframework.beans.factory.DisposableBean 可以为 bean 设置销毁回调方法，该接口定义了一个方法：\nvoid destroy() throws Exception;\n\n同样的，不建议实现该接口，可以使用 @PreDestroy 或指定 bean 的初始化方法。\n\n使用 xml 配置文件 \n\n&lt;bean id=\"exampleInitBean\" class=\"examples.ExampleBean\" destroy-method=\"cleanup\"/&gt;\n\n\n使用 Java @Bean 注解 \n\n@Bean(destroyMethod = \"cleanup\")\n\n从 Spring 2.5 开始，您有三个控制 bean 生命周期行为的选项：\n\nInitializingBean 和 DisposableBean 回调接口\n init () 和 destroy () 方法\n@PostConstruct 和 @PreDestroy 注解\n\n如果为一个 bean 同时配置了上述方法，则执行方法顺序为：\n\n@PostConstruct 定义的方法\nInitializingBean 回调接口定义的 afterPropertiesSet()\n自定义配置的 init() 方法\n\n销毁：\n\n@PreDestroy 定义的方法\nDisposableBean 回调接口 定义的 destroy()\n自定义配置的 destroy() 方法\n\nApplicationContextAware 和 BeanNameAware\nApplicationContextAware：实现该接口，将注入 ApplicationContext 实例的引用\nBeanNameAware：实现该接口，将注入 BeanName\n\n除了 ApplicationContextAware 和 BeanNameAware，Spring 还提供了一系列 Aware 接口，这些接口将为实现类注入对应的实例。\n\nApplicationContextAware：声明 ApplicationContext\nApplicationEventPublisherAware：ApplicationContext 的事件发布者\n BeanClassLoaderAware：用于加载 bean 类的类加载器。\nBeanFactoryAware：声明 BeanFactory\nBeanNameAware：声明 bean 的名称\n BootstrapContextAware\nLoadTimeWeaverAware\nMessageSourceAware\nNotificationPublisherAware：Spring JMX 通知发布者\n PortletConfigAware：当前 PortletConfig 容器\n PortletContextAware：当前 PortletContext 容器\n ResourceLoaderAware：配置的加载程序，用于对资源进行低级访问\n ServletConfigAware：当前 ServletConfig 容器\n ServletContextAware：当前 ServletContext 容器\n\nBean 的继承在 xml 配置文件里，我们可以定义 bean 的继承体系，使用 parent 属性定义父类。\n&lt;bean id=\"inheritedTestBean\" abstract=\"true\"        class=\"org.springframework.beans.TestBean\"&gt;    &lt;property name=\"name\" value=\"parent\"/&gt;    &lt;property name=\"age\" value=\"1\"/&gt;&lt;/bean&gt;&lt;bean id=\"inheritsWithDifferentClass\"        class=\"org.springframework.beans.DerivedTestBean\"        parent=\"inheritedTestBean\" init-method=\"initialize\"&gt;    &lt;property name=\"name\" value=\"override\"/&gt;    &lt;!-- the age property value of 1 will be inherited from parent --&gt;&lt;/bean&gt;\n\n在源码里，子类是通过 ChildBeanDefinition 来定义的。\n容器扩展点一般来说，我们不需要去继承 ApplicationContext 实现类，不过 Spring 预留了一些接口，让我们可以扩展 Spring IoC 容器。\nBeanPostProcessorpublic interface BeanPostProcessor {    //在每个bean初始化之前调用\tObject postProcessBeforeInitialization(Object bean, String beanName) throws BeansException;    //在每个bean初始化完毕后调用\tObject postProcessAfterInitialization(Object bean, String beanName) throws BeansException;}可以定义多个`BeanPostProcessor`，然后实现`Ordered`接口并修改属性order来控制`BeanPostProcessor`的执行顺序。注意：`ConfigurableBeanFactory`提供​```javavoid addBeanPostProcessor(BeanPostProcessor beanPostProcessor);\n\n来手动注册 BeanPostProcessor，这些 BeanPostProcessor 不需要遵循 Orderd 排序规则，总是在自动注入的 BeanPostProcessor 之前执行。\n一个 BeanPostProcessor 的实现例子 RequiredAnnotationBeanPostProcessor\n使用 BeanFactoryPostProcessor 自定义配置元数据public interface BeanFactoryPostProcessor {\tvoid postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException;}\n\n类似于 BeanPostProcessor，不同的是，BeanFactoryPostProcessor 操作配置元数据。也就是说，Spring 容器允许 BeanFactoryPostProcessor 读取配置并更改。\n这些 BeanPostProcessor 将在每个 bean 初始化时自动执行，以便将更改应用于定义容器的配置元数据。Spring 包含许多预定义的 BeanPostProcessor, 例如 PropertyOverrideConfigurer 和 PropertyPlaceholderConfigurer。\n使用 FactoryBean 自定义实例化逻辑public interface FactoryBean&lt;T&gt; {    // 自定义bean的初始化逻辑\tT getObject() throws Exception;\tClass&lt;?&gt; getObjectType();\tboolean isSingleton();}\n\n配置实现 FactoryBean&lt;T&gt; 的 bean 是，返回的是 getObject() 生成的 bean，如果要返回 FactoryBean 实例本身，应该使用 getBean(\"&amp;myBean\")\n基于注解的容器配置\n@Required\n@Autowired\n@Resource\n@Qualifier\n@PostConstruct and @PreDestroy\n\n类路径扫描和托管组件\n@Component,@Controller,@Repository,@Service\n@Scope,@SessionScope\n@ComponentScan\n\nJSR 330 标准注解和 Spring 注解对照\n\n\nSpring\njavax.inject.*\n\n\n\n@Autowired\n@Inject\n\n\n@Component\n@Named / @ManagedBean\n\n\n@Scope(“singleton”)\n@Singleton\n\n\n@Qualifier\n@Qualifier / @Named\n\n\n@Value\n-\n\n\n@Required\n-\n\n\n@Lazy\n-\n\n\nObjectFactory\nProvider\n\n\nEnvironment 抽象主要包含两个方面：profiles（多环境） and properties（配置）.\n多环境配置\n代码方式 \n\nAnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles(\"development\");ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh();\n\n\n配置方式 \n\nspring.profiles.active\n\n配置抽象代码演示下：\nApplicationContext ctx = new GenericApplicationContext();Environment env = ctx.getEnvironment();// 是否包含foo的配置boolean containsFoo = env.containsProperty(\"foo\");System.out.println(\"Does my environment contain the 'foo' property? \" + containsFoo);// 向环境中添加配置MutablePropertySources sources = ctx.getEnvironment().getPropertySources();sources.addFirst(new MyPropertySource());\n\n使用 @PropertySource 添加配置\n@Configuration@PropertySource(\"classpath:/com/myco/app.properties\")public class AppConfig {    @Autowired    Environment env;    @Bean    public TestBean testBean() {        TestBean testBean = new TestBean();        testBean.setName(env.getProperty(\"testbean.name\"));        return testBean;    }}\n\nBeanFactory 还是 ApplicationContext？尽量使用 ApplicationContext，因为 ApplicationContext 包含 BeanFactory 的所有功能：\n\n\n\n功能\n BeanFactory\nApplicationContext\n\n\n\nbean 初始化 / 编辑\n支持\n支持\n\n\n自动注册 BeanPostProcessor\n不支持\n支持\n\n\n自动注册 BeanFactoryPostProcessor\n不支持\n支持\n\n\n方便的 MessageSource 访问（适用于 i18n）\n不支持\n支持\n\n\n发布 ApplicationEvent\n不支持\n支持\n\n\n要使用 BeanFactory 实现显式注册 bean 后置处理器，您需要编写如下代码：\nDefaultListableBeanFactory factory = new DefaultListableBeanFactory();// populate the factory with bean definitions// now register any needed BeanPostProcessor instancesMyBeanPostProcessor postProcessor = new MyBeanPostProcessor();factory.addBeanPostProcessor(postProcessor);// now start using the factory\n\n要使用 BeanFactory 实现时显式注册 BeanFactoryPostProcessor，您必须编写如下代码：\nDefaultListableBeanFactory factory = new DefaultListableBeanFactory();XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(factory);reader.loadBeanDefinitions(new FileSystemResource(\"beans.xml\"));// bring in some property values from a Properties filePropertyPlaceholderConfigurer cfg = new PropertyPlaceholderConfigurer();cfg.setLocation(new FileSystemResource(\"jdbc.properties\"));// now actually do the replacementcfg.postProcessBeanFactory(factory);","categories":["个人笔记"],"tags":["Spring"]},{"title":"TamperMonkey 使用指南以及脚本推荐","url":"/p/29031/","content":"写在前面Chrome 浏览器是最适合开发者使用的浏览器，不仅仅是因为 Chrome 对于 Js 的友好支持，更是由于 Chrome 支持丰富且功能强大的插件，扩展了浏览器的功能和使用体验。\n在这些插件里面，相信你一定使用过 TamperMonkey，他可以让你加速下载百度网盘，跟百度限速说拜拜，也可以让你免费观看 VIP 影视和音乐，反正一句话，黑科技！\n\n\nTamperMonkey 使用TamperMonkey 的官网是：https://www.tampermonkey.net，支持各类 Chrome 内核的浏览器以及火狐浏览器（FireFox）。\n以下以 Chrome 浏览器为例。\n安装安装非常简单，打开 Chrome 商店，点击” 添加到 Chrome”\n\n安装脚本\n点击” 获取新脚本”\n\n\n\n 选择合适的脚本源，这里推荐 GreasyFork\n\n\n\n 安装脚本，以 VIP 视频解析为例\n\n\n我们点击第一个脚本（有时候会比较慢，请耐心等待下），点击” 安装此脚本”\n\n点击安装\n\n下面可以直接看到脚本的源码，有能力的话，可以自己修改或者编写脚本。\n\n看看脚本的效果\n\n我们打开爱奇艺，找个 vip 电影，比如最近热播的银河补习班\n\n\nTamperMonkey 脚本推荐在 TamperMonkey 的管理面板中，可以看到已经安装的所有脚本\n\n下面列举出我常用的脚本\n\n52pojie 吾爱破解论坛自动签到助手 - 免打扰\n\n吾爱破解论坛 - 百度网盘链接激活 - 提取码自动补全\n\nac-baidu - 重定向优化百度搜狗谷歌搜索 - 去广告 - favicon - 双列\n\nac-baidu - 优化百度 - 搜狗 - 谷歌搜索结果之关键词自动高亮\n\ncsdn 自动展开 - 去广告 - 净化剪贴板 - 免登陆\n\n一键 vip 视频解析 - 去广告 - 全网 - 一站式音乐搜索下载 - 百度云离线跳转 - 获取 b 站封面 - 淘宝京东优惠券 - 2019-10-01 - 更新 - 报错请及时反馈\n\n城通网盘 - 皮皮盘 - 牛盘显示正确下载地址\n\n百度文库文档免费下载 - 原文档 - 转换提取文档 - 文档内容自由复制 - 移除广告 - 豆丁网文档下载 - 解除大部分网站操作限制 - 全网 vip 视频免费在线看 - 支持电视剧免跳出选集\n\n百度网盘直链下载助手\n\n知乎网页助手 - 5 大功能集于一身\n\n贴吧全能助手\n\n\nTamperMonkey 脚本同步TamperMonkey 提供了非常方便的方法让我们同步脚本，从而避免每次安装都要重新安装脚本的烦恼。\n\n\n进入 TamperMonkey 插件设置页\n配置模式选择” 初学者”\n 勾选” 启用 TESLA”，类型选择” 浏览器同步”\n 点击保存\n\n","categories":["个人笔记"],"tags":["Chrome"]},{"title":"SpringJpa CRUD 代码生成器","url":"/p/30009/","content":"利用业余时间撸了一个 Spring Jpa 代码生成器 jpa-codegen。\n简介这是一款基于 Freemarker 模板驱动的代码生成器。\n依据现有的实体类代码，自动生成 CRUD 代码，解放双手，加快开发速度。\n生成的代码包括但不仅限于（可以自定义生成模块）\n\n\n\nForm 表单代码\n Repository 代码\n Service 代码\n Controller 代码\n\nSpringBoot 使用示例克隆示例项目，体会解放双手的美妙感受！\n如何使用导入仓库maven {    url 'https://dl.bintray.com/gcdd1993/maven'}dependencies {    // jpa code generator    testCompile 'io.github.gcdd1993:jpa-codegen:v1.0.1'    testCompile 'org.freemarker:freemarker:2.3.28'}\n\n配置代码生成器配置文件## 作者author=gcdd1993## 代码注释comments=code generated by jpa-codegen## 是否覆盖原文件，除非特殊情况，不然请不要覆盖cover=false## 代码模板目录template.dir=src/test/resources/template/## 实体类包名 Deprecated从v1.0.1开始从配置文件中移除- entity.package=com.maxtropy.sample.entity## 实体类标识符 Deprecated从v1.0.1开始从配置文件中移除- entity.flag=entity## 以下配置是模块配置(格式 模块名.配置名)，必须在模板目录下提供与模块名相同的模板## 生成的代码后缀repository.suffix=Repository## 模板名称repository.template=repository.ftl## 模块标识符repository.flag=entity.reposervice.suffix=Serviceservice.template=service.ftlservice.flag=serviceform.suffix=Formform.template=form.ftlform.flag=formcontroller.suffix=Controllercontroller.template=controller.ftlcontroller.flag=web\n\n其中\nrepository.suffix=Repositoryrepository.template=repository.ftlrepository.flag=entity.repo\n\n是模块配置，什么是模块？\n编写代码模板模板主要基于 Freemarker，如 Spring Boot2.x 代码模板可以像下面这样\npackage ${packageName};import ${entity.packageName}.${entity.className};import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.querydsl.QuerydslPredicateExecutor;&lt;#list imports as import&gt;import ${import};&lt;/#list&gt;/** * repository for ${entity.className} generated by jpa-codegen * ${comments} * * @author ${author} * Created On ${date}. */public interface ${className} extends JpaRepository&lt;${entity.className}, ${entity.id.className}&gt;, QuerydslPredicateExecutor&lt;${entity.className}&gt; {}\n\n\nSpring Boot 2.x 模板\n\n如何编写模板？\n\n\n编写生成器入口在 test 模块中编写生成器入口，如\npublic class Codegen {    @Test    public void generate() {        new CodeGenerator(\"src/test/resources/codegen.properties\")                .registerRender(\"repository\")                .generate();    }    }\n\n然后运行 generate()，在项目目录下将会生成\n\n\n生成的代码完全由模板以及实体类信息决定。\n如何编写模板？模板完全基于 FreeMarker 以及实体类信息，FreeMarker 参考 FreeMarker Docs\n支持的元素定义如下\n基本信息\n\n\nFreemarker 元素\n解释\n示例输出\n\n\n\n${ftlName}\n模板名称\ncontroller.ftl\n\n\n${ftlPath}\n模板目录\nsrc/main/resources/template/\n\n\n${savePath}\n保存路径\nsrc/main/resources/io/github/gcdd1993/controller\n\n\n${packageName}\njava 文件包名\nio.github.gcdd1993.controller\n\n\n${className}\njava 文件类名\nUserController\n\n\n${author}\n作者\ngaochen\n\n\n${date}\n创建日期，默认为当前日期\n2019/6/23\n\n\n${comments}\n注释信息\ngenerated by jpa-codegen\n\n\n${imports}\njava 文件引入信息\norg.springframework.beans.factory.annotation.Autowired\n\n\n实体信息\n\n\nFreemarker 元素\n解释\n示例输出\n\n\n\n${entity.className}\n实体类名，class.getSimpleName()\nUser\n\n\n${entity.packageName}\n实体包名，class.getPackage().getName()\nio.github.gcdd1993\n\n\n${entity.tableName}\n实体表名，@Table(name=\"\")\nsys_user\n\n\n${entity.id.className}\n实体主键类名，@Id 注释的字段的类名\n Integer\n\n\n${entity.id.packageName}\n实体主键包名，@Id 注释的字段的包名\n java.lang\n\n\n${entity.fields.className}\n实体所有字段（只支持基本类型）类名\n String\n\n\n${entity.fields.packageName}\n实体所有字段（只支持基本类型）包名\n java.lang\n\n\n${entity.fields.name}\n实体所有字段（只支持基本类型）属性名\n name\n\n\n${entity.fields.annotations.className}\n实体所有字段注解的类名\n Id\n\n\n${entity.fields.annotations.packageName}\n实体所有字段注解的包名\n javax.persistence\n\n\n自定义配置除了以上默认的信息之外，可能会有额外的信息需要填入生成的代码中，jpa-codegen 提供直接将配置文件中的配置渲染到模板的能力。\n例如在配置文件 autogen.properties 写下一行\ncustom.additional.comment=this is additional comment\n\n在模板中可以使用 ${otherParams.additional_comment} 获取到该配置。\n要注意的是：自定义配置使用 custom 开头，后面的 ** 配置会将。替换为_** 作为 FreeMarker 模板的 key，例如上述的 additional.comment 使用 ${otherParams.additional_comment} 获取。\n什么是模块？由于代码千变万化，为了尽可能的做到通用性，jpa-codegen 将每一种类型的代码抽象为模块，每一个模块将使用各自的模板，依照实体信息生成代码。\n需要为模板配置一下信息：\n\nrepository.suffix=Repository\n\n模块类名后缀，生成的类名规则由实体类名 + 后缀构成\n\nrepository.template=repository.ftl\n\n模块使用的 Freemarker 模板\n\nrepository.flag=entity.repo\n\n模块标识符，生成的代码包名由实体类将实体标识符替换为模块标识符来确认。\n如\n\n实体包名：io.github.gcdd1993.entity\n实体标识符：entity\n模块标识符：entity.repo\n\n则生成的 repository 代码包名为  –&gt; io.github.gcdd1993.entity.repo\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Ubuntu 安装 MongoDB","url":"/p/49357/","content":"Ubuntu16.04 安装 MongoDB 指南\n\n\n系统初始化$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean\n\n安装 mongodbsudo apt-get install mongodb\n\nmongodb 默认是监听在 127.0.0.1 端口的，要开启外网连接，需要修改 mongodb 配置文件：\nvim /etc/mongodb.conf\n\nbind_ip = 127.0.0.1 修改为 bind_ip = 0.0.0.0\n连接 mongodb使用工具 robo 3t，添加连接信息\n\n启用密码访问mongodb 默认是不开启密码登录的，如果要开启，修改 mongodb 配置文件：\n取消#auth = true 前面的注释，并重启 mongodbservice mongodb restart\n添加用户信息:\nuse test_db;db.createUser({user:'cool', pwd:'cool', roles: [ { role: \"readWrite\", db: \"test_db\" } ]});\n\n连接连接方式跟上面类似，唯一不同的是要添加 authentication，指定 database，username，password，以及选择 Mongodb-CR 验证方式\n","categories":["个人笔记"],"tags":["NoSql","MongoDB"]},{"title":"Spring 的 BeanFactory 和 FactoryBean","url":"/p/17046/","content":"官方定义\nBeanFactory：Spring Bean 容器的根接口\n FactoryBean：各个对象的工厂接口，如果 bean 实现了这个接口，它将被用作对象的工厂，而不是直接作为 bean 实例。\n\n\n\n源码解析BeanFactorypublic interface BeanFactory {    //标注是获取FactoryBean的实现类，而不是调用getObject()获取的实例\tString FACTORY_BEAN_PREFIX = \"&amp;\";\tObject getBean(String name) throws BeansException;\t&lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException;\tObject getBean(String name, Object... args) throws BeansException;\t&lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException;\t&lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException;\tboolean containsBean(String name);\tboolean isSingleton(String name) throws NoSuchBeanDefinitionException;\tboolean isPrototype(String name) throws NoSuchBeanDefinitionException;\tboolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException;\tboolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException;\tClass&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException;\tString[] getAliases(String name);}\n\n从源码的方法定义上，就可以看出，BeanFactory 作为 bean 的容器管理器，提供了一系列获取 bean 以及获取 bean 属性的方法。\n写一个小例子试验下：\nSimpleBean：\npublic class SimpleBean {    public void send() {        System.out.println(\"Hello Spring Bean!\");    }}\n\nSpring 配置文件 config.xml：\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:context=\"http://www.springframework.org/schema/context\"       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\t\t\t\t\t\thttp://www.springframework.org/schema/beans/spring-beans.xsd\t\t\t\t\t\thttp://www.springframework.org/schema/context\t\t\t\t\t\thttp://www.springframework.org/schema/context/spring-context.xsd\"&gt;    &lt;bean id=\"simpleBean\" class=\"base.SimpleBeanFactoryBean\"/&gt;&lt;/beans&gt;\n\n测试方法：\npublic static void main(String[] args) throws Exception {    ApplicationContext context = new ClassPathXmlApplicationContext(\"config.xml\");    BeanFactory beanFactory = context.getAutowireCapableBeanFactory();    System.out.println(\"通过名称获取bean\");    SimpleBean simpleBean = (SimpleBean) beanFactory.getBean(\"simpleBean\");    simpleBean.send();    System.out.println(\"通过名称和类型获取bean\");    simpleBean = beanFactory.getBean(\"simpleBean\", SimpleBean.class);    simpleBean.send();    System.out.println(\"通过类型获取bean\");    simpleBean = beanFactory.getBean(SimpleBean.class);    simpleBean.send();    boolean containsBean = beanFactory.containsBean(\"simpleBean\");    System.out.println(\"是否包含 simpleBean ? \" + containsBean);    boolean singleton = beanFactory.isSingleton(\"simpleBean\");    System.out.println(\"是否是单例? \" + singleton);    boolean match = beanFactory.isTypeMatch(\"simpleBean\", ResolvableType.forClass(SimpleBean.class));    System.out.println(\"是否是SimpleBean类型 ? \" + match);    match = beanFactory.isTypeMatch(\"simpleBean\", SimpleBean.class);    System.out.println(\"是否是SimpleBean类型 ? \" + match);    Class&lt;?&gt; aClass = beanFactory.getType(\"simpleBean\");    System.out.println(\"simpleBean 的类型是 \" + aClass.getName());    String[] aliases = beanFactory.getAliases(\"simpleBean\");    System.out.println(\"simpleBean 的别名 : \" + Arrays.toString(aliases));}\n\n控制台结果：\n通过名称获取beanHello Spring Bean!通过名称和类型获取beanHello Spring Bean!通过类型获取beanHello Spring Bean!是否包含 simpleBean ? true是否是单例? true是否是SimpleBean类型 ? true是否是SimpleBean类型 ? truesimpleBean 的类型是 base.SimpleBeansimpleBean 的别名 : []\n\nFactoryBeanpublic interface FactoryBean&lt;T&gt; {        /**     * 获取一个bean，如果配置了工厂bean，在getBean的时候，将会调用此方法，获取一个bean     */    T getObject() throws Exception;    /**     * 获取bean的类型     */    Class&lt;?&gt; getObjectType();    /**     * 是否是单例     */    boolean isSingleton();}\n\n接口是泛型，定义了三个方法，其中 getObject() 是工厂模式的体现，将会通过此方法返回一个 bean 的实例。\n一个小例子：\npublic class SimpleBeanFactoryBean implements FactoryBean&lt;SimpleBean&gt; {    @Override    public SimpleBean getObject() throws Exception {        System.out.println(\"MyFactoryBean getObject\");        return new SimpleBean();    }    @Override    public Class&lt;?&gt; getObjectType() {        System.out.println(\"MyFactoryBean getObjectType\");        return SimpleBean.class;    }    @Override    public boolean isSingleton() {        return false;    }}\n\n以上可以修改为单例模式，可以做成线程安全的单例，可塑性较高。\n配置文件 config.xml:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:context=\"http://www.springframework.org/schema/context\"       xsi:schemaLocation=\"http://www.springframework.org/schema/beans\t\t\t\t\t\thttp://www.springframework.org/schema/beans/spring-beans.xsd\t\t\t\t\t\thttp://www.springframework.org/schema/context\t\t\t\t\t\thttp://www.springframework.org/schema/context/spring-context.xsd\"&gt;    &lt;bean id=\"simple\" class=\"base.SimpleBeanFactoryBean\"/&gt;&lt;/beans&gt;\n\n注意，我们在这里只配置了 SimpleBeanFactoryBean，并没有配置 SimpleBean，接下来看下 getBean 方法的输出。\nApplicationContext context = new ClassPathXmlApplicationContext(\"config.xml\");SimpleBean simpleBean = context.getBean(SimpleBean.class);simpleBean.send();\n\n控制台输出：\nMyFactoryBean getObjectTypeMyFactoryBean getObjectHello Spring Bean!\n\n由此我们可以看出 FactoryBean 的执行流程\n\n通过 getObjectType 获取 bean 的类型\n调用 getObject 方法获取 bean 的实例\n\n总结BeanFactory 和 FactoryBean 其实没有关系，只是名称比较像而已。\n\nBeanFactory 是 IOC 最基本的容器，负责生产和管理 bean，它为其他具体的 IOC 容器提供了最基本的规范。\nFactoryBean 是一个接口，当在 IOC 容器中的 Bean 实现了 FactoryBean 后，通过 getBean(String BeanName) 获取到的 Bean 对象并不是 FactoryBean 的实现类对象，而是这个实现类中的 getObject() 方法返回的对象。要想获取 FactoryBean 的实现类，就要 getBean(&amp;BeanName)，在 BeanName 之前加上 &amp;。\n\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"Ubuntu 搭建 phpcms","url":"/p/5357/","content":"安装 Apache2$ sudo apt-get update -y$ sudo apt-get install apache2 -y$ sudo systemctl start apache2.service\n\n\n\n安装 Mysql$ sudo apt-get install mysql-server -y$ sudo /usr/bin/mysql_secure_installation## 都选y就行$ mysql -u root -p mysql&gt; CREATE DATABASE js_website;## 导入数据mysql&gt; source /tmp/jskj.sql;mysql&gt; \\q;\n\n安装 PHP$ sudo apt-get install php -y;$ sudo apt-get install -y php-{bcmath,bz2,intl,gd,mbstring,mcrypt,mysql,zip} &amp;&amp; sudo apt-get install libapache2-mod-php -y;\n\n部署 PHP 官网$ mkdir /var/www/html/phpcms$ cd /var/www/html/phpcms# 上传phpcms.zip包至此目录$ unzip phpcms.zip$ ls -ldrwxr-xr-x 11 root root      4096 Jun 24 17:21 ./drwxr-xr-x  3 root root      4096 Jun 24 17:21 ../-rw-r--r--  1 root root        48 Jun 24 15:53 admin.phpdrwxr-xr-x  3 root root      4096 Jun 24 15:53 api/-rw-r--r--  1 root root       991 Jun 24 15:53 api.phpdrwxr-xr-x 18 root root      4096 Jun 24 15:53 caches/-rw-r--r--  1 root root       104 Jun 24 15:53 crossdomain.xmldrwxr-xr-x  6 root root      4096 Jun 24 15:53 custom/-rw-r--r--  1 root root      3158 Jun 24 15:53 favicon.icodrwxr-xr-x  2 root root      4096 Jun 24 15:53 html/-rw-r--r--  1 root root      4444 Jun 24 15:53 index.htm-rw-r--r--  1 root root     22758 Jun 24 15:53 index.html-rw-r--r--  1 root root       318 Jun 24 15:53 index.php-rw-r--r--  1 root root       523 Jun 24 15:53 js.htmldrwxr-xr-x  8 root root      4096 Jun 24 15:53 mes/drwxr-xr-x  8 root root      4096 Jun 24 15:53 phpcms/-rw-r--r--  1 root root 168191200 Jun 24 16:38 phpcms.zipdrwxr-xr-x  7 root root      4096 Jun 24 15:53 phpsso_server/-rw-r--r--  1 root root      3621 Jun 24 15:53 plugin.php-rw-r--r--  1 root root       170 Jun 24 15:53 robots.txtdrwxr-xr-x  6 root root      4096 Jun 24 15:53 statics/drwxr-xr-x  4 root root      4096 Jun 24 15:53 uploadfile/\n\n👉这里的 zip 压缩包，是已经 install 后的 phpcms，因为项目经理给我的就是安装好的，所以就直接用了。\n反正原理都一样，配置 Apache 解析域名指向路径就行。\n配置 Apache$ cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/phpcms.conf$ cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/phpcms-mes.conf$ ln -s /etc/apache2/sites-available/phpcms.conf /etc/apache2/sites-enabled/phpcms.conf$ ln -s /etc/apache2/sites-available/phpcms-mes.conf /etc/apache2/sites-enabled/phpcms-mes.conf$ vim /etc/apache2/sites-available/phpcms.confServerName js.dbpe-cps.com# ServerAdmin webmaster@localhostDocumentRoot /var/www/html/phpcms$ vim /etc/apache2/sites-available/phpcms-mes.confServerName mes.js.dbpe-cps.com# ServerAdmin webmaster@localhostDocumentRoot /var/www/html/phpcms/mes$ service apache2 restart\n\n域名解析配置你的域名指向你的服务器就行。这里略过。\n其他Apache 常用命令## 重启Apache2$ service apache2 restart $ service apache2 status$ service apache2 start\n\nApache 目录\n配置目录：/etc/apache2\n默认 www 目录：/var/www/html\n\n这一点跟其他的不一样，我也是看到配置文件才知道是这个目录的\n\n/etc/apache2/apache2.conf\n","categories":["个人笔记"],"tags":["phpcms"]},{"title":"Ubuntu 一键部署 Docker","url":"/p/46199/","content":"安装 DockerDocker 官网方式\n有时候国内镜像同步不及时，可能会安装失败，此时只能通过官网来进行安装\n\nsudo apt-get update &amp;&amp; sudo apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common &amp;&amp; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &amp;&amp; echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get -y install docker-ce docker-ce-cli containerd.io &amp;&amp; sudo docker --version\n\n\n国内方式\n国内的网络环境众所周知，所以推荐使用镜像站进行安装\n\nsudo apt-get update &amp;&amp; sudo apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common &amp;&amp; curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - &amp;&amp; sudo apt-key fingerprint 0EBFCD88 &amp;&amp; sudo add-apt-repository \"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get -y install docker-ce docker-ce-cli containerd.io &amp;&amp; sudo docker --version\n\n安装校验root@iZbp12adskpuoxodbkqzjfZ:$ docker versionClient:Version: 17.03.0-ceAPI version: 1.26Go version: go1.7.5Git commit: 3a232c8Built: Tue Feb 28 07:52:04 2017OS/Arch: linux/amd64Server:Version: 17.03.0-ceAPI version: 1.26 (minimum version 1.12)Go version: go1.7.5Git commit: 3a232c8Built: Tue Feb 28 07:52:04 2017OS/Arch: linux/amd64Experimental: false\n\nDocker-compose 安装## 自行修改version为最新版本的docker-composesudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose &amp;&amp; sudo chmod +x /usr/local/bin/docker-compose &amp;&amp; docker-compose --version\n\n参考资料\nInstall Docker Engine on Ubuntu\nDocker CE 镜像源站\n\n","categories":["个人笔记"],"tags":["Docker","Ubuntu"]},{"title":"Ubuntu 禁用 root 账号，开启 Ubuntu 密钥登录","url":"/p/46186/","content":"新建普通用户## 新建普通用户$ adduser ubuntu$ apt-get install sudo## 将用户加入sudo组$ usermod -a -G sudo ubuntu\n\n\n\n为普通用户添加公钥$ su ubuntu$ mkdir -p ~/.ssh$ cd ~/.ssh## 添加公钥$ touch authorized_keys$ cat '你的公钥字符串' &gt;&gt; authorized_keys$ chmod 600 authorized_keys$ chmod 700 ~/.ssh\n\n设置 SSH，打开密钥登录$ vim /etc/ssh/sshd_configRSAAuthentication yesPubkeyAuthentication yes## 禁用root账号登录PermitRootLogin no## 禁用密码登录PasswordAuthentication no$ service sshd restart\n\n","categories":["个人笔记"],"tags":["Ubuntu"]},{"title":"Ubuntu 切换为阿里镜像源","url":"/p/12805/","content":"前言在 VM 虚拟机搭建 Ubuntu 系统学习或者测试时，常常要使用 apt 安装测试，但是由于系统自带的下载源在国外服务器上，下载速度慢的无法忍受。所以我们需要切换为国内镜像源，能显著加快安装包下载速度。\n\n\n步骤$ cd /etc/apt/$ cp sources.list sources.list.bak ## 备份系统自带的source列表## 选择合适的镜像源，如阿里云的镜像 http://mirrors.aliyun.com/ubuntu$ sed -i 's/^\\(deb\\|deb-src\\) \\([^ ]*\\) \\(.*\\)/\\1 http:\\/\\/mirrors.aliyun.com\\/ubuntu \\3/' sources.list## 更新apt$ apt-get update\n\n国内镜像源\n\n\n名称\n地址\n\n\n\n阿里镜像源 \nhttp://mirrors.aliyun.com/ubuntu\n\n\n 清华大学镜像源 \nhttps://mirrors.tuna.tsinghua.edu.cn/ubuntu/\n\n\n 网易镜像源 \nhttps://mirrors.163.com/ubuntu/\n\n\n 东北大学镜像源 \nhttp://mirror.neu.edu.cn/ubuntu/\n\n\n至于哪个源比较快，看个人的网络吧，可以自行测试下。本人使用的是阿里镜像源。\n","categories":["个人笔记"],"tags":["Ubuntu"]},{"title":"Untitled","url":"/p/6/","content":"问题背景在日常版本控制操作中，时常会遇到因混淆不同场景下的身份信息而导致的邮件地址误用问题，例如，在提交企业内部项目时意外使用了个人邮箱地址，或是在向 GitHub 等公共平台提交代码时采用了公司专属邮箱。为解决此类问题，期望实现一种自动化机制，使得 Git 在执行提交操作时能根据目标远程仓库的域名智能切换相应的邮箱配置，确保与项目及环境相匹配的身份标识得以正确运用。\n实现方案认识 Git HooksGit Hook 是 Git 仓库中的一组脚本，它们允许你在特定的 Git 事件发生时执行自定义操作。这些脚本放置在 .git/hooks 目录下，并且每个脚本对应一个特定的 Git 生命周期事件。\n以下是一些常见的 Git Hook 类型及其触发时机：\n\npre-commit: 在提交信息被记录之前运行。这个钩子可以用来进行代码格式检查、linting、单元测试等，如果脚本执行失败（返回非零退出码），则会阻止这次提交。\n\nprepare-commit-msg: 在编辑提交消息文件之前运行，可以用于修改或预填充提交消息。\n\ncommit-msg: 当提交消息被创建后运行，可用于验证提交消息是否符合项目规范。\n\npost-commit: 提交完成后立即运行，通常用于更新其他系统或者触发后续动作。\n\npre-receive: 在服务器端接收到推送请求之后但在实际接受提交之前运行，可以用于实现对推送内容的全局性预检。\n\nupdate: 同样是服务器端的 hook，在 pre-receive 之后调用，对于每一个更新到仓库中的分支都会执行一次，常用于实现更细粒度的访问控制和策略。\n\npost-receive: 推送操作完成后在服务器上运行，可用来触发构建、部署或其他后处理任务。\n\npre-auto-gc: 在自动垃圾回收开始前运行，可以用来定制垃圾回收的行为。\n\npost-checkout：在完成 git checkout 或 git switch 命令后触发。\n\n\n每个 hook 脚本都是可执行文件（需要设置正确的执行权限），并且在执行时会传入相应的参数，以便获取关于所执行操作的详细信息。通过 Git Hook，开发者能够根据团队需求定制工作流程，确保遵循特定的开发规范和实践。\n\n基于 Hook 机制，我们可以使用 post-checkout 在代码拉取时自动按照域名设置不同的提交用户名\n\n配置全局设置必须配置用户名邮箱git config --global user.useConfigOnly true\n并且删除全局的 user.name 和 user.email 配置，所有的全局配置都在～/.gitconfig 文件中，删除 [user] 下的用户名和邮箱配置。然后使用 git config 查看一下是否删除成功。\ngit config --list...user.useconfigonly=true\n设置 git hooks templates 目录mkdir -p ~/.git-templates/hooksgit config --global init.templatedir ~/.git-templates\n配置 post-checkout 脚本在 ~/.git-templates/hooks 目录里新建 post-checkout 文件，内容如下：\n#!/bin/bashif [[ $1 == 00000000000* ]]; then      remote=`git remote -v | awk '/\\(push\\)$/ {print $2}'`      email=xxx@gmail.com # 默认邮箱      name=\"xxx\" # 默认提交用户名      echo $remote      if [[ $remote =~ '公司git域名' ]]; then        email=xxxx@xxx.com # 该域名使用的邮箱        name=\"xxxx\" # 该域名使用的用户名      fi            echo \"Configuring user &lt;name: $name email: $email&gt;\"      git config user.email \"$email\"      git config user.name  \"$name\"fi\n配置完成后，测试一下\n$ git clone ssh://git@xxxx.com/xxxx.gitCloning into 'xxxx'...remote: Enumerating objects: 1284, done.remote: Counting objects: 100% (1284/1284), done.remote: Compressing objects: 100% (828/828), done.remote: Total 265979 (delta 275), reused 423 (delta 18)Receiving objects: 100% (265979/265979), 61.32 MiB | 2.00 MiB/s, done.Resolving deltas: 100% (98214/98214), done.ssh://git@xxxx.com/xxxx.gitConfiguring user &lt;name: xxxx email: xxxx@xxxx.com&gt;\n\n至此就配置完毕了，以后在拉取代码或切换分支的时候，会自动根据域名判断使用的用户名，不会再出错了。\n"},{"title":"MacOs 使用 CleanMyMac X 清除可清除空间","url":"/p/46997/","content":"写在前面本文介绍如何使用 CleanMyMac X 清除可清除的空间\n\n\n\n可以看到，可清除的空间达到了 125.79GB，虽然说不影响系统的使用，但是在使用时间机器进行备份的时候，仍然会将可清除空间当成备份的一部分，造成备份文件过大，首次备份时间过长。\n准备清除可清除空间，你只需要 CleanMyMac X 这个工具即可，我分享下我使用的版本，当然有能力的建议使用正版。\nhttps://pan.baidu.com/s/1L05kBwZIghM73IRC8rMpMw\n开始安装完毕后，打开 CleanMyMac X\n\n点击” 维护 “，你可以使用 “释放可清除空间” 或者是 “时间机器快照瘦身”，我使用的是 “时间机器快照瘦身”\n建议先使用 “时间机器快照瘦身”，如果不行，再释放可清除空间，因为释放可清除空间耗时较长\n\n点击运行，稍作等待即可\n\n这时候，我们回到磁盘工具，再次查看可清除空间，可以发现，可清除空间小了不少！\n\n","categories":["个人笔记"],"tags":["macOs"]},{"title":"Spring-Cloud 微服务踩坑记录","url":"/p/61811/","content":"记录在开发微服务过程中遇到的问题以及解决方案。\nNo Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-netflix-ribbon?@feignclient 和 @requestmapping 混用的时候出错重写 springmvc 扫描 controller 时不带有 @feignclient 才实例化\n@Configuration@ConditionalOnClass({Feign.class})public class FeignConfiguration {    @Bean    public WebMvcRegistrations feignWebRegistrations() {        return new WebMvcRegistrationsAdapter() {            @Override            public RequestMappingHandlerMapping getRequestMappingHandlerMapping() {                return new FeignRequestMappingHandlerMapping();            }        };    }    private static class FeignRequestMappingHandlerMapping extends RequestMappingHandlerMapping {        @Override        protected boolean isHandler(Class&lt;?&gt; beanType) {            return super.isHandler(beanType) &amp;&amp;                    !AnnotatedElementUtils.hasAnnotation(beanType, FeignClient.class);        }    }}\n\nSpringCloud 使用 Zuul 出现 “Forwarding error” 错误解决方法在 application.yml 中添加 ribbon 的超时时间设置：\nribbon:    ReadTimeout: 3000  ConnectTimeout: 3000zuul:    host:        connect-timeout-millis: 3000        socket-timeout-millis: 3000hystrix:    command:        default:            execution:                isolation:                    thread:                        timeout-in-milliseconds: 3000\n\n","categories":["个人笔记"],"tags":["Spring Cloud"]},{"title":"Postman 使用技巧","url":"/p/4537/","content":"Postman 是什么Postman 是 chrome 的一款插件，用于做接口请求测试，无论是前端，后台还是测试人员，都可以用 postman 来测试接口，用起来非常方便。\nPostman 安装官网下载 (翻墙)https://www.getpostman.com/downloads/\n蓝奏云https://www.lanzous.com/i2en5xc\nPostman 常用功能安装好之后，我们先打开 Postman，可以看到界面分成左右两个部分，右边是我们后头要讲的 collection，左边是现在要讲的 request builder。在 request builder 中，我们可以通过 Postman 快速的随意组装出我们希望的 request。一般来说，所有的 HTTP Request 都分成 4 个部分，URL, method, headers 和 body。而 Postman 针对这几部分都有针对性的工具。\n\nURL要组装一条 Request, URL 永远是你首先要填的内容，在 Postman 里面你曾输入过的 URL 是可以通过下拉自动补全的哦。如果你点击 Params 按钮，Postman 会弹出一个键值编辑器，你可以在哪里输入 URL 的 Parameter，Postman 会帮你自动加入到 URL 当中，反之，如果你的 URL 当中已经有了参数，那 Postman 会在你打开键值编辑器的时候把参数自动载入\n\nHeaders点击’Headers’按钮，Postman 同样会弹出一个键值编辑器。在这里，你可以随意添加你想要的 Header attribute，同样 Postman 为我们通过了很贴心的 auto-complete 功能，敲入一个字母，你可以从下拉菜单里选择你想要的标准 atrribute\n\nMethod要选择 Request 的 Method 是很简单的，Postman 支持所有的 Method，而一旦你选择了 Method，Postman 的 request body 编辑器会根据的你选择，自动的发生改变。\n\nRequest Body如果我们要创建的 request 是类似于 POST，那我们就需要编辑 Request Body，Postman 根据 body type 的不同，提供了 4 中编辑方式：\n\nform-data\nx-www-form-urlencoded\nraw\nbinary\n\n\n（我们这里是可以传文件的哦）\npostman 高级用法colletions (接口集合)在开发过程中，可能会遇到多项目同时开发维护的情况，Postman 友好的提供了 colletions 功能，类似与项目文件夹一样，可以把归属于同一类的接口分类到一起，便于管理维护。\n\n点击 NEW -&gt; 选择 collection，创建一个项目空间。\n\n\n\n输入项目名称，点击 create。\n\n\ncolletions folder (集合中的文件夹)每个项目会有多个接口，有些是一类功能，例如，用户管理接口，文章列表接口，Postman 提供 folder 目录来进行细致的分类。\n\n选择一个项目，点击 Add Folder\n\n\n\n 输入目录名称，点击 create\n\n\n每个接口都可以归类到某个项目，或某个项目的子目录中。\n\nEnvironment (环境变量)Postman 允许定义自己的环境变量（Environment），最常见的是将测试 URL 进行定义成变量的形式，这样随着你的域名怎么变，URL 就不用变更，非常方便。除此之外，也可以将一些敏感的测试值定义为环境变量，比如密码。接下来，来看下怎么新建一组环境变量，如下操作打开环境变量的管理入口：\n\n打开管理环境变量的窗口，输入名称，添加一组键值对，如下图所示：\n\n环境变量要以双大括号的方式来引用，可以在右上方下拉框处选择相应的环境变量，我们实测一下刚才添加的 Url 的变量：\n\n通过脚本设置变量Postman 允许用户自定义脚本，并提供了两种类型的脚本：\n\nPre-request Script：执行 request 请求前先运行，可以在里面预先设置些所需变量\n Tests：request 返回后执行的，可以对返回信息进行提取过滤，或者执行一些验证操作\n\n例子获取如下返回信息中的 user_id 值:\n// 假设服务端返回的Body内容如下:{  \"token\": {    \"user_id\": \"2079876912\",    \"access_token\": \"26A90E317DBC1AD363B2E2CE53F76F2DD85CB172DF7D813099477BAACB69DC49C794BAECEDC68331\",    \"expires_at\": \"2016-06-22T12:46:51.637+0800\",    \"refresh_token\": \"26A90E317DBC1AD3CD1556CF2B3923DD60AEBADDCBC1D9D899262A55D15273F735E407A6BEC56B84\",    \"mac_key\": \"4FAhd4OpfC\",    \"mac_algorithm\": \"hmac-sha-256\",    \"server_time\": \"2016-06-15T12:46:51.649+0800\"  }}\n\n在 Tests 中对 user_id 值进行提取并赋值成全局变量:\n// 判断是否存在'user_id'值tests[\"Body contains user_id\"] = responseBody.has(\"user_id\");if(tests[\"Body contains user_id\"]){    // 将返回信息解析成对象    var responseData = JSON.parse(responseBody);    tests[\"value_user_id\"]=responseData.token.user_id    // 设置全局变量    postman.setGlobalVariable(\"user_id\",tests[\"value_user_id\"]);}else{    postman.setGlobalVariable(\"user_id\",\"默认user_id\");}\n\n实践案例项目接口分类管理\n登录获取 token 并设置为全局变量\n接口使用登录后的 token\n","categories":["个人笔记"],"tags":["工具技巧"]},{"title":"一键 HTTPS 并开启自动更新","url":"/p/29831/","content":"本文使用 Let's Encrypt 的免费 HTTPS 证书，并搭配自动续约脚本，达到 https 一劳永逸的效果。基本上部署一次，之后就再也不需要维护了，异常方便。\n\n\n方案简介Let’s Encrypt 和 CertBot\n我们申请和使用 Let's Encrypt 的免费 HTTPS 证书，就需要一个证书申请和管理的工具，然后 certbot 是官方推荐的申请工具，我们使用这个工具申请和管理我们的证书\n\n\ncertbot 支持大部分的 Linux 发行版\n\n上面也提到，现在阿里云不售卖免费证书了，但是如果我们（实际上是公司）想白嫖怎么办呢？就得用到 Let's Encrypt 了。\n下面我就分享下如何一键部署 CertBot 并开启自动续期。\n方案实施准备\nDocker 以及 Docker Compose 环境\n域名 DNS 已经指向待部署的服务器，因为脚本校验证书所属权，需要访问域名\n\n废话少说，这就开始！\n克隆仓库\n💡 提示：这一步必不可少，一定要按照仓库目录结构来执行，完成后，可以自行更改 nginx/conf.d 下的配置文件。\n\n$ mkdir -p /data$ cd /data$ git clone https://ghproxy.com/https://github.com/gcdd1993/nginx-certbot$ cd nginx-certbot$ ls -ldrwxr-xr-x 4 root root 4096 Jun  8 22:01 ./drwxr-xr-x 5 root root 4096 Jun  8 21:49 ../drwxr-xr-x 4 root root 4096 Jun  8 21:53 data/-rw-r--r-- 1 root root  660 Jun  8 21:49 docker-compose.ymldrwxr-xr-x 8 root root 4096 Jun  8 21:49 .git/-rw-r--r-- 1 root root   14 Jun  8 21:49 .gitignore-rwxr-xr-x 1 root root 2286 Jun  8 22:01 init-letsencrypt.sh*-rw-r--r-- 1 root root 1074 Jun  8 21:49 LICENSE-rw-r--r-- 1 root root 1376 Jun  8 21:49 README.md\n\n修改邮箱$ vim init-letsencrypt.sh...email=\"gcwm99@gmail.com\"...\n\n修改操作域名\n修改 your_domain 为你的域名（只能是单域名，不能是泛域名）\n\n$ sed -i 's/example.org/your_domain/g' data/nginx/app.conf \\\t&amp;&amp; sed -i 's/example.org/your_domain/g' init-letsencrypt.sh\n\n执行脚本\n出现以下内容，说明已经成功！\n\n$ ./init-letsencrypt.sh...Requesting a certificate for your_domainSuccessfully received certificate.Certificate is saved at: /etc/letsencrypt/live/your_domain/fullchain.pemKey is saved at:         /etc/letsencrypt/live/your_domain/privkey.pemThis certificate expires on 2021-09-06.These files will be updated when the certificate renews.NEXT STEPS:- The certificate will need to be renewed before it expires. Certbot can automatically renew the certificate in the background, but you may need to take steps to enable that functionality. See https://certbot.org/renewal-setup for instructions.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -If you like Certbot, please consider supporting our work by:* Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate* Donating to EFF:                    https://eff.org/donate-le- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n修改配置\n将配置更改为你自己网站的配置，下面给一个示例配置\n\n$ mv app.conf app.conf.bak # 注释默认配置$ cd /data/nginx-cert/data/nginx$ vim nginx-example.conf # 编写自己的配置upstream my.site {    server localhost:8080;}server {    server_name your_domain;    proxy_read_timeout 600s;    proxy_send_timeout 600s;    location / {        add_header X-Frame-Options deny;        proxy_pass http://my.site;    }\t# 以下内容保持不变即可，只需要修改your_domain为你的域名    listen 443 ssl; # managed by Certbot    ssl_certificate /etc/letsencrypt/live/your_domain/fullchain.pem;    ssl_certificate_key /etc/letsencrypt/live/your_domain/privkey.pem;    include /etc/letsencrypt/options-ssl-nginx.conf;    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;    server_tokens off;}# 以下内容保持不变即可，只需要修改your_domain为你的域名server {    if ($host = your_domain) {        return 301 https://$host$request_uri;    } # managed by Certbot    location /.well-known/acme-challenge/ {        root /var/www/certbot;    }    server_name your_domain;    listen 80;    return 404; # managed by Certbot}\n\n其中的 location /.well-known/acme-challenge/ 很重要，千万不要漏了，否则在进行证书续期的时候，是无法通过网站所属验证的。\n重启 Nginx 服务$ cd /data/nginx-certbot$ docker-compose restart\n\n附录多域名操作\n步骤同上，先修改域名为待操作域名，然后执行 init-letsencrypt.sh\n\n$ sed -i 's/your_domain/your_domain2/g' data/nginx/app.conf \\\t&amp;&amp; sed -i 's/your_domain/your_domain2/g' init-letsencrypt.sh$ ./init-letsencrypt.sh...\n\n更新证书$ docker exec -it nginx-certbot_certbot_1 certbot renew# ...The following certificates are not due for renewal yet:  /etc/letsencrypt/live/my.site/fullchain.pem expires on 2021-09-06 (skipped)No renewals were attempted. # 还未到更新时间，证明证书还是有效的- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n修改更新间隔\n修改 docker-compose.yml 里面的间隔即可。\n\n$ cd /data/nginx-certbot$ vim docker-compose.ymlentrypoint: \"/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h &amp; wait $${!}; done;'\" # 修改12h为你喜欢的值# 修改完毕，重启$ docker-compose restart\n\n好了，还不快试试？\n","categories":["个人笔记"],"tags":["HTTPS"]},{"title":"使用 Hyper-V 替代 VMware","url":"/p/14834/","content":"Hyper-V 是什么\nHyper-V 硬件要求为 Windows 10 企业版、专业版或教育版，如果你使用的是 Mac 或者 Linux 的电脑，可以不往下看了。\n\n虚拟机大家都懂吧，简单来说，Hyper-V 就是虚拟机管理工具。如果你使用过 VMware Workstation Pro 或者是 VirtualBox，那你一定不陌生了。\n\n\n具体来说，Hyper-V 提供硬件虚拟化。 这意味着每个虚拟机都在虚拟硬件上运行。 Hyper-V 允许你创建虚拟硬盘驱动器、虚拟交换机以及许多其他虚拟设备，所有这些都可以添加到虚拟机中。\n为什么要使用 Hyper-V 而不是 VMware？首先为什么要使用虚拟机？\n\n运行需要早期版本的 Windows 操作系统或非 Windows 操作系统的软件。\n实验其他操作系统。 通过虚拟机，可轻松创建和删除不同的操作系统。\n使用多个虚拟机在多个操作系统上测试软件。 通过虚拟机，可以在一部台式机或便携式计算机上运行所有内容。\n\n那么，为什么要使用 Hyper-V？\n\n首先，Hyper-V 是 Windows 10 专业版自带的功能，无需安装其他任何工具 \nDocker for Windows 推荐使用 Hyper-V 作为虚拟化方案\n免费\n\n所以，在 Hyper-V 能胜任的场景下，我们应该使用 Hyper-V。\n如何使用 Hyper-V检查系统要求\nWindows 10 企业版、专业版或教育版。\n具有二级地址转换 (SLAT) 的 64 位处理器。\n虚拟机监视器模式扩展的 CPU 支持 (Intel Cpu 上的 VT-c)。\n最小 4 GB 内存。\n\n注意：系统必须是 Windows 10 企业版、专业版或教育版。\n开启 Hyper-V使用 PowerShell 启用 Hyper-V\n以管理员身份打开 PowerShell 控制台。\n运行以下命令：\n\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All\n\n如果无法找到此命令，请确保你以管理员身份运行 PowerShell。\n安装完成后，请重启。\n使用 CMD 和 DISM 启用 Hyper-V部署映像服务和管理工具 (DISM) 可帮助配置 Windows 和 Windows 映像。 在众多应用程序中，DISM 可以在操作系统运行时启用 Windows 功能。\n使用 DISM 启用 Hyper-V 角色：\n\n以管理员身份打开 PowerShell 或 CMD 会话。\n键入下列命令：\n\nDISM /Online /Enable-Feature /All /FeatureName:Microsoft-Hyper-V\n\n\n通过 “设置” 启用 Hyper-V 角色推荐使用这种方式\n\n右键单击 Windows 按钮并选择 “应用和功能”。\n在 “相关设置” 下的右侧选择 “程序和功能 “。\n选择 “打开或关闭 Windows 功能”。\n选择 Hyper-V，然后单击确定。\n\n\n同样的，安装完成后，请重启。\n创建虚拟机在开始菜单找到并打开 Hyper-V 管理器，它应该位于 Windows 管理工具文件夹下面。\n或者直接搜索 Hyper-V\n\n打开后界面如下，我觉得比 VMware 界面好看点。\n\n快速创建点击快速创建，你将会看到\n\n类似于在线安装，比较简单。\n我尝试了导入本地安装源安装 Ubuntu 16.04，但是启动报错，找不到 Boot 信息。\n可能原因是：我的电脑不支持第二代虚拟机世代（是一种较新的虚拟化功能）\n新建虚拟机点击新建虚拟机，你将会进入一下界面\n\n跟着一步步来吧，首先你得准备好一个系统镜像（ISO 结尾的系统镜像文件）\n\n\n\n\n\n点击下一步，完成。\n\n接着，就进入了 Ubuntu 系统安装环节，省略了，大家应该都会装的。\n导入虚拟机除了自己创建，我们还可以导入别人创建好的虚拟机\n点击导入虚拟机\n\n\n以下是我创建的 Ubuntu 16.04 虚拟机，你可以直接导入使用。\nhttps://1drv.ms/f/s!AjfBPvEeW2r2hukqwAdOrPSMPpKZ4A\n参考Windows 10 上的 Hyper-V 简介\n","categories":["个人笔记"],"tags":["Hyper-V"]},{"title":"使用 Tesseract-Ocr 识别数字","url":"/p/5479/","content":"前言Tesseract-Ocr 是我在编写爬虫项目中，用来识别图片（不是验证码）的本地解决方案（因为客户不想使用 API 识别，太贵），识别率目前达到了 100%，可以说是相当了得，当然了，这取决于使用的 traineddata。\n\n\n简介\nTesseract 最初是在 1985 年至 1994 年间在 Hewlett-Packard Laboratories Bristol 和 Greeley Colorado 的 Hewlett-Packard Co 开发的，1996 年进行了一些更改，移植到 Windows，并且随着 C++ 在 1998 年兴起。2005 年 Tesseract 由惠普开源，然后从 2006 年至今，由谷歌继续开发。\n\nTesseract-Ocr 并不是一个软件，它是一个软件包，包含了一个 OCR 引擎【libtesseract】和一个命令行程序 【tesseract】。Tesseract 4 增加了一个基于 OCR 引擎的新神经网络（LSTM），该引擎专注于行级识别，但仍然支持 Tesseract 3 的传统 Tesseract OCR 引擎，该引擎通过识别字符模式来工作。\n要启用与 Tesseract 3 的兼容性，你需要使用 Legacy OCR Engine 模式（–oem 0）。它还需要支持传统引擎的 traineddata（训练好的数据文件），这些文件可以从 tessdata 存储库的文件获取。\nTesseract 支持识别 unicode（UTF-8），可以 “开箱即用” 识别 100 多种语言。\nTesseract 支持多种输出格式：纯文本，hOCR（HTML），PDF，TSV。主分支还具有 ALTO（XML）输出的实验支持。\n⭐️⭐️⭐️ 具体介绍可以上 tesseract-wiki 查看。\n在 Java 上使用创建项目，并引入 Jar 包Maven&lt;!-- https://mvnrepository.com/artifact/net.sourceforge.tess4j/tess4j --&gt;&lt;dependency&gt;    &lt;groupId&gt;net.sourceforge.tess4j&lt;/groupId&gt;    &lt;artifactId&gt;tess4j&lt;/artifactId&gt;    &lt;version&gt;4.3.1&lt;/version&gt;&lt;/dependency&gt;\n\nGradlecompile 'net.sourceforge.tess4j:tess4j:4.3.1'\n\n导入 traineddatatraineddata 是使用 Tesseract-Ocr 训练好的数据文件，可以直接使用。这些文件你可以去 tessdata 存储库查找，也可以去谷歌搜索，当然了，你也可以自己训练😂。\ntraineddata 通常以 *.traineddata 命名，其中 * 指的是支持的语言类型。在这里你可以看到 4.0.0 版本支持的语言以及 traineddata 列表。\n这次，我们选择 eng.traineddata 进行测试。下载 eng.traineddata 放入 /resources/traineddata 目录，如下图所示。\n\n编写测试代码初始化 Tesseract 引擎public class TesseractTest {    private ITesseract tesseract;    @Before    public void init() {        tesseract = new Tesseract();        System.out.println(\"tesseract init done...\");    }}\n\n实际上，上面的代码是无法正常运行的，因为找不到指定语言版本的 traineddata 文件。\nnet.sourceforge.tess4j:tess4j:4.1.1 提供的 API 并不好，在 Tesseract 构造函数中，没有提供可选参数的构造器。\npublic class Tesseract implements ITesseract {    // Tesseract使用的语言版本，用以选择traineddata    private String language = \"eng\";    // traineddata目录，里面放*.traineddata数据文件    private String datapath;\t    // 省略其他代码 ...    public Tesseract() {        try {            // 默认从系统环境变量获取traineddata目录            datapath = System.getenv(\"TESSDATA_PREFIX\");        } catch (Exception e) {            // ignore        } finally {            if (datapath == null) {                datapath = \"./\";            }        }    }        /**     * Sets language for OCR.     *     * @param language the language code, which follows ISO 639-3 standard.     */    @Override    public void setLanguage(String language) {        this.language = language;    }        /**     * Sets path to &lt;code&gt;tessdata&lt;/code&gt;.     *     * @param datapath the tessdata path to set     */    @Override    public void setDatapath(String datapath) {        this.datapath = datapath;    }        // 省略其他代码 ...}\n\n所以，我们可以选择设置环境变量 TESSDATA_PREFIX 为数据目录，或者通过 Java 编码的方式来设置。\ntesseract.setLanguage(\"eng\"); // 默认就是eng，你可以选择其他langtesseract.setDatapath(TesseractTest.class.getResource(\"/traineddata\").getPath().substring(1));\n\nOCR 识别测试tesseract 提供了一系列 doOcr 方法的重载，我们可以方便的进行 OCR 识别。\nString doOCR(File imageFile) throws TesseractException;String doOCR(File imageFile, Rectangle rect) throws TesseractException;String doOCR(BufferedImage bi) throws TesseractException;String doOCR(BufferedImage bi, Rectangle rect) throws TesseractException;String doOCR(List&lt;IIOImage&gt; imageList, Rectangle rect) throws TesseractException;String doOCR(List&lt;IIOImage&gt; imageList, String filename, Rectangle rect) throws TesseractException;String doOCR(int xsize, int ysize, ByteBuffer buf, Rectangle rect, int bpp) throws TesseractException;String doOCR(int xsize, int ysize, ByteBuffer buf, String filename, Rectangle rect, int bpp) throws TesseractException;\n\n可以看出，doOcr 方法支持多种图片识别方式，如图片文件、多个图片文件、图片文件局部处理等等方式。\n为了方便测试，我们选取最简单的图片文件方式测试。\n图片是个 URL 链接，如下所示\n\n@Testpublic void testOcr() throws IOException, TesseractException {    BufferedImage image = ImageIO.read(new URL(\"http://static8.ziroom.com/phoenix/pc/images/price/aacd14fbc53a106c7f0f0d667535683as.png\"));    String ocr = tesseract.doOCR(image);    System.out.println(\"ocr result : \" + ocr);}\n\n控制台输出：\ntesseract init done...ocr result : 2710386495\n\n识别准确率，主要在于你选择的训练数据文件，我使用的是数据文件是这个，对于数字的准确率基本上是 100%。\n异常如果你遭遇 Invalid memory access 异常，这是由于找不到对应 lang 的 *.traineddata 文件，请修改 language 和 datapath。\nInvalid memory accessjava.lang.Error: Invalid memory access\tat com.sun.jna.Native.invokePointer(Native Method)\tat com.sun.jna.Function.invokePointer(Function.java:470)\tat com.sun.jna.Function.invoke(Function.java:404)\tat com.sun.jna.Function.invoke(Function.java:315)\tat com.sun.jna.Library$Handler.invoke(Library.java:212)\tat com.sun.proxy.$Proxy9.TessBaseAPIGetUTF8Text(Unknown Source)\tat net.sourceforge.tess4j.Tesseract.getOCRText(Tesseract.java:495)\tat net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:321)\tat net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:293)\tat net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:274)\tat net.sourceforge.tess4j.Tesseract.doOCR(Tesseract.java:258)    ...\n\n训练工具https://github.com/tesseract-ocr/tesseract/wiki/AddOns\n训练数据仓库\ntessdata_best：基于 LSTM 引擎的训练数据，最佳最准确的 \ntessdata_fast：基于 LSTM 引擎的训练数据，快速（精简）版本 \ntessdata：支持双引擎（LSTM 和传统引擎），但 LSTM 训练数据不是最新的版本\n\n推荐使用 tessdata_best，虽然识别速度相对于 tessdata_fast 稍慢，但是准确率可以保证。\n参考tesseract-ocr-wiki\n","categories":["个人笔记"],"tags":["Tesseract-Ocr"]},{"title":"使用 certbot 给网站上免费的 SSL 证书","url":"/p/16060/","content":"本文介绍使用 certbot 为网站添加 HTTPS 支持，并自动更新\n\n\n前提\ndocker\ndocker-compose\n\n部署克隆仓库\n这一步必不可少，一定要按照作者的仓库目录结构来执行，完成后，可以自行更改 nginx/conf.d 下的配置文件。\n具体原因我也不知，但是不照做，会出现一些奇怪的问题。\n\n$ mkdir -p /data$ cd /data$ git clone https://ghproxy.com/https://github.com/gcdd1993/nginx-certbot$ cd nginx-certbot$ ls -ldrwxr-xr-x 4 root root 4096 Jun  8 22:01 ./drwxr-xr-x 5 root root 4096 Jun  8 21:49 ../drwxr-xr-x 4 root root 4096 Jun  8 21:53 data/-rw-r--r-- 1 root root  660 Jun  8 21:49 docker-compose.ymldrwxr-xr-x 8 root root 4096 Jun  8 21:49 .git/-rw-r--r-- 1 root root   14 Jun  8 21:49 .gitignore-rwxr-xr-x 1 root root 2286 Jun  8 22:01 init-letsencrypt.sh*-rw-r--r-- 1 root root 1074 Jun  8 21:49 LICENSE-rw-r--r-- 1 root root 1376 Jun  8 21:49 README.md\n\n为域名添加证书\n💡在这一步执行前，请确认已经将需要添加证书的域名指向本机公网 IP，因为在执行过程中，会进行服务器所属权校验，需要访问你所操作的域名\n\n1、修改 init-letsencrypt.sh 的 email 为你的邮箱$ vim init-letsencrypt.sh...email=\"gcwm99@gmail.com\"...\n\n2、修改操作域名$ sed -i 's/example.org/your_domain/g' data/nginx/app.conf \\\t&amp;&amp; sed -i 's/example.org/your_domain/g' init-letsencrypt.sh\n\n3、执行 init-letsencrypt.sh\n直到出现以下内容，说明已经完成\n\n$ ./init-letsencrypt.sh...Requesting a certificate for your_domainSuccessfully received certificate.Certificate is saved at: /etc/letsencrypt/live/your_domain/fullchain.pemKey is saved at:         /etc/letsencrypt/live/your_domain/privkey.pemThis certificate expires on 2021-09-06.These files will be updated when the certificate renews.NEXT STEPS:- The certificate will need to be renewed before it expires. Certbot can automatically renew the certificate in the background, but you may need to take steps to enable that functionality. See https://certbot.org/renewal-setup for instructions.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -If you like Certbot, please consider supporting our work by:* Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate* Donating to EFF:                    https://eff.org/donate-le- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n4、多域名操作\n步骤同上，先修改域名为待操作域名，然后执行 init-letsencrypt.sh\n\n$ sed -i 's/your_domain/your_domain2/g' data/nginx/app.conf \\\t&amp;&amp; sed -i 's/your_domain/your_domain2/g' init-letsencrypt.sh$ ./init-letsencrypt.sh...\n\n5、启动你的网站# 注释app.conf$ cd data/nginx$ mv app.conf app.conf.bak# 添加你的网站配置\n\n示例配置\nupstream my.site {    server localhost:8080;}server {    server_name your_domain;    proxy_read_timeout 600s;    proxy_send_timeout 600s;    location / {        add_header X-Frame-Options deny;        proxy_pass http://my.site;    }    listen 443 ssl; # managed by Certbot    ssl_certificate /etc/letsencrypt/live/your_domain/fullchain.pem;    ssl_certificate_key /etc/letsencrypt/live/your_domain/privkey.pem;    include /etc/letsencrypt/options-ssl-nginx.conf;    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;    server_tokens off;}server {    if ($host = your_domain) {        return 301 https://$host$request_uri;    } # managed by Certbot    server_name your_domain;    listen 80;    return 404; # managed by Certbot}\n\n更新证书\n作者给出的 docker-compose.yml 已经默认 12 小时检查并更新一次，所以非常省心\n\n$ docker exec -it nginx-certbot_certbot_1 certbot renew...The following certificates are not due for renewal yet:  /etc/letsencrypt/live/your_domain/fullchain.pem expires on 2021-09-06 (skipped)No renewals were attempted.\n\n相关资料\ncertbot\nnginx-certbot\n\n","categories":["个人笔记"],"tags":["默认"]},{"title":"分布式任务调度 XXL-JOB 初体验","url":"/p/29085/","content":"简介XXL-JOB 是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现已开放源代码并接入多家公司线上产品线，开箱即用。\n官方文档很完善，不多赘述。本文主要是搭建 XXL-JOB 和简单使用的记录。\n搭建 xxl-job-admin 管理端运行环境\nUbuntu 16.04 64 位\n Mysql 5.7\n\n安装 Mysql$ sudo apt-get update$ sudo apt-get install mysql-server## 设置mysql，主要是安全方面的，密码策略等$ mysql_secure_installation## 配置远程访问$ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnfbind-address = 0.0.0.0$ sudo service mysql restart$ sudo service mysql status● mysql.service - MySQL Community Server   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)   Active: active (running) since Wed 2019-06-05 13:23:41 HKT; 45s ago...\n\n创建数据库$ mysql -u root -pmysql&gt; CREATE database if NOT EXISTS `xxl-job` default character set utf8 collate utf8_general_ci;\n\n创建用户$ mysql -u root -pmysql&gt; CREATE USER 'xxl-job'@'%' IDENTIFIED BY 'xxlJob2019@';mysql&gt; GRANT ALL PRIVILEGES ON `xxl-job`.* TO 'xxl-job'@'%';\n\n本地测试 xxl-job-admin拉取最新源码$ git clone git@github.com:xuxueli/xxl-job.git$ cd xxl-job\n\n导入项目我比较熟悉 Idea 开发工具，所以这里使用 Idea 的 Gradle 项目进行演示。\n打开 xxl-job，项目结构如下\n\n测试项目打开 xxl-job-admin/resources/application.properties，修改 mysql 连接信息\n### xxl-job, datasourcespring.datasource.url=jdbc:mysql://192.168.32.129:3306/xxl-job?Unicode=true&amp;characterEncoding=UTF-8spring.datasource.username=xxl-jobspring.datasource.password=xxlJob2019@\n\n使用 /xxl-job/doc/db/tables_xxl_job.sql 初始化数据库，初始化完应该如下图\n\n准备就绪后，就可以启动项目了，然后打开地址 http://localhost:8080/xxl-job-admin 将会看到首页\n\n部署打包调度中心$ cd /xxl-job$ mvn install...[INFO] xxl-job ............................................ SUCCESS [  0.513 s][INFO] xxl-job-core ....................................... SUCCESS [  4.258 s][INFO] xxl-job-admin ...................................... SUCCESS [  5.525 s][INFO] xxl-job-executor-samples ........................... SUCCESS [  0.016 s][INFO] xxl-job-executor-sample-spring ..................... SUCCESS [  2.188 s][INFO] xxl-job-executor-sample-springboot ................. SUCCESS [  0.892 s][INFO] xxl-job-executor-sample-jfinal ..................... SUCCESS [  1.753 s][INFO] xxl-job-executor-sample-nutz ....................... SUCCESS [  1.316 s][INFO] xxl-job-executor-sample-frameless .................. SUCCESS [  0.358 s][INFO] xxl-job-executor-sample-jboot ...................... SUCCESS [  1.279 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time:  18.549 s[INFO] Finished at: 2019-06-05T14:40:25+08:00[INFO] ------------------------------------------------------------------------\n\n看到以上信息，说明我们打包成功了，在 /xxl-job/xxl-job-admin 目录下会存在 jar 文件：xxl-job-admin-2.1.0-SNAPSHOT.jar\n部署到服务器$ sudo apt install openjdk-8-jdk$ java -versionopenjdk version \"1.8.0_212\"OpenJDK Runtime Environment (build 1.8.0_212-8u212-b03-0ubuntu1.16.04.1-b03)OpenJDK 64-Bit Server VM (build 25.212-b03, mixed mode)$ sudo mkdir -p /data/xxl-job$ sudo cd /data/xxl-job## 上传我们打包好的jar至此目录，并添加软连接$ sudo ln -s xxl-job-admin-2.1.0-SNAPSHOT.jar current.jar## 注册为system服务，可以达到异常重启，开机自启等目的$ sudo vim /etc/systemd/system/xxl-job.service[Unit]Description=xxl-job Service DaemonAfter=mysql.service[Service]Environment=\"JAVA_OPTS= -Xmx1024m -Xms1024m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:NewRatio=3 -Dserver.port=8081\"# java要写绝对路径ExecStart=/usr/local/jdk/bin/java -jar /data/xxl-job/current.jarRestart=alwaysWorkingDirectory=/data/xxl-job/[Install]WantedBy=multi-user.target$ sudo systemctl enable xxl-job.service$ sudo service xxl-job start$ sudo service xxl-job status● xxl-job.service - xxl-job Service Daemon   Loaded: loaded (/etc/systemd/system/xxl-job.service; enabled; vendor preset: enabled)   Active: active (running) since Thu 2019-07-18 18:19:08 CST; 2min 19s ago Main PID: 27572 (java)   CGroup: /system.slice/xxl-job.service           └─27572 /usr/local/jdk/bin/java -jar /data/xxl-job/current.jar\n\n我们访问一下 http://192.168.32.129:8080/xxl-job-admin：\n\n测试任务调度以上，我们的任务调度管理端已经搭建完成，接下来，让我们测试下任务调度。\n直接使用自带的 SpringBoot 测试项目 xxl-job-executor-sample-springboot 进行测试，修改配置文件\nxxl-job-executor-sample-springboot=http://192.168.32.129:8080/xxl-job-admin\n\n自定义任务编写一个简单的任务，打印 100 次当前序列\npackage com.xxl.job.executor.service.jobhandler;import com.xxl.job.core.biz.model.ReturnT;import com.xxl.job.core.handler.IJobHandler;import com.xxl.job.core.handler.annotation.JobHandler;import com.xxl.job.core.log.XxlJobLogger;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;/** * @author gaochen * @date 2019/6/5 */@JobHandler(value=\"gcddJobHandler\")@Componentpublic class GcddJobHandler extends IJobHandler {    @Override    public ReturnT&lt;String&gt; execute(String param) throws Exception {        for (int i = 0; i &lt; 100; i++) {            XxlJobLogger.log(\"XXL-JOB, print \" + i);            TimeUnit.SECONDS.sleep(1);        }        return SUCCESS;    }}\n\n启动执行器然后启动执行器，启动完成后，我们会发现管理页面的执行器列表会多出我们刚才启动的执行器\n\n添加任务\n\n查看任务执行日志\n可以看到，任务已经按照我们的规划执行成功了，非常的方便。\n结语想要了解更详细的内容，请访问 xxl-job 官网\n","categories":["个人笔记"],"tags":["xxl-job","分布式"]},{"title":"切换国内源大全（收集整理）","url":"/p/14719/","content":"众所周知，国内的网络环境太糟糕了，所以收集整理各类服务切换国内源的方法\nUbuntu命令## 备份系统自带的source列表sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak \\\t&amp;&amp; sed -i 's/^\\(deb\\|deb-src\\) \\([^ ]*\\) \\(.*\\)/\\1 http:\\/\\/mirrors.aliyun.com\\/ubuntu \\3/' /etc/apt/sources.list \\\t&amp;&amp; apt-get update\n\n国内镜像源\n\n\n名称\n地址\n\n\n\n阿里镜像源 \nhttp://mirrors.aliyun.com/ubuntu\n\n\n 清华大学镜像源 \nhttps://mirrors.tuna.tsinghua.edu.cn/ubuntu/\n\n\n 网易镜像源 \nhttps://mirrors.163.com/ubuntu/\n\n\n 东北大学镜像源 \nhttp://mirror.neu.edu.cn/ubuntu/\n\n\nCentOS命令sudo sed -e 's|^mirrorlist=|#mirrorlist=|g' \\         -e 's|^#baseurl=http://mirror.centos.org|baseurl=https://mirrors.tuna.tsinghua.edu.cn|g' \\         -i.bak \\         /etc/yum.repos.d/CentOS-*.repo \\# 更新软件包缓存\t&amp;&amp; sudo yum makecache\n\n国内镜像源\n\n\n名称\n地址\n\n\n\n阿里镜像源 \nhttp://mirrors.aliyun.com/centos\n\n\n 清华大学镜像源 \nhttps://mirrors.tuna.tsinghua.edu.cn/centos\n\n\n 网易镜像源 \nhttps://mirrors.163.com/centos\n\n\n 东北大学镜像源 \nhttp://mirror.neu.edu.cn/centos\n\n\nAlpine命令cp /etc/apk/repositories /etc/apk/repositories.bak \\    &amp;&amp; sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\    &amp;&amp; apk update\n\nDocker命令# 修改/etc/docker/daemon.json#registry-mirrorssudo vim /etc/docker/daemon.json{    \"registry-mirrors\":[    \t\"https://registry.cn-hangzhou.aliyuncs.com\",    \t\"https://mirror.ccs.tencentyun.com\",    \t\"https://05f073ad3c0010ea0f4bc00b7105ec20.mirror.swr.myhuaweicloud.com\",    \t\"https://registry.docker-cn.com\",    \t\"http://f1361db2.m.daocloud.io\",        \"https://hub-mirror.c.163.com\",        \"https://mirror.baidubce.com\"    ]}systemctl daemon-reloadsystemctl restart docker\n\nMaven/Gradle\n在 setting.gradle 里面修改，Gradle 版本 6 以上\n\npluginManagement {    repositories {        mavenLocal()        repositories {            maven { url 'https://maven.aliyun.com/repository/google' }            maven { url 'https://maven.aliyun.com/repository/gradle-plugin' }            maven { url 'https://maven.aliyun.com/repository/public/' }        }        mavenCentral()        gradlePluginPortal()    }}dependencyResolutionManagement {    repositories {        mavenLocal()        maven { url = uri(\"https://maven.aliyun.com/repository/central\") } // central        maven { url = uri(\"https://maven.aliyun.com/repository/public\") } // jcenter &amp; public        maven { url = uri(\"https://maven.aliyun.com/repository/google\") } // google        maven { url = uri(\"https://maven.aliyun.com/repository/spring\") } // spring        maven { url = uri(\"https://maven.aliyun.com/repository/spring-plugin\") } // spring plugin        maven { url = uri(\"https://maven.aliyun.com/repository/grails-core\") } // spring plugin    }}\n\nPython命令pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\n国内镜像源\n\n\n名称\n地址\n\n\n\n清华源 \nhttps://pypi.tuna.tsinghua.edu.cn/simple\n\n\n 阿里源 \nhttps://mirrors.aliyun.com/pypi/simple/\n\n\n 腾讯源 \nhttp://mirrors.cloud.tencent.com/pypi/simple\n\n\n 豆瓣源 \nhttp://pypi.douban.com/simple/\n\n\nNodeJS命令$ npm config set registry https://registry.npm.taobao.org$ npm config get registryhttps://registry.npm.taobao.org\n\n参考资料\n阿里云镜像\n\n","categories":["个人笔记"],"tags":["运维"]},{"title":"发布开源项目到 Jcenter","url":"/p/53809/","content":"前言\n为了将阿里云短信开箱即用发布到 Jcenter 仓库，前前后后花费了 1 天半的时间，把端午节都搭进去了。终于今天收到了 Jcenter 的消息，自己发布的包被添加到了 Jcenter 仓库，也算给开源社区做了次小贡献😁😁😁。\n\n\n\n\n现在记录下踩过的坑。\n注册 Jcenter 账号要注意的地方，Jcenter 账号跟国内一样分为社区版和企业版，企业版当然是要付费的，而且很坑的是点进 Bintray 官网，首先映入眼帘的就是大大的 Start Your Free Trial（开始免费试用），一开始我就注册了企业版账号，后来删号重建了😂。我们应该点这里：\n\n填写信息后注册，我是直接使用的 Github 账号注册。\n创建 Repository点击右上角 View Profile\n\n在账号信息下方，我们点击 Add New Repository，创建新的仓库。\n\n在填写信息的时候，选择 Public（Private 是需要付钱的，大家都懂），如果你是 maven 项目，仓库名最好填写 maven，因为我在申请 Add To Jcenter 时，第一次失败了，要求我把项目放在 maven 路径下。\n\n创建 Package创建完仓库，就是创建包了，没什么好说的，你的应用叫啥名，包就叫啥名就行。\n创建完可以看到包的基本信息：\n\n打包上传这里使用的是开源项目 bintray-release，官方文档 bintray-release/wiki\n主要在 build.gradle 里添加如下信息\nbuildscript {    repositories {        jcenter()    }    dependencies {        classpath 'com.novoda:bintray-release:0.9.1'    }}apply plugin: 'com.novoda.bintray-release'publish {    userOrg = '你的Bintray用户名'    groupId = '应用的groupId，例如：io.github.gcdd1993'    artifactId = '应用的名称，例如：ali-sms-spring-boot-starter'    publishVersion = '应用的版本号，例如：1.0.0.RELEASE'    desc = '一句话概述你的应用干啥的'    website = '应用链接，一般写github地址就行，例如：https://github.com/gcdd1993/ali-sms-spring-boot-starter'}/** * 以下是我自己加的 * 第一个解决Gradle Task:jar skipped的问题 * 第二个解决javaDoc 'UTF-8'乱码问题 */jar {    enabled = true}tasks.withType(JavaCompile) {    options.encoding = \"UTF-8\"}\n\n接下来执行 gradle 命令：\n./gradlew bintrayUpload -PbintrayUser=BINTRAY_USERNAME -PbintrayKey=BINTRAY_KEY -PdryRun=false\n\n本地测试可以把 -PdryRun=false 改为 -PdryRun=true，这样就不会帮你上传到 Bintray，其他的都执行。\n\n看到以上信息，证明发布成功了。\nAdd To Jcenter发布成功后，你应该会在 Package 的 Files 标签下看到你上传的文件\n\n我们点击右上角 Actions 下的 Add To Jcenter\n\n填写信息，两个复选框我都勾选了，然后填写 Group Id，填上应用说明（最好用英文），然后等着就行了。\n一般来说 1~3 天你将会收到一封邮件，通知你的申请通过没有，如下\n\n👉如果没有通过，也会告诉你怎么改，所以不用担心。\n这时候再打开 Bintray 的 Package 页面，会发现 Included In Jcenter，证明已经被 Jcenter 收录了，其他人就可以正常使用啦。\n\nTravis CI 持续集成Travis CI 是什么就不介绍了，不明白的可以看下阮一峰的网络日志 - 持续集成服务 Travis CI 教程，Github 公开仓库免费的持续集成工具。\n项目根目录添加.travis.yml，填入以下信息（针对 Gradle 搭建的 Java 项目适用）\nlanguage: javasudo: requireddist: xenialjdk:  - openjdk8branches:  only:    - masterbefore_cache:  - rm -f  $HOME/.gradle/caches/modules-2/modules-2.lock  - rm -fr $HOME/.gradle/caches/*/plugin-resolution/cache:  directories:    - $HOME/.gradle/caches/    - $HOME/.gradle/wrapper/before_install:  - chmod +x gradlewinstall:  - ./gradlew jarscript:  - ./gradlew bintrayUpload -PbintrayUser=${bintray_user} -PbintrayKey=${bintray_key} -PdryRun=false\n\n其中变量 ${bintray_user} 和 ${bintray_key} 是 Travis CI 运行时环境变量，请到 Travis CI Settings 填写。\n\n参考文档\nbintray-release-wiki\nMaven Publish Plugin\n阮一峰的网络日志 - 持续集成服务 Travis CI 教程\n\n","categories":["个人笔记"],"tags":["Jcenter"]},{"title":"后端跨域的 N 种方法","url":"/p/34331/","content":"\n简单来说，CORS 是一种访问机制，英文全称是 Cross-Origin Resource Sharing，即我们常说的跨域资源共享，通过在服务器端设置响应头，把发起跨域的原始域名添加到 Access-Control-Allow-Origin 即可。\n\n\n\n返回新的 CorsFilter (全局跨域)\n在任意配置类，返回一个新的 CorsFilter Bean，并添加映射路径和具体的 CORS 配置信息。\n\n@Configurationpublic class GlobalCorsConfig {    @Bean    public CorsFilter corsFilter() {        //1.添加CORS配置信息        CorsConfiguration config = new CorsConfiguration();        //放行哪些原始域        config.addAllowedOrigin(\"*\");        //是否发送Cookie信息        config.setAllowCredentials(true);        //放行哪些原始域(请求方式)        config.addAllowedMethod(\"*\");        //放行哪些原始域(头部信息)        config.addAllowedHeader(\"*\");        //暴露哪些头部信息(因为跨域访问默认不能获取全部头部信息)        config.addExposedHeader(\"*\");        //2.添加映射路径        UrlBasedCorsConfigurationSource configSource = new UrlBasedCorsConfigurationSource();        configSource.registerCorsConfiguration(\"/**\", config);        //3.返回新的CorsFilter.        return new CorsFilter(configSource);    }}\n\n使用注解 (局部跨域)在方法上 (@RequestMapping) 使用注解 @CrossOrigin@RequestMapping(\"/hello\")@ResponseBody@CrossOrigin(\"http://localhost:8080\") public String index(){    return \"Hello World\";}\n\n在控制器 (@Controller) 上使用注解 @CrossOrigin@Controller@CrossOrigin(origins = \"http://domain.com\", maxAge = 3600)public class AccountController {    @RequestMapping(\"/hello\")    @ResponseBody    public String index(){        return \"Hello World\";    }}\n\n手工设置响应头 (局部跨域)\n使用 HttpServletResponse 对象添加响应头（Access-Control-Allow-Origin）来授权原始域，这里 Origin 的值也可以设置为”*” ，表示全部放行。\n\n@RequestMapping(\"/hello\")@ResponseBodypublic String index(HttpServletResponse response){    response.addHeader(\"Access-Control-Allow-Origin\", \"*\");    return \"Hello World\";}\n\nNginx 配置跨域upstream server {        server 127.0.0.1:8091;}server {        listen 80;        server_name domain.com;        location ^~/api {                //添加跨域请求头                proxy_set_header Access-Control-Allow-Origin *;                proxy_set_header Access-Control-Allow-Methods *;                proxy_set_header Access-Control-Allow-Headers *;                if ($request_method = 'OPTIONS') {                        return 204;                }                rewrite ^/api/(.+?)$ /$1 break;                proxy_pass http://server;        }}\n\n","categories":["个人笔记"],"tags":["跨域"]},{"title":"在 Dubbo 中使用 Kryo 序列化协议","url":"/p/34460/","content":"Kryo 是什么？Kryo 是用于 Java 的快速高效的二进制对象图序列化框架。\n\n该项目的目标是高速，小尺寸和易于使用的 API。不管是将对象持久保存到文件、数据库还是通过网络传输时，都可以尝试 Kryo。\n\nKryo 还可以执行自动的深浅复制 / 克隆。这是从对象到对象的直接复制，而不是从对象到字节的复制。\n\n\n具体可以参考 Kryo 官网\n在 Dubbo 中使用 Kryo\n本文基于 Dubbo 版本 2.7.8\n\nDubbo 支持非常多的序列化方式，如 hession2、avro、FST 等等，其中 Dubbo 官网推荐的序列化方式是 Kryo，因为 Kryo 是一种非常成熟的序列化实现，已经在 Twitter、Groupon、Yahoo 以及多个著名开源项目（如 Hive、Storm）中广泛的使用。\n开始在 Dubbo 中使用 Kryo 非常方便，首先引入依赖\n// 解决一些Kryo特殊序列化，https://github.com/magro/kryo-serializersimplementation  'de.javakaffee:kryo-serializers:0.43'// 高性能序列化框架, https://github.com/EsotericSoftware/kryoimplementation 'com.esotericsoftware:kryo:4.0.2'\n\n如果只是简单使用，引入 kryo 即可，如果要支持一些例如 List 接口，则需要引入 kryo-serializers，它针对一些特殊类为 Kryo 做了适配。\n配置在 Dubbo 中启用 Kryo 序列化方式，这里使用 SpringBoot YML 配置方式\nprotocol:    serialization: kryo    optimizer: org.hmwebframework.microservice.dubbo.serialize.SerializationOptimizerImpl\n\n其中 org.hmwebframework.microservice.dubbo.serialize.SerializationOptimizerImpl 是指定 Kryo 序列化类，例如\npublic class SerializationOptimizerImpl implements SerializationOptimizer {    public Collection&lt;Class&gt; getSerializableClasses() {        List&lt;Class&gt; classes = new LinkedList&lt;Class&gt;();        classes.add(BidRequest.class);        classes.add(BidResponse.class);        classes.add(Device.class);        classes.add(Geo.class);        classes.add(Impression.class);        classes.add(SeatBid.class);        return classes;    }}\n\n到这，Dubbo 使用 Kryo 就已经 OK 了。\n为什么要定义 SerializationOptimizer 实现类？首先我们分析下 SerializationOptimizer\npublic interface SerializationOptimizer {    /**     * Get serializable classes     *     * @return serializable classes     * */    Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses();}\n\n提供了一个接口方法，用于获取序列化的 java 类型列表，在 DubboProtocol#optimizeSerialization 中被使用\nprivate void optimizeSerialization(URL url) throws RpcException {    // ...    try {        Class clazz = Thread.currentThread().getContextClassLoader().loadClass(className);        // 判断是否为SerializationOptimizer实现类        if (!SerializationOptimizer.class.isAssignableFrom(clazz)) {            throw new RpcException(\"The serialization optimizer \" + className + \" isn't an instance of \" + SerializationOptimizer.class.getName());        }        SerializationOptimizer optimizer = (SerializationOptimizer) clazz.newInstance();        if (optimizer.getSerializableClasses() == null) {            return;        }        // 将SerializationOptimizer中定义的类型列表，注册到SerializableClassRegistry        for (Class c : optimizer.getSerializableClasses()) {            SerializableClassRegistry.registerClass(c);        }        optimizers.add(className);    } catch (ClassNotFoundException e) {        // ...    }}\n\n接着，从 SerializableClassRegistry 中拿出注册的类型，进行 Kryo 的类型注册，可以看到 SerializableClassRegistry#getRegisteredClasses 被 FST 和 Kryo 使用，证明 FST 和 Kryo 都需要进行序列化类的注册，当然 FST 也支持不注册序列化类型。\nKryo 类注册的具体细节，AbstractKryoFactory#create\n// ...for (Class clazz : registrations) {    kryo.register(clazz);}// 遍历取出SerializableClassRegistry的注册类，依次将类注册到KryoSerializableClassRegistry.getRegisteredClasses().forEach((clazz, ser) -&gt; {    if (ser == null) {        kryo.register(clazz);    } else {        kryo.register(clazz, (Serializer) ser);    }});\n\n循环取出 SerializableClassRegistry 中的注册类进行注册，看到这里也能明白，为什么 Dubbo 官网的 SerializationOptimizer 例子需要使用 LinkedList。\n为什么 Kryo 需要进行类的注册，且保持顺序？类的注册在 Dubbo 这样的 RPC 框架进行通信时，性能瓶颈往往在于 RPC 传输过程中的网络 IO 耗时，提升网络 IO 的办法，一是加大带宽，二是减小传输的字节数，而高性能序列化框架可以做到的就是减小传输的字节数。\nKryo 注册类的时候，使用了一个 int 类型的 ID 来与类进行关联，在序列化该类的实例时，用 int ID 来标识类型，反序列化该类时，同样通过 int ID 来找到类型，这比写出类名高效的多。\n维持类注册顺序Kryo 注册类的时候，可以指定类关联的 int ID，例如\nKryo kryo = new Kryo();kryo.register(SomeClass.class, 10);kryo.register(AnotherClass.class, 11);kryo.register(YetAnotherClass.class, 12);\n\n但是上面我们讲到，Dubbo 对 Kryo 做了相当程度的集成，导致我们没法给类指定 int ID，但是我们可以保证服务提供方和消费方类注册顺序的一致，间接地保证了 int ID 的一致性。\n优化反射获取代注册的类\n在 Dubbo 中使用 Kryo 时，我们需要实现一个 SerializationOptimizer，并提供一个注册类列表。随着项目规模扩大，不可能时时刻刻想着维护这个注册类列表，所以我们可以使用反射来自动获取这个注册类列表\n\n引入依赖\n// Java反射工具包implementation 'org.reflections:reflections:0.9.11'\n\n编写接口，\npublic interface KryoDubboSerializable        extends Serializable {}\n\n编写 SerializationOptimizer 实现类\n@Slf4jpublic abstract class AbstractSerializationOptimizerImpl        implements SerializationOptimizer {    private final List&lt;Class&lt;?&gt;&gt; classList;    public AbstractSerializationOptimizerImpl() {        var reflections = new Reflections(                new ConfigurationBuilder()                        .forPackages(basePackage())                        .addScanners(new SubTypesScanner())        );        this.classList = reflections.getSubTypesOf(KryoDubboSerializable.class)                .stream()            \t// Kryo序列化协议要求类注册顺序一致                .sorted(Comparator.comparing(Class::getSimpleName))                .collect(Collectors.toList());        log.info(\"load {} classes to use kryo serializable\", this.classList.size());        log.debug(\"kryo serializable classes: {}\", this.classList.stream().map(Class::getSimpleName).collect(Collectors.joining(\",\")));    }    @Override    public Collection&lt;Class&lt;?&gt;&gt; getSerializableClasses() {        return classList;    }    /**     * 扫描包路径     *     * @return packages     */    protected abstract String[] basePackage();}\n\n每次使用时，只需要继承 AbstractSerializationOptimizerImpl，并提供待注册包路径（支持多个），待注册的类需要实现 KryoDubboSerializable 接口，这是为了在一定程度上提升灵活性（如果不需要注册到 Kryo，不实现该接口即可）。\n参考\nhttps://dubbo.apache.org/zh/docs/v2.7/user/serialization/\nhttps://github.com/EsotericSoftware/kryo\n\n","categories":["个人笔记"],"tags":["Java"]},{"title":"在数字时代谈数字隐私","url":"/p/39168/","content":"在数字时代，如何保护个人隐私？这篇文章或许能帮到你。\n\n此文系转载\n\n\n\nManual\n世界上有两种密码：一种是防止你的小妹妹偷看你的文件；另一种是防止当局阅读你的文件.\n\n\nBruce Schneier《应用密码学》\n\n\n    \n\n\n\n项目起因 : 为了保护自己的隐私，慢慢的开始学习与收集各种手段方法、网站、工具，我把这种行为称作数字洁癖。这些手段方法藏着掖着也不能当传家宝，那干脆就分享出来造福大众.\n 涉及内容 : 个人敏感信息查询，保护措施，开源信息收集 (OSINT) 对抗\n事件集合 : 还不清楚严重性？进来了解近年来的数据泄露、供应链污染事件:Dork-Admin\n 项目地址 : https://github.com/ffffffff0x/Digital-Privacy\n\n\n免责声明\n本项目所有内容,仅供学习和研究使用,请勿使用项目的技术手段用于非法用途,任何人造成的任何负面影响,与本人无关.\n本文档所有内容、新闻皆不代表本人态度、立场,如果有建议或方案,欢迎提交 issues\n未收及不会收取任何广告费用,推荐的所有工具链接与本人无任何利害关系\n\nTips\n\n对于各类平台尽量使用不同昵称、头像.\n 多平台不要使用统一、相似的密码，请建立一套自己的密码管理方式，推荐使用密码管理器。对于密码管理器本身的安全性，可以参考这个报告 https://www.securityevaluators.com/casestudies/password-manager-hacking/\n 管住自己的炫耀欲.\n 不要相信哪个公司不作恶、重视隐私.(感觉和 2 有冲突啊 XD)\n 尽量少用部分浏览器 “记住密码” 的功能，针对浏览器的取证工具不少，很容易就可以提取出 Cookie、浏览记录、保存的密码等。\n不要以为开虚拟机、挂 vpn 就很安全，webRTC 泄露 IP, 浏览器指纹，通过 DNS 判断 (参考网飞), 系统时间，浏览器 0day, 等等等等.\n 定时清空你的邮件、短信、通话记录、回收站.\n 所以不要干坏事、不要干坏事、不要干坏事。\n\n\n大纲\n敏感信息\n\n隐私查询\n浏览器指纹\n密码泄露查询\nDNS 信息\n定位\n\n\n保护措施\n\n操作系统\n软件 - 脚本\n浏览器扩展\n安全删除\n加密\nflash\n輸入法\n搜索引擎\n身份生成\n邮箱 - 信息\n查邮箱存活\n短信接码\n临时邮箱\n匿名邮箱\n\n\n平台管控设置\n\n\nOSINT\n\n设备 - 语音助手\n平台 - 软件\n社交网络\n各类搜索\n常用搜索引擎\n网页快照\n图片反向搜索\nacg 图片反向搜索\n\n\n航班 / 飞机信息\n船舶信息\n货车位置\n物流信息\n车辆信息\nVIN 码\n\n\n个人可信度\n网络空间测绘\ntor 信息\n学术信息\n专利 - 商标\n报刊信息\n开放数据集\nBGP 信息\n电子硬件相关\n社交 - 人际关系\n企业信息\n博客搜索\n\n\n文件资源\nPDF\n音乐\n\n\n天气 - 环境\n地图\n数据展示\n网络威胁\n\n\n\n\n\n\n敏感信息隐私查询\n从邮箱查询\n\n你注册过哪些网站？一搜便知\nFind email addresses in seconds • Hunter (Email Hunter) - email 信息查询工具\n\n\n从用户名查询\n\nInstant Username Search - 实时搜索 100 多个社交媒体网站的用户名。\nCheckUsernames - 测某账号是否在全球 500 多个社交媒体中是否有注册。\nWhatsMyName Web - 搜索许多网站上存在的用户名。\nNameCheckup - 查找可用的用户名 \nNamechk\nKnowEm Username Search\nsherlock-project/sherlock - 在不同的社交网络上通过用户名搜寻账户\n\n\n从 IP 查询\n\nTorrent downloads and distributions for IP - 查你这个 IP 下载过哪些磁力链接🔗 (太缺德了😂)\n\n\n 混合查询\n\nUsername Search - 找到用户名、电子邮件地址或电话号码背后的人。\nn0tr00t/Sreg - 使用者通过输入 email、phone、username 的返回用户注册的所有互联网信息.\n\n\n\n\n浏览器指纹\nAm I unique? - 显示了你的操作系统、浏览器、浏览器版本以及你的时区和语言。\n\nUnique Machine - 这是一个浏览器指纹技术的项目，它不仅可以在一个浏览器内跟踪用户，还可以在同一台机器上的不同浏览器间跟踪用户。\n\nPanopticlick - 分析你的浏览器和附加组件对在线追踪技术的保护程度。\n\nDetect Canvas Fingerprint - 这个页面使用不同的技术来识别是否安装了浏览器扩展来欺骗 canvas 指纹结果。\n\nWhat is my User Agent - 检测网站服务器和客户端代码在访问网站时看到的用户代理字符串。\n\nSploit.io - 这个网页可以测试你在浏览网页的时候到底会暴露出哪些信息出去；从漏洞，到地理位置，到浏览器指纹，用没用代理等等。\n\nSupercookie - 这个网站可以通过浏览器访问过的图标来识别用户指纹。\n\n相关文章\n\n浏览器指纹\n2.5 代指纹追踪技术 — 跨浏览器指纹识别\n浏览器指纹真的有效吗？\n浏览器的隐身模式有多隐身？\nCookieless cookies - 一种不使用 cookies 甚至是 Javascript 的方式来追踪用户。网站解释了它是如何工作的，以及如何保护自己。\n\n\nwebRTC\n\n 你的 VPN 泄漏 IP 了吗：仍有 20% 的 VPN 服务商未解决 WebRTC 漏洞问题 \n\n\nleak HTTP\n\ncure53/HTTPLeaks - 对于网页泄漏 HTTP 请求的方法总结\n\n\n\n案例\n\nStripe is Silently Recording Your Movements On its Customers’ Websites - Stripe 在客户网站上隐秘记录用户访问 URL 和鼠标光标移动等信息\n\n\n密码泄露查询\nDeHashed - 可通过用户名、邮箱、地址等搜索是否在该网站收集的一百多亿条信息内。\nHave I Been Pwned - 通过查询用于注册的邮箱，发现是否有相关密码被盗。\npwd query - 检查你的密码是否已经从数据泄露中泄露了……\nFirefox Monitor - 可通过邮箱搜索看看你是否已经成为了网络数据泄露的一部分。\nVigilante.pw - 搜索数据泄露事件，数据库列表 \nOCCRP Aleph - 可通过搜索名字、账号、邮箱等查询相关泄露信息情况。\nSnusbase - 搜索电子邮件，名称和用户名，IP 地址，电话，哈希或密码，检测自己的信息是否已泄露。\n泄露密码库 \nhttps://cloud.mail.ru/public/2eHX/38Ek7Lmfx?tdsourcetag=s_pctim_aiomsg\nhttps://downloads.skullsecurity.org/passwords/\n\n\n\n\nDNS 信息\nDNS leak test - 检测 DNS 泄露的网站 \nWhat’s My DNS Server? - 该网站通过观察你的 DNS 请求在互联网上的处理方式，主动确定你的计算机使用的 DNS 服务器。\n\n\n定位案例\n\n看我如何通过邮箱获取 IP 定位\n利用 Wireshark 任意获取 QQ 好友 IP 实施精准定位\n跨国定位手机の奥义\nFor sale: Systems that can secretly track where cellphone users go around the globe - The Washington Post - 利用 SS7 信令漏洞跟踪全球手机用户的系统 \nUsing the Sun and the Shadows for Geolocation - 通过阳光和阴影进行定位\n\nGPS 定位\n\nMyGeoPosition.com - 免费地理编译，地理译码 / 地理元数据标记 (Geo-Metatag) / 地理标记 (Geotag) / KML 文件！\nRTBAsia ODX - Open Data Exchange \n拾取坐标系统\nopenGPS.cn - 传统 IP 定位\n经纬度在线查询\n免费的客户端反向地理编码\n\n基站定位\n\nminigps - 基站定位查询\n\n手机号码查询\n\n138 查 - 手机归属地查询\n手机号码定位\nReverse Phone Lookup | Phone Number Search - Spokeo - 将姓名和身份与未知电话号码进行匹配 \nCarrierLookup - 反查电话\n\nIP 信息\n\nRIPEstat - Internet Measurements and Analysis\nIP Info - IP 信息 \nWhat is your IP, what is your DNS - IP/DNS Detect\nip8 - IP Lookup Tool \n查看自己的 IP 地址\nIPList\nPlot IP - IP Addresses\nDB-IP - IP Geolocation API and databases\nBigDataCloud - Essential IP Geolocation APIs\nIPIP.NET - IP 地址库 \nIP 查询\nip 查询\nWhois\n查错网 - 国家 IP 段查询、全球国家 IP 段 \nipplus360.com - 全球 IP 地址定位平台 \nopenGPS.cn - 高精度 IP 定位 \nIP 地址查询\n多数据源 IP 地址查询\nGet your IPv4 and IPv6 address instantly\nWhat is my IP Address\nWhat Is My IP Address?\nipapi - IP Address Location\nIPinfo.io - IP Address API and Data Solutions\n\nIPv6 信息\n\nIPv6 地址查询工具\nLocate IPv6 Address Online - IPv6 Whois Lookup\nfindIPv6 - Lookup and locate an IPv6 address\n\n邮箱\n\nEmail Search | Reverse Email Lookup - Spokeo - 查询任何电子邮件以查看谁拥有它\n\n\n保护措施文章\n\nGoogleAlternatives - FuckOffGoogle -  - 谷歌工具替代品清单 \nNo More Google - 谷歌工具替代品清单\n安全手册：这里是你需要的几乎所有安全上网工具；以及为什么建议不要使用以美国为基地的网络服务\n隱私工具 - 加密安全對抗全球大規模監控\nSecurity Checklist\n终极在线隐私指南\n隐私大爆炸，你得学几招保护自己\n如何知道有人正在线人肉你？6 个简单的方法\n\n操作系统\nTails\nQubes OS\nWhonix\n\n\n软件 - 脚本\n浏览器\n\nEloston/ungoogled-chromium\n\n\n 通讯\n\ntelegram\n\n\n 网络审计\n\nW10Privacy\nabcnews/data-life\n\n\n 匿名\n\ntor\ni2p/i2p.i2p\n\n\n\n\n浏览器扩展\nchrome\n\nDecentraleyes\nCookie AutoDelete\nLastPass Password Manager\nPrivacy Badger\nUser-Agent Switcher for Chrome\nHTTPS Everywhere\nNoScript\nuBlock Origin\n\n\nfirefox🦊\n\nDecentraleyes\nCookie AutoDelete\nLastPass Password Manager\nPrivacy Badger\nUser-Agent Switcher and Manager\nHTTPS Everywhere\nNoScript\nuBlock Origin\n\n\nstylish\n\n“Stylish” browser extension steals all your internet history\nStylus is a Stylish fork without analytics\n\n\n\n安全删除\n如何：在 Windows 上安全地删除数据\n\n\n加密\nVeraCrypt\nVHD 虚拟磁盘 + BitLocker 加密\n\n\nflash最好的选择就是远离flash\n\n支持到 32_0_0_223_Adobe {过}{滤} FlashPlayer 去区域限制\n\n\n輸入法\nRIME\n\n\n某狗输入法参考文章 SougouCloud.exe 浅析\n\n搜索引擎以下是在隐私保护方面较为优秀的搜索引擎,但其搜索质量不敢保证\n\nDuckDuckGo\nsearx.me\nStartPage Web Search\n\n\n身份生成\nGenerate a Random Name - 随机身份生成 \nFake Address, Random Address Generator - 随机身份生成 \nBehind the Name - Random Name Generator\nEasy Random Name Picker - Random Name Generator\nElfQrin - Fake Identity ID Random Name Generator\nRandom User Generator\n在线身份证号码生成器\n中国大陆内地姓名、身份证号、银行卡号生成器\n在线身份证号码生成器\nairob0t/idcardgenerator 身份证图片生成工具\ngh0stkey/RGPerson - 随机身份生成脚本 \nnaozibuhao/idcard - 身份证生成器 \nJust Delete Me - 假身份生成器 (这个网站的图标，好像在哪里看过🤔)\nFake Person/Name Generator | User Identity, Account and Profile Generator\nfaker.js\nFake Person/Name Generator\nFull Contact Information Generator\n\n图片生成\n\n伪造人像\nArtbreeder\nComixify\nThis Waifu Does Not Exist - Gwern\n虚拟猫咪\nWhich Face is Real?\nSPADE Project Page\nSelfie2Anime\nReflect.tech\nGallery of AI Generated Faces | Generated.photos\nピクセルミー | ドット絵ジェネレーター\n\n\n照片信息EXIF 信息\n\nEXIF 信息查看器\nExifShot App\n如何为老照片添加 Exif 日期数据？ - 小众软件\n\n相关文章\n\nThe Secret Life Of JPEGs – NixIntel\n\n\n邮箱 - 信息查邮箱存活\nEmail 地址检查、检测 Email 地址真实性、检测电子邮件地址真实性–查错网\nVerify Email Address Online - Free Email Verifier - Free Email Address Verification\n免费在线批量验证邮箱有效性 - EmailCamel.com\nEmail Verifier - Verify Email Address For Free With Our Verifier Tool\n\n短信接码注:此类平台来的快,去的快,慎用\n\n以下部分内容来自 [国内外短信接码平台合集 | 合集网]\n\n国外免费接码平台\nhttp://sms.sellaite.comhttps://ch.freephonenum.comhttps://zh.mytrashmobile.comhttps://www.receive-sms-online.infohttps://sms-online.co/receive-free-smshttps://receive-sms.comhttp://receivefreesms.com/https://www.receivesmsonline.net/https://www.freeonlinephone.org/https://us-phone-number.comhttps://temporary-phone-number.comhttps://www.receivesms.co/https://pingme.tel/receive-sms-online-cn/http://receivefreesms.net/http://receivesmsonline.in/https://sms-receive.net/https://www.receivesms.net/https://www.textnow.com/http://receive-sms-now.com/http://receive-sms-online.com/https://www.afreesms.com/freesms/https://textfree.us/#/loginhttp://receivesmsverification.com/http://freereceivesmsonline.com/\n\n国内免费接码平台\nhttps://www.bfkdim.com/https://www.yinsiduanxin.com/https://www.materialtools.com/http://www.114sim.com/http://zg.114sim.com/http://z-sms.com/https://www.zusms.com/https://yunjiema.net/http://jiema.tech/https://mianfeijiema.com/http://www.xnsms.com/https://xinghai.party/https://jiemahao.com/https://www.lothelper.com/cnhttp://www.zsrq.net/http://www.kakasms.com/https://www.suiyongsuiqi.com/zh/http://www.z-sms.com/https://yunduanxin.net/\n\n国外收费接码平台\nhttps://sms-activate.ru/cn/ 1$起充，有中文页面https://5sim.nethttp://smspva.comhttp://give-sms.com 俄罗斯接码平台https://onlinesim.ru/zh 就俄罗斯的码便宜，每个码最少1卢布https://www.smsjiema.com/ 美国实体卡号码，可以注册GVhttps://www.textverified.com/ 2刀起充，接码也挺贵的https://autofications.com/ 不算贵也不算便宜，最低$0.5一个码https://service.pvaverify.com/ 美国实体卡https://getsms.online/ 俄罗斯接码平台\n\n国内收费接码平台\nJieMa.Tech：https://f4.work随用随弃：https://www.suiyongsuiqi.com\n\n临时邮箱注:此类平台来的快,去的快,慎用\nhttp://www.yopmail.com/zh/https://10minutemail.com/https://10minutemail.net/https://www.guerrillamail.com/zh/inboxhttp://www.fakemailgenerator.com/#/dayrep.com/Firly1970/https://temp-mail.org/en/https://www.guerrillamail.com/http://tool.chacuo.net/mailsendhttps://maildrop.cc/http://tool.chacuo.net/mailanonymoushttps://tempmail.altmails.com/https://www.snapmail.cc/https://www.linshi-email.com/\n\n匿名邮箱\nProtonMail\nGet secure, reliable email hosting – FastMail\nxyfir/ptorx\nTutanota\n\n\n平台管控设置\nbaidu\n\n应用授权\n\n\n Facebook\n\n应用授权\n\n\n github\n\n应用授权\n\n\n google\n\n广告个性化 / 谷歌眼中的你\n活动记录\n活动控件\n最近使用过的设备\n应用授权\n扩展阅读\n自動刪除你的 Google 網路和應用程式活動紀錄設定教學\n\n\n\n\n Microsoft\n\n产品和服务的隐私设置，以及查看和清除 Microsoft 保存到云的数据的位置\n应用授权\n\n\n twitter\n\n应用授权 旧版 新版\n興趣和廣告資料\n密碼重設保護\n\n\n tencent\n\n广告个性化\n应用授权\n微信应用授权 [设置 - 隐私 - 授权管理]\n\n\n\n\nOSINT\nOSINT (Open-source intelligence) 指开源情报，一项从媒体、网络、博客、视频，等公开来源中进行信息收集、提取的技术。\n\n相关的网站与资源\n\n丁爸网\n微信公众号 情报小蜜蜂 little_bee007\niYouPort OSINT 专栏\nsinwindie/OSINT - 各种平台的 OSINT “一张图” 系列 \nblaCCkHatHacEEkr/OSINT_TIPS - OSINT 技巧合集 \nThe Privacy, Security, &amp; OSINT Show - 讲述、介绍各类 OSINT 技能的博客 \nOSINT Framework - 非常著名的 OSINT 框架，有着非常丰富的 OSINT 资源\n\n案例\n\nUsing Flight Tracking For Geolocation – Quiztime 30th October 2019 – NixIntel - 通过一张照片中的飞机轨迹寻找到目标地址的案例 \nA Guide to Open Source Intelligence (OSINT) - Columbia Journalism Review - 一份 OSINT 指南，汇总了很多案例 \nIntelligence Gathering on U.S. Critical Infrastructure - Industrial Control Systems (ICS) Cyber Security Conference - 一篇对于美国工控领域设备的 OSINT 分析文章 \nbellingcat - A Beginner’s Guide To Flight Tracking - bellingcat - 一篇关于追踪飞机的文章\n如果只知道一个电话号码，你能挖出多少有效信息？\nGeoPrivacy and news media. Earlier this week I read a news article… - 查询新闻中鬼屋的真实位置 \nFinding McAfee: A Case Study on Geoprofiling and Imagery Analysis - 通过一张图片分析 McAfee 的位置 \nTracking ships and visualize them in QGIS - 使用 QGIS 可视化跟踪目标船只 \nOSINT Amateur Hour - 调查照片位置的案例\n一张快递单到底能泄露多少个人信息\n已知邮箱，求手机号码？\n【图片挖掘】国外图片挖掘案例（附过程及工具）\nBringing VandaTheGod down to Earth: Exposing the person behind a 7-year hacktivism campaign - checkpoint 社工 VandaTheGod 的案例 \nSubtle Information Hackers Find in the Background of Your Social Media Photos - 常见的通过图片泄露敏感信息的案例 \nCorporate Reconnaissance - 介绍如何挖掘一家企业的相关信息\n寻找时间的踪迹：侦探挑战游戏\n如何获取、挖掘、分析各种来源的调查数据完整指南：解码秘密（4）- 确保真实和确保道德的技巧\n\nOSINT 情报工具\n\nhttps://intelx.io/tools - 在线使用的开源情报和取证工具清单 \nOSINT Recon Tool - 在线的 osint 工具集合，加上思维脑图 \nwoj-ciech/SocialPath - 跟踪社交媒体平台上的用户 \nGreenwolf/social_mapper - 通过面部识别跟踪不同社交平台目标的工具 \nbhavsec/reconspider - 可用于扫描 IP 地址，电子邮件，网站，组织的 OSINT 工具 \nSpiderFoot\n\n设备 - 语音助手\nAlexa\n\n除非手动删除，不然 Alexa 上的语音资料会被亚马逊一直保留\n隔屏有耳调查｜亚马逊智能音箱有千人监听团队，曾听到性侵案\n亚马逊提供 Alexa 录音的非人工审听选项\n\n\n Cortana\n\n继苹果谷歌后：微软被曝监听用户 Skype 和 Cortana 录音\nRevealed: Microsoft Contractors Are Listening to Some Skype Calls\n微软被指使用廉价合同工完成 Cortana 语音收听工作\n微软：暂不会停止对 Skype 和 Cortana 对话的人工审查\n\n\n Google Home\n\n谷歌承认通过语音助手收集用户谈话内容：仅用于开发\nGoogle admits workers listen to some smart device recordings\n\n\nHP printers\n\n惠普打印机被发现偷偷回传数据：隐藏极深\nHP printers try to send data back to HP about your devices and what you print\n\n\nSiri\n\nSiri 被曝偷偷给用户隐私录音，还上传给苹果\n苹果回应 Siri 录音用户谈话内容：使用了 1% 的用户录音\nSiri 人工评估计划的员工说，自己每天要听 1000 条录音\nApple to stop default practice of keeping Siri recordings\n\n\nMisc\n\n卖二手设备一定要注意，你的信息可能并没被删除\n\n\n\n\n\n MIUI\n\n小米被指记录用户的 Web 和手机使用数据\n关于去广告 \n1000 字够不够？小米 MIUI 10 去广告教程\nHow to disable most push advertisement on MIUI China Version\n 在「设置 - 小米账号 - 隐私协议等 - 系统广告」里关闭广告.  \n\n\n\n\nios\n  设置–&gt; 隐私–&gt; 定位服务–&gt; 系统服务–&gt; 重要地点\n\n\n\n平台 - 软件\nAirbnb\n\n相关文章 \nAirbnb 上的 OSINT 信息收集 - 讲述了在 Airbnb 上可能造成的隐私泄露问题。\nI Accidentally Uncovered a Nationwide Scam on Airbnb - 一篇描述关于 Airbnb 上诈骗状况的文章，揭露了如今人们在 Airbnb 上被骗的种种方式\n\n\n\n\n AVAST\n\n知名安全软件 AVAST 被爆收集用户各种隐私信息并公开出售给其他公司\n\n\n AWS S3\n\n相关工具 \nPublic buckets by grayhatwarfare - S3 Buckets 搜索引擎\n\n\n\n\n Brave\n\n事件记录 \nBrave 劫持链接插入返利代码\nBrave 称自动插入返利代码是失误所致\n\n\n\n\n Chrome\n\n事件记录\n\n谷歌崩溃报告究竟收集了哪些信息 个人信息如何处置\n用户浏览器被互联网大厂私自 [托管]？仔细一查，这事并不简单\n111 个 Chrome 扩展被发现秘密收集用户敏感数据\n\n\n隐身模式\n\nBypassing anti-incognito detection in Google Chrome\nGoogle Chrome Incognito Mode Can Still Be Detected by These Methods\n\n\n\n\nEDGE\n\n事件记录 \nUWP 版 EDGE 浏览器被发现将用户安全标识符和网址发送给微软分析\nEdge 被吐槽向微软发送包含用户 SID 和访问站点完整 URL 等在内的信息\nMicrosoft Edge 被指悄悄导入了 Firefox 数据\n\n\n\n\n Flickr\n\n相关文章 \nEmail to Flickr account\nEmail to Flickr account part 2\n\n\n\n\nfirefox\n\n国际版和中文版 \nFirefox 火狐国际版和中文版的区别\nFirefox 如何查看和切换 本地服务 与 全球服务\nMozilla 向用户展示 Firefox 收集的遥测数据\n在地址栏输入 about:telemetry，用户可以看到 Mozilla 收集的遥测数据如浏览器设置、安装的扩展、操作系统 / 硬件信息，浏览器会话以及运行的进程。\n\n\n\n\n\n\nkaspersky\n\n事件记录\n卡巴斯基修复四年老漏洞 注入 HTML 源码的唯一标识符会泄露用户隐私\n安全研究人员在测试卡巴斯基杀毒软件时发现它会以安全的名义在用户访问的每一个网页注入它的脚本，而这个脚本还带有唯一 ID, 这个 ID 在不同计算机上是不同的，也就是说它可以作为跟踪代码使用。研究人员将这一发现报告给了卡巴斯基。卡巴斯基承认了数据泄漏，它释出了补丁修复了编号为 CVE-2019-8286 的问题。这个补丁去除了唯一 ID, 留下了相同的 ID, 也就是说网站仍然会知道有安装了卡巴斯基软件的用户访问了.\n\n\nUnique Kaspersky AV User ID Allowed 3rd-Party Web Tracking\n\n\n\n\nNetflix\n\n事件记录 \nNetflix 解释他们追踪用户活动数据的原因\n\n\n\n\n Office 365\n\n事件记录 \nOffice 365 的 Webmail 在电子邮件中显示用户的 IP 地址\n\n\n\n\n Opera\n\n事件记录\n第一次启动 Google Chrome 会发生什么？\n\nBrave 的 Jonathan Sampson 在 Twitter 上发表了一系列帖子，他在 Windows 机器上第一次安装了 Google Chrome、Microsoft Edge (Chromium) Beta、Opera 和 Vivaldi 、Dissenter、Brave 和 Firefox, 每个浏览器在安装之后都打开几分钟，期间他会对浏览器发出的请求进行抓包，对抓包结果进行一番分析。他发现 Brave 会发出 23 个请求，访问的都是 Brave.com 域名；Firefox 会发出 26 个请求，部分与 Google 的服务有关；Edge 会发出超过 130 个请求，有微软的还有 Google、Facebook 和 Twitter 的，Edge 在首次运行之后还收集了用户系统的详细信息；Opera 发出的请求有些特别，它有 19 个请求指向了俄罗斯的 yandex.ru, 还有亚马逊、ebay 和阿里巴巴，它还预加载了十多个第三方网站的 cookies, 它甚至已经开始与第三方共享用户信息，许多人声称这家中国公司拥有的挪威浏览器已经变成了间谍软件.\n\n\n\n\n\n\nRiot Games\n\n事件记录 \nRiot Games 热门新作《Valorant》安装了 rootkit 去防止作弊\n\n\n\n\n Skype\n\n事件记录\n当 Skype 翻译器功能处于活动状态时 微软承包商可以获知对话内容\n\n\n\n\n Steam\n\n相关工具 \nsteamid - 索引了个人资料过去使用的所有名称，除此之外，它还列出了可能的公众好友，为你提供了类似名称的列表，还有一些与 Steam 有关的其它工具。\n\n\n\n\nzoom\n\n事件记录 \nZoom CEO 称为配合 FBI 免费版不加密\nZoom 漏洞：超 50 万个 Zoom 账户泄露并在 Dark Web 出售\n\n\n\n\n百度云\n\n自动备份\n  说不定某时某人就给你开了个自动备份呢？\n  https://pan.baidu.com/disk/discovery\n  \n      \n  \n\n\n 滴滴\n\n查询历史行程\n  我没下过 APP 版的，在支付宝中的滴滴是可以查询历史行程的。点击头像–&gt; 订单–&gt; 查看历史行程\n  \n      \n  \n\n\n淘宝\n\n查看历史消费金额\n  淘宝搜索 : 淘宝人生 点右上角[成就]\n  淘宝搜索 : 看鬼故事\n\n\n\n\n\n社交网络\nfacebook\n\n事件记录\n\nfacebook 正在你下载的照片中嵌入跟踪数据\nFacebook Embeds ‘Hidden Codes’ To Track Who Sees And Shares Your Photos\nMessenger 发音频安全吗？FB 承认曾转录用户音频\n彭博：Facebook 雇人记录用户语音通话以改善 AI 技术\n知情人士透露称，Facebook 付费聘请几百名外部承包商，让他们转录音频片段，这些音频来自使用 Facebook 服务的用户.\n\n\n\n\n相关文章\n\nThe new Facebook Graph Search – part 1\nThe new Facebook Graph Search – part 2\nFacebook Tips\nThink Private Facebook Profiles Pages Are A Dead End? Think Again!\n\n\n 相关工具\n\nharismuneer/Ultimate-Facebook-Scraper - 一个抓取 facebook 用户信息的 python 脚本 \nHumanitarian Data Exchange - Facebook 的公开数据查询平台\n\n\n\n\n google\n\n事件记录\n\nGoogle accused of leaking personal data to thousands of advertisers\n执法部门找 Google 查用户信息需缴费，明码标价童叟无欺\n\n\n相关文章\n\n关于 Google ID 的 OSINT 信息挖掘\n\n\n相关工具\n\nGoogle Account Finder\nmxrch/GHunt - 用电子邮件调查谷歌账户\n\n\n\n\n Instagram\n\n相关文章\n\nFind an Instagram user ID - 检索 Instagram 用户 ID \n在最难搜索的地方：解剖 Instagram 用户\n\n\n相关工具\n\nsc1341/InstagramOSINT - 对 Instagram 帐户进行基本的信息收集的 python 脚本.\nDatalux/Osintgram\n\n\n 相关搜索\n\nInstagram Search Engine\nWebstagram - Instagram Search Account Instagram Web Viewer\nGramPages\nSkimagram - Search engine for Instagram\n\n\n\n\nLinkedIn\n\n相关文章\n\nA guide to searching LinkedIn by email address - 教你如何通过电子邮件地址搜索 LinkedIn 个人资料 \nOSINT, Part 3: Extracting Employee Names from Companies (Tesla and Breitbart) on LinkedIn - 提取 LinkedIn 中员工姓名和邮箱\n【技巧】利用谷歌搜索引擎和手机网页检测功能查看非好友领英网页\n\n\n相关工具\n\n0x09AL/raven - Linkedin 信息收集工具，渗透测试人员可以使用该工具收集有关使用 Linkedin 的组织员工的信息。\n\n\n相关搜索\n\nFree People Search Tool\nFREE LinkedIn Xray Search Tool\nLinkedIn X-Ray Search Tool | Sourcinglab\nTrevisan LinkedIn Boolean Search Generator\nLinkedIn X-Ray Search Tool\n\n\n\n\nQQ\n\n事件记录\n\nQQ 正在尝试读取你的浏览记录\n关于 QQ 读取 Chrome 历史记录的澄清\n如何看待 QQ 扫描读取所有浏览器的历史记录？\n腾讯官方声称扫描浏览器历史数据是防止恶意登陆\n\n\n查看历史头像\n  貌似只有 QQ 可以，TIM 不行\n  \n      \n  \n\n\ntelegram\n\n相关文章\n\nMODIFYING TELEGRAM’S “PEOPLE NEARBY” FEATURE TO PINPOINT PEOPLE’S HOMES - 利用 Telegram 的 “PEOPLE NEARBY” 功能 (技术上) 精确定位全球各地的人\n\n\n相关工具\n\npaulpierre/informer - 一个机器人库，可以让你在 telegram 上伪装成多个真实用户，并对每个账号 500 多个 telegram 频道进行监视。\nth3unkn0n/TeleGram-Scraper - 电报群组扫描器工具\n\n\n相关搜索\n\nCombot - 分析聊天情况，活跃情况.\ntele.me - 分析聊天情况，活跃情况.\ntgstat - 频道索引 \nStatistics and Telegram Tools - 发现 telegram 用户、群组、频道、机器人 \nTelegram channels online web catalog and bot for news reading - 发现 telegram 用户、群组、频道、机器人 \nTelegram channels rating - Telegram channels 排名 \n18000+ Telegram Channels, Groups, Bots and Stickers List - Discover Telegram Channels\nTelegram Group - Find Telegram Channels, Bots &amp; Groups\nSearch.buzz.im - 深度搜索 telegram 信息 \nTelegram Search. Search for posts - 整合搜索 \ngoq/telegram-list - 群组列表 \nTelegramic\nTelegroups\nTelegram Groups List\nBots for Telegram\nLyzem Search\n\n\nTips\n\nTelegram 账号的” 数字 id” 是注册时间越晚就越大吗？\n不是。如果多注册一些账号，可以发现有可能后注册的账号数字 id 是要小于先期注册的，因此通过数字 id 来判断一个账号是否为新号是没有依据的。出现这种现象，应该是由于旧账号注销后，该账号的数字 id 又被重新分配给新注册的账号.Telegram 官方客户端无法显示账号数字 id, 若想查询自己的账号数字 id 可以用过机器人 @getidsbot , 还有其他的机器人也有类似的功能，某些第三方客户端也可以显示账号的数字 id (请谨慎使用第三方客户端).\n\n\n\n\n\n\ntiktok\n\n相关文章\n\nTikTok OSINT: targeted user investigation (Part 1/3: User) - 针对 Tiktok 用户的的开源调查案例\n\n\n相关工具\n\nsc1341/TikTok-OSINT - 用于 tiktok 信息收集的开源工具集\n\n\n\n\n Twitter\n\n事件记录\n\nTwitter 承认未经允许将用户数据与广告商共享\n\n\n相关文章\n\n从推特中挖掘真相不需要太复杂的工具：一个常用工具的全面指南 - 介绍如何在 twitter 进行 osint 的教程 \nEmail to Twitter account - 通过邮箱找到 twitter 账户\n\n\n相关工具\n\ntwintproject/twint - 使用 Python 编写，抓取 Twitter 的 OSINT 工具.\nsowdust/tafferugli: Tafferugli is a Twitter Analysis Framework - 一个 web 应用程序形式的 Twitter 分析框架，能够过滤、收集和分析 tweet\n\n\n 相关搜索\n\ntinfoleak - 一个查 twitter 用户资料的工具 \nTweetBeaver - 帮助调查 twitter 账户的网站 \nTwitter Account Analytics by burrrd. - 分析 twitter 帐号的工具 \nTrendsmap - Twitter 主题标签，关键字或位置分析 \nHoaxy - twitter 分析工具，可视化文章连接和某条 twitter 被转载的次数 \nTwlets | Twitter to Excel - 下载任何人的推文，关注者，喜欢的视频到 excel 中 \nTwitter Search Engine\nTwitterfall - 瀑布流版本 Twitter\nTwitter Shadowban Test - 检测指定账号状态 \nTwitter Search Tool - Search For Tweets - 搜索 \nTwitter Search Engine - 搜索 \nTweetDeck - 在一个简单的界面中查看多个时间轴，从而提供了更便捷的 Twitter 体验。\nTwiMap - Explore Twitter on the Map - 查看附近用户的推文 \nOmniSci Tweetmap - 查看附近用户的推文 \nonemilliontweetmap - 查看附近用户的推文\n\n\n\n\n whatapp\n\n相关工具 \nLoranKloeze/WhatsAllApp - 通过手机号查询 whatapp 的注册信息\n\n\n\n\n YouTube\n\n相关工具 \nYouTube DataViewer - 显示 YouTube 上任意视频的所有可用元数据。\nanvaka/yasiv-youtube - 寻找相似视频关系 \nGeo Search Tool - 按地理位置寻找相关视频的工具 \nExtract Meta Data - 从 YouTube 的视频中提取隐藏数据 \nLocation Search - Discover Geo-tagged Videos - YouTube Geofind - 查看附近用户的视频 \nGeo Search Tool - 查看指定地址范围用户上传的视频 \nThe YouTube Channel Crawler - YouTube 频道爬虫 \nTagsYouTube - Youtube 视频标签生成器和关键字在线搜索\n\n\n\n\n网易云\n\n链接指纹\n  试着分享一个音乐 https://music.163.com/#/song/1346907833/?userid=48353\n  注意一下 userid 变量，构造一下链接: https://music.163.com/#/user/home?id=&lt;!userid!&gt;\n  https://music.163.com/#/user/home?id=48353\n\n历史评论\n  ios 端、安卓端通用，账号–&gt; 关于我–&gt; 我的评论\n\n\n\n微信\n\n事件记录\n\nHow unwitting users of WeChat aid the Chinese messaging app’s blacklisting of sensitive images\n\n阿里巴巴旗下的南华早报引用加拿大多伦多大学公民实验室的报告报道，腾讯的微信利用实时和追溯分析的方法审查用户的图片。报告发现，微信对用户对话中发送的图片进行实时自动检测和审查，审查是基于图片中包含的文字以及目标图片与系统数据库中的敏感图片的相似度匹配；微信通过建立哈希索引 (Hash Index) 实现过滤，该哈希索引由微信用户在聊天对话中发送的图像的 MD5 值组成；对比微信朋友圈，一对一聊天以及群组聊天的图片审查比例，发现这三项功能的敏感图片库并不相同，其中朋友圈和群组聊天所审查的范围要远大于一对一聊天；与关键词审查一样，微信图片审查与新闻事件相关.\n\n\n\n\n链接指纹  阅读下面这个文章大致了解一下微信链接组成\n\n微信公众号文章 URL 的种类与结构\n\n  那么类似 https://mp.weixin.qq.com/s?__biz=MzIyAAANzY0OA==&amp;mid=101111431&amp;idx=1&amp;sn=62accd1299d25d54d1f3ad3f3d7d214&amp;chksm=683d2e402f6sa2dsa2d154058807d1xxxx151213131dasdasdsadasd675ce59fae94ff9908&amp;scene=18&amp;xtrack=1&amp;key=917D458AS46D146SD14AF541DSA4FDSAF131DS31F31DSA31FDSAde153285841fdc398a67d61be441cb0e1898a08232811308bf31dfc92757c3d7d5e3SD54AD1SA1D351S3A1D31S3AD034f1cb34170ecd27b6d7d69&amp;ascene=1&amp;uin=MTk3ODkwODMxMA%3D%3D&amp;devicetype=Windows+7&amp;version=62055833&amp;lang=zh_CN&amp;pass_ticket=r6jSAD55SAF458F61A4S56F51BW2hfIQPocX2O0er0vUheGSD45ASD11DASD361SADAWDbiqW 这么一串可以携带多少信息\n\n\n\n支付宝\n\n支 fu 宝可以查婚姻状况望周知\n\n\n\n\n各类搜索下方所有搜索引擎不保证其安全性、隐私性,仅保证其功能性\n常用搜索引擎\nhttps://www.ask.com/\nhttps://start.duckduckgo.com/\nhttps://www.ecosia.org/\nhttps://www.google.com/\nhttps://www.qwant.com/\nhttps://searx.me/\nhttps://www.startpage.com/\nhttps://yandex.com/\nhttp://search.chongbuluo.com/\nhttps://magi.com/\nhttps://www.onesearch.com/\n\n网页快照\n网页快照网 - 搜索引擎网页快照查询，支持手机移动端 \nInternet Archive: Digital Library of Free &amp; Borrowable Books, Movies, Music &amp; Wayback Machine - 互联网档案馆是一个非营利性的数字图书馆组织。提供数字数据如网站、音乐、动态图像、和数百万书籍的永久性免费存储及获取。\nCachedView\nCachedViews\nPage Cached\nGoogle Cache Browser 3.0\nCached websites check from Google webcache and Archive org\nArquivo.pt: pesquise páginas do passado!\n\n图片反向搜索\nGoogle 图片\nJeffrey Friedl’s Image Metadata Viewer\nTinEye Reverse Image Search\n百度图片\nGoogle Art &amp; Culture Experiment - Art Palette\nYandex.Images\nAliseeks - 支持在 AliExpress 或 eBay 列表中对产品进行反向图像搜索 \nGoogle Reverse Image Search for Mobile\n\nacg 图片反向搜索\nMulti-service image search - 多服务反向图像搜索 \nSauceNAO Image Search - 反向图像搜索引擎，搜 pixiv 效果极佳\n二次元画像詳細検索 - 专搜二次元图片 \nWAIT: What Anime Is This? - 动画片段搜索引擎，可以帮助用户通过截图追溯原著动漫\n\n航班 / 飞机信息\nFlight Tracker | Flightradar24 | Track Planes In Real-Time\nFlightradar24 — how it works? / Habr - 一篇介绍网站如何运作的文章 \n\n\nFlightAware - 航班跟踪 / 航班状态 / 飞行跟踪 \nreal-time flight tracking | Flightadsb | VariFlight\nDirect Flights | Explore all non-stop flights from any airport\nAirNav RadarBox - Live Flight Tracker and Airport Status\nADS-B Exchange - tracking 2534 aircraft\nFLIGHTVIEW FLIGHT TRACKER\nPlane Flight Tracker\nFlightStats - Global Flight Status &amp; Tracker, Airport Weather and Delays\niFly.com - Flight Status | Track Flights\nFAA Registry - Aircraft - N-Number Inquiry - 搜索在美国联邦航空管理局（FAA）注册的所有飞机的登记册。\nVirtual Radar\n\n船舶信息\nMarineTraffic: Global Ship Tracking Intelligence | AIS Marine Traffic\nFree AIS Ship Tracking of Marine Traffic - VesselFinder\nMy Ship Tracking Free Realtime AIS Vessel Tracking Vessels Finder Map\nShipTracker - 船舶动态查询_AIS 船位_船舶跟踪_船舶定位_船舶位置查询 \nGlobal Container Shipping Platform | Container Tracking, Ocean Schedules\nMarine Vessel Traffic\nLive AIS Ships Map!—shipfinder\n船员证书查询\n船讯网 - 船舶动态、船舶档案、AIS 船位、货物跟踪、租船、OP、航运大数据\n海管家 - 船舶动态查询；船舶定位；船舶跟踪 \nLive AIS Vessel Tracker with Ship and Port Database\n中国港口\n国家水上交通信息服务平台\n船问网 - 船舶档案，船舶在线揽货交易，运费托盘全程垫付\n青岛港区 货物跟踪 - i 跟踪\n云当网 - 物流可视化 - 船舶轨迹定位 - 海运跟踪 - 空运货物跟踪 - 码头 - 集装箱进港查询\nGlobal Fishing Watch - 显示商业渔船的位置\n\n相关文章\n\nOSINT on the Ocean: Maritime Intelligence Gathering Techniques\n\n货车位置\n货车定位，集卡跟踪 - 海管家\n货车位置、货车定位软件\n货车位置实时查询\n\n物流信息\n快递 100 - 查快递\n中华人民共和国国家邮政局\n快递查询\n全球物流查询平台\n\n车辆信息\nПроверка авто по гос номеру - Поиск машины бесплатно онлайн - Номерограм - 车牌号搜索引擎 (仅限俄罗斯，不是你想得那种车牌)\nVINCheck® | National Insurance Crime Bureau - 协助确定车辆是否被报案为失窃但未被追回，或被 NICB 成员保险公司报案为残余车辆。\n\nVIN 码\nVIN 码是英文 (Vehicle Identification Number) 的缩写，VIN 码是表明车辆身份的代码。VIN 码由 17 位字符（包括英文字母和数字）组成。是制造厂为了识别而给一辆车指定的一组字码。该号码的生成有着特定的规律，对应于每一辆车，并能保证五十年内在全世界范围内不重复出现。因此又有人将其称为” 汽车身份证”。车辆识别代号中含有车辆的制造厂家、生产年代、车型、车身型式、发动机以及其它装备的信息。\n\n\n中国汽车网 - VIN 车辆识别代码查询\n宜配网 VIN 查询\n奉新行 车辆识别码 (VIN) 查询\n17VIN 17 位车架号查询\n车信会 VIN 查询\n力洋汽车信息查询\n搜配网 - VIN 码识别_车架号识别_专业汽车配件数据库_车型配件精准查询\n聚合数据 - VIN 码查询数据接口_免费 API 接口调用\n极速数据 - VIN 车辆识别代码查询 API 接口_免费数据接口\n易源数据 - 车架号 VIN 查询车辆信息\nFree VIN Code Search Service - 车主姓名，地址，电话号码和汽车注册状态等信息 \nVehicle History Reports - 车辆 VIN 码查询\n\n个人可信度\n个人信用查询搜索_企业信息查询搜索_统一社会信用代码查询 - 信用中国\n统一社会信用代码查询_诚信体系实名制查询_组织机构代码 - 全国组织机构统一社会信用代码数据服务中心 (原全国组织机构代码管理中心)\n中国执行信息公开网\n中国人民银行征信中心\n风险信息网 - 可查询个人和企业工商信息以及法院判决、税务、海关、市场监管等各类关联信息。并且支持批量监控，并有短信通知功能。\n查企业工商_诉讼案件_失信被执行人_对外投资_催收公告信息_风险预警网 - 被列入失信执行人的名单将在网站上展示。\n物业费催收 | 互联网催收平台 | 贷后催收系统 | 债务案源 - 催天下 - 可以查询被催收人信息\n汇法网 - 网上法务平台：找律师、裁判文书、法律法规、合同、法律新闻 - 提供法律法规及裁判文书查询\n\n网络空间测绘\nShodan - 网络空间安全搜索引擎 \nBinaryEdge - 网络空间安全搜索引擎，瑞士 Shodan\nFOFA Pro - 网络空间安全搜索引擎，国产 Shodan\nZoomEye - 网络空间安全搜索引擎，国产 Shodan\nCensys - 搜索 IP 地址、设备、网站和证书配置、部署信息的搜索引擎 \nSpyse - SSL 证书搜索引擎 \nsearchcode - 开源代码搜索引擎\n知风 - 互联网联网工控资产搜索引擎\n\ntor 信息\nOnion Search Engine\nDarkSearch - Dark Web search engine\nkilos\nDargle\nGenesis Search\nBullmask\nOnion Search Engine\nOnionLand Search\nAhmia\nhaystak\nTor2Web\nGenesis Search\n\n\n\nTorBot - Open Source Intelligence Tool for the Dark Web - 用于暗网的开源情报工具\n\n学术信息\nlibgen - 有关书籍、插画、文章的搜索引擎 \nSemantic Scholar - 科学文章的学术搜索引擎\n远见搜索 - 知网提供的搜索引擎 \nLibrary Genesis - 創世紀圖書館是科學論文及書籍的搜尋引擎，可以免費提供被擋在付費牆後的內容。\nWolfram|Alpha\n\n专利 - 商标\n佰腾网\nDawei Innojoy Patent Search Engine\nSooPAT 专利搜索\n润桐 RainPat 专利检索\n专利信息服务平台\n权查查 - 商标查询 - 商标注册 - 商标监控 - 商标品牌保护 - 知识产权服务平台\n\n报刊信息\nHeadlineSpot - 世界新闻头条 \nrefdesk - 全球报纸 \nNewspapers List - 聚集全球各地的报纸（在线版）\nNews Conc - 全球报纸\n\n开放数据集\nWorld Bank Open Data - 免费并公开获取世界各国的发展数据 \nDatabasd - 开放数据集的搜索引擎 \nICIJ Offshore Leaks Database - OFFSHORE LEAKS DATABASE\nQResear.ch - 该网站收录了很多小众话题、板块和文章，包含了从人口贩卖到白宫访客，从 8Chan 到 Epstein 的黑名单等等。\njudyrecords - 可对来自美国的 3.6 亿多个逮捕记录和法院文件进行索引 \nBoardreader - 搜索全球各个论坛平台的内容\n\nBGP 信息\nBGP Update Reports\nCollectors – Routeviews\n\n电子硬件相关\nSearch FCC ID Database - 通过 FCC ID、CMIIT ID 或 KCC MSIP 搜索。\nBIOS Master Password Generator for Laptops - 笔记本电脑的 BIOS 密码恢复\n无线电设备查询\n行政许可结果公开系统 - 电信设备进网许可证查询\n\n社交 - 人际关系\nGolgozar - 社交搜索引擎 \nGenes Reunited - 通过族谱、家庭故事寻找家人的网站 \nUK Birth Adoption Register - 英国出生收养登记册 \nSocial Searcher - 社交媒体搜索引擎 \nBuzzglobe.com - 社交媒体搜索引擎 \nEnginuity Social Search - 社交媒体搜索引擎 \nGoogle Social Search - 社交媒体搜索引擎 \nFindwith.me - 社交媒体搜索引擎 \nAnymail finder - 输入人名和公司名称，查找任何人的 email 地址 \nLittleSis\n\n企业信息\n企查查 - 工商信息查询_公司企业注册信息查询_全国企业信用信息公示系统\n国家企业信用信息公示系统\n天眼查 - 企业信息调查工具_企业信息查询_公司查询_工商查询_信用查询平台\n启信宝 - 企业注册信息查询 | 企业工商信息查询 | 企业信用信息查询平台\n企业信用信息查询\n悉知 - 企业信息查询\n信用视界 - 企业信息查询_公司查询_企业信用信息查询_企业工商信息查询_企业注册信息查询_工商登记信息查询\n中国海关企业进出口信用信息公示平台\n看准网 - 查工资 | 聊面试 | 评公司 | 搜职位\n职友集\nCrunchbase: Discover innovative companies and the people behind them\nCorporation Wiki\nGlobal B2B Online Directory\nManta\nOpenCorporates - 世界上最大的企业开放数据库 \nbrownbook\nSpokeo\nBiznar\nNorth Data Smarte Recherche - 德国公司注册和公告（付费）信息 \nCompanies House service - 英国公司信息 \nOpenGazettes - 欧洲商业活动的情报 \nEnigma\nSEC.gov | Company Search Page - 证券交易委员会文件的数据库\n\n供应商\n\nThomasnet®\n\n博客搜索\nBest of the Web Blog Directory\nBlog Directory - BlogDire\nBlog Directory - Submit Your Blog to the Blogville Directory\nBlog Directory, Submit your blogs today, Blog directories search engine\nBlog Flux\nBlog Search : Blog Search Engine Directory\nBlog Top Sites - Directory of the Best Blog Sites\nBlogarama - Blog directory\nBlogs Directory - Blogs-Collection.com\nSocial Network, Blog Directory, Blog Search Engine, Free Blog Hosting\nBlog Search\nBlogging Fusion - Blog Directory - Article Directory - RSS Directory - Web Directory\nJustia Blawg Search - Law Blogs, Lawyer Blogs, Legal Blogs Directory &amp; Search Engine\nBlogspot Blog Search\nPrepare for Meetings - Selling Intel Search Engine\n\n\n文件资源PDF\nPDFSEARCH.IO - Document Search Engine\nPDF search engine for free scientific publications - FreeFullPDF\nDocuments Free Download PDF\nPDF Books for Download\nPDF Search Engine, free search, PDF download\nPDF Search engine | Find public PDF documents\nPDF Search Engine - Free download PDF files\nPDF Drive - Search and download PDF files for free.\nFree PDF Search Engine\n\n音乐\nmidomi\n\n\n天气 - 环境\nAntiweather - 寻找对蹠点 (位于地球直径两端的点) \n亚洲空气污染 - 实时空气质量指数地图 \nearth - 风、天气和海洋状况全球地图 \nWindy - 风向图和天气预报\n台风路径实时发布系统\nLight pollution map - 光污染地图 \nPurpleAir - 实时空气质量监测\n\n\n地图\n天地图 - 国家测绘地理信息局建设的地理信息综合服务网站\n高德地图\n百度地图\nBing 地图\nGoogle Maps\nGoogle Earth\nGoogle Earth Timelapse - 查看数十年来世界变化的地图 \n\n\nOpenStreetMap - OpenStreetMap 是由一个地图绘制者社区建立的，他们提供并维护世界各地的道路、小径、咖啡馆、火车站等数据。\nWaze\n\n数据展示\nHE 3D Network Map - 3D 版海底电缆地图 \nJapan Night Life - uMap - 日本夜遊地圖 \nEarthExplorer - 在线访问美国遥感数据 \nSubmarine Cable Map - 海底电缆地图 \nCarbon Brief - 世界上的核电站\n发现中国\n俄罗斯军事基地的位置\nSoar - 提供卫星，航空和无人机图像 \nMineral Resources Data System: US - 美国地质调查局 (USGS) 提供全球矿产资源的历史数据，包括矿山所有权的信息\n\n网络威胁偷偷告诉你,这里面,好几个,都是假的\n\nFortinet Threat Map - Fortinet 威胁地图 \nLive Cyber Attack Threat Map - checkpoint 网络威胁地图 \nFireEye Cyber Threat Map - FireEye 网络威胁地图\n可视化全球互联网性能 - Akamai 提供的互联网监控器 \nKaspersky Cyberthreat real-time map - Kaspersky 网络威胁实时地图 \nDigital Attack Map - 展示全球每天最多的 DDoS 攻击 (巨卡无比！！！)\n\n","categories":["个人笔记"],"tags":["Privacy"]},{"title":"常用软件集合","url":"/p/37491/","content":"常用软件工具收藏集，收藏了在工作生活中遇到的好用实用的软件。\n开发工具\nBeyondCompare 破解版\nNavicat Premium 12 破解版\nmarkdown pad2 破解版 密码：23w2\nRedisDesktopManager 免费版 密码:ciq1\nQTransate 翻译工具\n正则表达式测试工具\ndraw.io 拓扑图工具\nDevCenter cassandra 管理工具\nPostMan 便携版\nGit\nSourceTree (Git Gui 工具)\nXShell5 破解版\n\n实用工具\nRSS 订阅工具 (只限 windows)\n科学上网 ShadowSocks\nwin10 激活工具\nOffice 安装工具\nOCR 文字识别工具\nGIF 录制工具\n冰点文库下载器破解版\n\n推荐工具\n现代化 Markdown 编辑工具\n\n","categories":["个人笔记"],"tags":["软件收藏"]},{"title":"手写 IOC 容器","url":"/p/11305/","content":"前言本文是为了学习 Spring IOC 容器的执行过程而写，不能完全代表 Spring IOC 容器，只是简单实现了容器的依赖注入和控制反转功能，无法用于生产，只能说对理解 Spring 容器能够起到一定的作用。\n\n\n开始创建项目创建 Gradle 项目，并修改 build.gradle\nplugins {    id 'java'    id \"io.franzbecker.gradle-lombok\" version \"3.1.0\"}group 'io.github.gcdd1993'version '1.0-SNAPSHOT'sourceCompatibility = 1.8repositories {    mavenCentral()}dependencies {    testCompile group: 'junit', name: 'junit', version: '4.12'}\n\n创建 BeanFactoryBeanFactory 是 IOC 中用于存放 bean 实例以及获取 bean 的核心接口，它的核心方法是 getBean 以及 getBean 的重载方法，这里简单实现两个 getBean 的方法。\npackage io.github.gcdd1993.ioc.bean;/** * bean factory interface * * @author gaochen * @date 2019/6/2 */public interface BeanFactory {    /**     * 通过bean名称获取bean     *     * @param name bean名称     * @return bean     */    Object getBean(String name);    /**     * 通过bean类型获取bean     *     * @param tClass bean类型     * @param &lt;T&gt;    泛型T     * @return bean     */    &lt;T&gt; T getBean(Class&lt;T&gt; tClass);}\n\n创建 ApplicationContext 上下文ApplicationContext，即我们常说的应用上下文，实际就是 Spring 容器本身了。\n我们创建 ApplicationContext 类，并实现 BeanFactory 接口。\npublic class ApplicationContext implements BeanFactory {}\n\ngetBean 方法既然说是容器，那肯定要有地方装我们的 bean 实例吧，使用两个 Map 作为容器。\n/** * 按照beanName分组 */private final Map&lt;String, Object&gt; beanByNameMap = new ConcurrentHashMap&lt;&gt;(256);/** * 按照beanClass分组 */private final Map&lt;Class&lt;?&gt;, Object&gt; beanByClassMap = new ConcurrentHashMap&lt;&gt;(256);\n\n然后，我们可以先完成我们的 getBean 方法。\n@Overridepublic Object getBean(String name) {    return beanByNameMap.get(name);}@Overridepublic &lt;T&gt; T getBean(Class&lt;T&gt; tClass) {    return tClass.cast(beanByClassMap.get(tClass));}\n\n直接从 Map 中获取 bean 实例，是不是很简单？当然了，在真实的 Spring 容器中，是不会这么简单啦，不过我们这次是要化繁为简，理解 IOC 容器。\n构造器Spring 提供了 @ComponentScan 来扫描包下的 Component，我们为了简便，直接在构造器中指定要扫描的包。\nprivate final Set&lt;String&gt; basePackages;/** * 默认构造器，默认扫描当前所在包 */public ApplicationContext() {    this(new HashSet&lt;&gt;(Collections.singletonList(ApplicationContext.class.getPackage().getName())));}/** * 全参构造器 * @param basePackages 扫描的包名列表 */public ApplicationContext(Set&lt;String&gt; basePackages) {    this.basePackages = basePackages;}\n\nrefresh 方法refresh 的过程基本按照以下流程来走\n\n\n扫描指定的包下所有带 @Bean 注解（Spring 中是 @Component 注解）的类。\n\nList&lt;Class&gt; beanClasses = PackageScanner.findClassesWithAnnotation(packageName, Bean.class);System.out.println(\"scan classes with Bean annotation : \" + beanClasses.toString());for (Class beanClass : beanClasses) {    try {        createBean(beanClass);    } catch (ClassNotFoundException | NoSuchMethodException | IllegalAccessException | InvocationTargetException | InstantiationException e) {        e.printStackTrace();    }}\n\n\n遍历类，获取类的构造器以及所有字段。\n\nConstructor constructor = beanClass.getDeclaredConstructor();Object object = constructor.newInstance();Field[] fields = beanClass.getDeclaredFields();\n\n\n判断字段是依赖注入的还是普通字段。\n\n如果是普通字段，通过字段类型初始化该字段，并尝试从 @Value 注解获取值塞给字段。\n\n\nValue value = field.getAnnotation(Value.class);if (value != null) {    // 注入    field.setAccessible(true);    // 需要做一些类型转换，从String转为对应的类型    field.set(object, value.value());}\n\n\n如果是依赖注入的字段，尝试从 beanByClassMap 中获取对应的实例，如果没有，就先要去实例化该字段对应的类型。\n\nAutowired autowired = field.getAnnotation(Autowired.class);if (autowired != null) {    // 依赖注入    String name = autowired.name();    // 按照名称注入    Object diObj;    if (!name.isEmpty()) {        diObj = beanByNameMap.get(name) == null ?                createBean(name) :                beanByNameMap.get(name);    } else {        // 按照类型注入        Class&lt;?&gt; aClass = field.getType();        diObj = beanByClassMap.get(aClass) == null ?                createBean(aClass) :                beanByClassMap.get(aClass);    }    // 注入    field.setAccessible(true);    field.set(object, diObj);}\n\n测试我们的 IOC 容器创建 Address\n@Data@Beanpublic class Address {    @Value(\"2222\")    private String longitude;    @Value(\"1111\")    private String latitude;}\n\n创建 Person 并注入 Address\n@Data@Beanpublic class Person {    @Autowired    private Address address;    @Value(\"gaochen\")    private String name;    @Value(\"27\")    private String age;}\n\n创建测试类 ApplicationContextTest\npublic class ApplicationContextTest {    @Test    public void refresh() {        Set&lt;String&gt; basePackages = new HashSet&lt;&gt;(1);        basePackages.add(\"io.github.gcdd1993.ioc\");        ApplicationContext ctx = new ApplicationContext(basePackages);        ctx.refresh();        Person person = ctx.getBean(Person.class);        System.out.println(person);        Object person1 = ctx.getBean(\"Person\");        System.out.println(person1);    }}\n\n控制台将会输出：\nscan classes with Bean annotation : [class io.github.gcdd1993.ioc.util.Address, class io.github.gcdd1993.ioc.util.Person]scan classes with Bean annotation : [class io.github.gcdd1993.ioc.util.Address, class io.github.gcdd1993.ioc.util.Person]Person(address=Address(longitude=2222, latitude=1111), name=gaochen, age=27)Person(address=Address(longitude=2222, latitude=1111), name=gaochen, age=27)\n\n可以看到，我们成功将 Address 实例注入到了 Person 实例中，并且将它们存储在了我们自己的 IOC 容器中。其实，Spring 容器的原理大致就是如此，只不过为了应对企业级开发，提供了很多便捷的功能，例如 bean 的作用域、bean 的自定义方法等等。\n获取源码完整源码可以在我的 github 仓库获取👉Simple-IOC-Container\n","categories":["个人笔记"],"tags":["Spring"]},{"title":"我的 Java 环境搭建","url":"/p/12106/","content":"每次重装系统后的开发环境搭建，总是会花费大量的时间精力，软件下载安装啦，配置修改啦等等，索性把这些流程记录一下，毕竟时间就是金钱。\n软件列表\nJDK1.8\nIntelliJ IDEA\nNavicat 数据库管理工具\n Postman\nGit\nSourceTree\nXShell5\nDevCenter (cassandra 数据库管理工具)\nRedisDesktopManager (redis 管理工具)\n\n这些工具已经可以满足我的日常工作了，什么印象笔记，markdownPad2 等等不包含于此。这些都可以在我的博客下载到 -&gt; 常用软件集合\n软件配置IntelliJ IDEA破解IntelliJ IDEA 注册码\n主题当然是选黑色主题了，毕竟提倡保护眼睛。\n\n字体我一般使用默认字体 + 14 号大小。\n\n设置编辑器的快捷键，也就是 keymap由于以前用惯了 Eclipse, 所以还得改为 Eclipse 的快捷键\n\n代码自动提示不区分大小写这个比较重要，毕竟谁也不可能无时无刻注意大小写，到时候不快捷提示就浪费太多时间了，也影响开发体验。\n\n自动导入包和导入包优化的设置\nJava 代码默认注释一般创建一个 java 类的时候，需要指定创建者以及创建时间\n\n注释代码可以自己决定，这里举个例子:\n/** * @author gaochen * @date ${DATE} */\n\n 然后创建的类是这样的:\n /** * @author gaochen * @date 2019/1/31 */\n\nIntelliJ IDEA 启动设置不默认打开前一个项目 \nIntelliJ IDEA 常用插件\n.ignore: 自动生成.ignore 文件，并支持一键添加文件到.ignore 列表\n Grep Console: 在控制台支持筛选，类似 shell 命令的 cat 1.txt | grep ‘11’\nLombok plugin: 使用 lombok 必须要装的一个插件\n CodeGlance: 代码编辑区迷你缩放图插件，非常好用\n HighlightBracketPair: 自动化高亮显示光标所在代码块对应的括号，可以定制颜色和形状，再也不怕看代码看到眼花了\n Rainbow Brackets: 彩色显示所有括号，有点类似上一个\n Alibaba Java Coding Guidelines: 阿里巴巴 Java 开发手册配套插件，一键扫描帮你优化代码。\nCodota：让代码提示更加智能（只支持 Java）\n\nNavicat破解Navicat Premium 12.0.18 / 12.0.24 安装与激活\n参考资料\nIntellij IDEA 插件推荐\n\n","categories":["个人笔记"],"tags":["Java"]},{"title":"掘金解绑方案（已成功解绑微信）","url":"/p/34461/","content":"掘金的绑定限制为同一个第三方账号只能绑定一个掘金账号，且必须留存一个第三方绑定。\n比如，我只绑定了微信，想要解绑微信，对不起，不支持。\n\n\n\n解决方案我突然想到，既然必须留存一个第三方绑定，那我留存一个邮箱绑定不就好了吗？\n思路就是临时电子邮件地址，通过绑定一个邮箱账号，来绑定邮箱，从而将我们的账号解放出来，绑定我们的大号或者其他账号。\n获取临时电子邮件地址打开临时电子邮件地址，你将会获取到一个临时电子邮件地址，我们将使用这个邮件绑定我们的掘金账号。\n\n绑定临时邮箱点击绑定邮箱，输入我们上一步获取到的临时邮件地址，返回临时电子邮件地址网站，耐心等待 10s 左右。\n\n你将会受到掘金的邮箱绑定验证邮件，打开并点击，直到绑定成功。\n\n解绑接下来我们就可以开心的解绑我们自己的账号了。我要解绑的是微信，试一下吧。\n\n友情提醒\n由于使用的是一次性邮件地址，该做法可能会导致你解绑的账号登录不上，请谨慎操作！\n\n","categories":["个人笔记"],"tags":["技巧"]},{"title":"消息队列（一）简介","url":"/p/27791/","content":"消息队列 (MQ) 概述消息队列（Message Queue），是分布式系统中重要的组件，其通用的使用场景可以简单地描述为：\n\n当不需要立即获得结果，但是并发量又需要进行控制的时候，差不多就是需要使用消息队列的时候。\n\n消息队列主要解决了应用耦合、异步处理、流量削锋等问题。\n\n\n当前使用较多的消息队列有 RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、MetaMq 等，而部分数据库如 Redis、Mysql 以及 phxsql 也可实现消息队列的功能。\n消息队列使用场景消息队列在实际应用中包括如下四个场景：\n\n应用耦合：多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败；\n异步处理：多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间；\n限流削峰：广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况；\n消息驱动的系统：系统分为消息队列、消息生产者、消息消费者，生产者负责产生消息，消费者 (可能有多个) 负责对消息进行处理；\n\n下面详细介绍上述四个场景以及消息队列如何在上述四个场景中使用：\n异步处理具体场景：用户为了使用某个应用，进行注册，系统需要发送注册邮件并验证短信。对这两个操作的处理方式有两种：串行及并行。\n串行方式新注册信息生成后，先发送注册邮件，再发送验证短信；\n\n在这种方式下，需要最终发送验证短信后再返回给客户端。\n并行处理新注册信息写入后，由发短信和发邮件并行处理；\n\n在这种方式下，发短信和发邮件 需处理完成后再返回给客户端。\n假设以上三个子系统处理的时间均为 50ms，且不考虑网络延迟，则总的处理时间：\n\n串行：50+50+50=150ms  并行：50+50 = 100ms\n\n使用消息队列\n并在写入消息队列后立即返回成功给客户端，则总的响应时间依赖于写入消息队列的时间，而写入消息队列的时间本身是可以很快的，基本可以忽略不计，因此总的处理时间相比串行提高了 2 倍，相比并行提高了一倍；\n应用耦合具体场景：用户使用 QQ 相册上传一张图片，人脸识别系统会对该图片进行人脸识别，一般的做法是，服务器接收到图片后，图片上传系统立即调用人脸识别系统，调用完成后再返回成功，如下图所示：\n\n该方法有如下缺点：\n\n人脸识别系统被调失败，导致图片上传失败；\n延迟高，需要人脸识别系统处理完成后，再返回给客户端，即使用户并不需要立即知道结果；\n图片上传系统与人脸识别系统之间互相调用，需要做耦合；\n\n若使用消息队列：\n\n客户端上传图片后，图片上传系统将图片信息如 uin、批次写入消息队列，直接返回成功；而人脸识别系统则定时从消息队列中取数据，完成对新增图片的识别。 \n此时图片上传系统并不需要关心人脸识别系统是否对这些图片信息的处理、以及何时对这些图片信息进行处理。事实上，由于用户并不需要立即知道人脸识别结果，人脸识别系统可以选择不同的调度策略，按照闲时、忙时、正常时间，对队列中的图片信息进行处理。\n限流削峰具体场景：购物网站开展秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统可以从消息队列中取数据，相当于消息队列做了一次缓冲。\n\n该方法有如下优点： \n\n请求先入消息队列，而不是由业务处理系统直接处理，做了一次缓冲，极大地减少了业务处理系统的压力； \n队列长度可以做限制，事实上，秒杀时，后入队列的用户无法秒杀到商品，这些请求可以直接被抛弃，返回活动已结束或商品已售完信息；\n\n消息驱动的系统具体场景：用户新上传了一批照片， 人脸识别系统需要对这个用户的所有照片进行聚类，聚类完成后由对账系统重新生成用户的人脸索引 (加快查询)。这三个子系统间由消息队列连接起来，前一个阶段的处理结果放入队列中，后一个阶段从队列中获取消息继续处理。\n\n该方法有如下优点：\n\n避免了直接调用下一个系统导致当前系统失败；\n每个子系统对于消息的处理方式可以更为灵活，可以选择收到消息时就处理，可以选择定时处理，也可以划分时间段按不同处理速度处理；\n\n消息队列的两种模式消息队列包括两种模式，点对点模式（point to point， queue）和发布 / 订阅模式（publish/subscribe，topic）。\n点对点模式\n消息队列\n发送者 (生产者)\n 接收者（消费者）\n\n\n消息发送者生产消息发送到 queue 中，然后消息接收者从 queue 中取出并且消费消息。消息被消费以后，queue 中不再有存储，所以消息接收者不可能消费到已经被消费的消息。\n点对点模式特点：\n\n每个消息只有一个接收者（Consumer）(即一旦被消费，消息就不再在消息队列中)；\n发送者和接收者间没有依赖性，发送者发送消息之后，不管有没有接收者在运行，都不会影响到发送者下次发送消息；\n接收者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接收的消息；\n\n发布 / 订阅模式发布 / 订阅模式下包括三个角色：\n\n角色主题（Topic）\n发布者 (Publisher)\n 订阅者 (Subscriber)\n\n\n发布者将消息发送到 Topic, 系统将这些消息传递给多个订阅者。\n发布 / 订阅模式特点：\n\n每个消息可以有多个订阅者；\n发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息；\n为了消费消息，订阅者需要提前订阅该角色主题，并保持在线运行；\n\n常用消息队列\nRabbitMQ\nActiveMQ\nRocketMQ\nApache Kafka\n\n","categories":["个人笔记"],"tags":["消息中间件"]},{"title":"消息队列（三）Apache ActiveMQ","url":"/p/32495/","content":"在 Ubuntu 上安装 ActiveMQ系统初始化$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean\n\n\n\n搭建 activemq 服务$ mkdir /home/active-mq$ cd /home/active-mq$ wget http://www.apache.org/dist/activemq/5.15.9/apache-activemq-5.15.9-bin.tar.gz# 具体版本请查看http://www.apache.org/dist/activemq$ tar -zxvf apache-activemq-5.15.9-bin.tar.gz# 如果未安装jdk，执行 sudo apt-get install openjdk-8-jdk$ ./activemq startINFO: Loading '/home/active-mq/apache-activemq-5.15.9//bin/env'INFO: Using java '/usr/bin/java'INFO: Starting - inspect logfiles specified in logging.properties and log4j.properties to get detailsINFO: pidfile created : '/home/active-mq/apache-activemq-5.15.9//data/activemq.pid' (pid '6356')\n\n监控浏览器打开 http://localhost:8161/admin/，输入 admin，admin\n\n至此，ActiveMQ 搭建完成。\n理解 JMS (Java Message Service)Java 消息服务指的是两个应用程序之间进行异步通信的 API，它为标准消息协议和消息服务提供了一组通用接口，包括创建、发送、读取消息等，用于支持 JAVA 应用程序开发。\nJMS 模型\n点对点（P2P）或队列模型\n\n只有一个消费者将获得消息\n生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。\n每一个成功处理的消息都由接收者签收\n\n\n发布 / 订阅模型\n\n多个消费者可以获得消息\n在发布者和订阅者之间存在时间依赖性。发布者需要创建一个订阅（subscription），以便客户能够购订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者创建了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。\n\n\n\n传统 API传统 API 提供的主要接口如下：\n\nConnectionFactory：客户端用来创建连接的受管对象。简化 API 也会使用此接口。\n\nConnection：客户端到 JMS 提供者之间的活动连接。\n\nSession：发送和接收消息的一个单线程上下文。\n\nMessageProducer：由 Session 创建的对象，用于发送消息到 Queue 或 Topic\n\nMessageConsumer：由 Session 创建的对象，用于接收 Queue 或 Topic 中的消息\n\n\n\n简化 API简化 API 与传统 API 提供的消息功能是一样的，但是它需要的接口更少、使用更方便。 简化 API 提供的主要接口如下：\n\nConnectionFactory：客户端用来创建连接的受管对象。传统 API 也会使用此接口。\nJMSContext：客户端到 JMS 提供者之间的活动连接，以及发送和接收消息的一个单线程上下文。\nJMSProducer：由 JMSContext 创建的对象，用于发送消息到 Queue 或 Topic\nJMSConsumer：由 JMSContext 创建的对象，用于接收 Queue 或 Topic 中的消息\n\n\n在简化 API 中，一个 JMSContext 对象封装了传统 API 中 Connection 和 Session 两个对象的行为。\n开发一个 JMS 客户端一个使用传统 API 的 JMS 客户端典型的使用步骤如下：\n\n使用 JNDI 查找一个 ConnectionFactory 对象\n使用 JNDI 查找一个或多个 Destination 对象\n使用 ConnectionFactory 创建一个 JMS Connection 对象\n使用 Connection 创建一个或多个 JMS Session 对象\n使用 Session 和 Destination 对象创建需要的 MessageProducer 和 MessageConsumer 对象\n通知 Connection 对象开始投递消息\n\n\nActive MQ 是完全实现 JMS 规范的 JMS 客户端\n\nHello World创建 Hello World 项目创建 gradle 项目，并编辑 build.gradle\ncompile group: 'org.apache.activemq', name: 'activemq-all', version: '5.15.9'compile group: 'com.fasterxml.jackson.core', name: 'jackson-databind', version: '2.9.8'\n\n创建生产者public class HelloWorldProducer implements Runnable {    @Override    public void run() {        try {            // 1. 创建连接工厂            ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"vm://localhost\");            // 2. 创建连接            Connection connection = connectionFactory.createConnection();            connection.start();            // 3. 创建会话            Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);            // 4. 创建目的地（主题或队列）            Destination destination = session.createQueue(\"TEST.FOO\");            // 5. 从会话创建到目的地的消息发布者            MessageProducer producer = session.createProducer(destination);            producer.setDeliveryMode(DeliveryMode.NON_PERSISTENT);            // 6. 创建并发布消息            String text = \"Hello world! From: \" + Thread.currentThread().getName() + \" : \" + this.hashCode();            TextMessage message = session.createTextMessage(text);            System.out.println(\"Sent message: \" + message.hashCode() + \" : \" + Thread.currentThread().getName());            producer.send(message);            // 7. 销毁资源            session.close();            connection.close();        } catch (JMSException e) {            System.out.println(\"Caught: \" + e);            e.printStackTrace();        }    }}\n\n创建消费者public class HelloWorldConsumer implements Runnable, ExceptionListener {    @Override    public void run() {        try {            // 1. 创建连接工厂            ActiveMQConnectionFactory connectionFactory = new ActiveMQConnectionFactory(\"vm://localhost\");            // 2. 创建连接            Connection connection = connectionFactory.createConnection();            connection.start();            // 3. 创建会话            Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE);            // 4. 创建目的地（主题或队列）            Destination destination = session.createQueue(\"TEST.FOO\");            // 5. 从会话创建到目的地的消息消费者            MessageConsumer consumer = session.createConsumer(destination);            // 6. 等待接收消息            Message message = consumer.receive(1000);            if (message instanceof TextMessage) {                TextMessage textMessage = (TextMessage) message;                String text = textMessage.getText();                System.out.println(\"Received: \" + text);            } else {                System.out.println(\"Received: \" + message);            }            // 7. 销毁资源            consumer.close();            session.close();            connection.close();        } catch (JMSException e) {            System.out.println(\"Caught: \" + e);            e.printStackTrace();        }    }    @Override    public synchronized void onException(JMSException exception) {        System.out.println(\"JMS Exception occured.  Shutting down client.\");    }}\n\n测试类public class App {    public static void main(String[] args) throws InterruptedException {        thread(new HelloWorldProducer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        Thread.sleep(1000);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        Thread.sleep(1000);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldProducer(), false);        Thread.sleep(1000);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldConsumer(), false);        thread(new HelloWorldProducer(), false);    }    public static void thread(Runnable runnable, boolean daemon) {        Thread brokerThread = new Thread(runnable);        brokerThread.setDaemon(daemon);        brokerThread.start();    }}\n\n运行我们的测试程序，控制台将会打印：\nSent message: 507732978 : Thread-6Sent message: 2056557229 : Thread-0Sent message: 39234146 : Thread-8Sent message: 1100925878 : Thread-13Sent message: 1566392082 : Thread-17Sent message: 1329793151 : Thread-1Sent message: 988436874 : Thread-16Received: Hello world! From: Thread-6 : 1442537083Received: Hello world! From: Thread-1 : 1531760310Received: Hello world! From: Thread-0 : 1817576164Received: Hello world! From: Thread-8 : 262381200Received: Hello world! From: Thread-17 : 1647178742Received: Hello world! From: Thread-13 : 1610404140\n\n\n\n","categories":["个人笔记"],"tags":["消息中间件","ActiveMQ"]},{"title":"消息队列（二）RabbitMQ","url":"/p/45284/","content":"在 Ubuntu 上安装 RabbitMQ系统初始化$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean$ echo 127.0.0.1 mq &gt; /etc/hosts$ echo rabbitmq &gt; /etc/hostname$ export HOSTNAME=mq\n\n\n\n搭建 rabbitmq 服务$ echo 'deb http://www.rabbitmq.com/debian/ testing main'| sudo tee /etc/apt/sources.list.d/rabbitmq.list$ wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install rabbitmq-server\n\n创建管理账户$ sudo rabbitmqctl add_user test test$ sudo rabbitmqctl add_vhost /test$ sudo rabbitmqctl set_user_tags test administrator$ sudo rabbitmqctl set_permissions -p /test test \".*\" \".*\" \".*\"$ sudo rabbitmq-plugins enable rabbitmq_management\n\nAMQP 规范AMQP（高级消息队列协议）是一个网络协议。它支持符合要求的客户端应用（application）和消息中间件代理（messaging middleware broker）之间进行通信。\n消息代理和他们所扮演的角色消息代理（message brokers）从发布者（publishers）亦称生产者（producers）那儿接收消息，并根据既定的路由规则把接收到的消息发送给处理消息的消费者（consumers）。\n由于 AMQP 是一个网络协议，所以这个过程中的发布者，消费者，消息代理 可以存在于不同的设备上。\nAMQP 0-9-1 模型简介AMQP 0-9-1 的工作过程如下图：消息（message）被发布者（publisher）发送给交换机（exchange），交换机常常被比喻成邮局或者邮箱。然后交换机将收到的消息根据路由规则分发给绑定的队列（queue）。最后 AMQP 代理会将消息投递给订阅了此队列的消费者，或者消费者按照需求自行获取。\n\n\n队列，交换机和绑定统称为 AMQP 实体（AMQP entities）。\n\n交换机和交换机类型交换机是用来发送消息的 AMQP 实体。交换机拿到一个消息之后将它路由给一个或零个队列。它使用哪种路由算法是由交换机类型和被称作绑定（bindings）的规则所决定的。AMQP 0-9-1 的代理提供了四种交换机\n\n\n\nName（交换机类型）\nDefault pre-declared names（预声明的默认名称）\n\n\n\nDirect exchange（直连交换机）\n(Empty string) and amq.direct\n\n\nFanout exchange（扇型交换机）\namq.fanout\n\n\nTopic exchange（主题交换机）\namq.topic\n\n\nHeaders exchange（头交换机）\namq.match (and amq.headers in RabbitMQ)\n\n\n除交换机类型外，在声明交换机时还可以附带许多其他的属性，其中最重要的几个分别是：\n\nName\nDurability （消息代理重启后，交换机是否还存在）\nAuto-delete （当所有与之绑定的消息队列都完成了对此交换机的使用后，删掉它）\nArguments（依赖代理本身）\n\n交换机可以有两个状态：持久（durable）、暂存（transient）。持久化的交换机会在消息代理（broker）重启后依旧存在，而暂存的交换机则不会（它们需要在代理再次上线后重新被声明）。\n队列AMQP 中的队列（queue）跟其他消息队列或任务队列中的队列是很相似的：它们存储着即将被应用消费掉的消息。\n队列跟交换机共享某些属性，但是队列也有一些另外的属性。\n\nName\nDurable（消息代理重启后，队列依旧存在）\nExclusive（只被一个连接（connection）使用，而且当连接关闭后队列即被删除）\nAuto-delete（当最后一个消费者退订后即被删除）\nArguments（一些消息代理用他来完成类似与 TTL 的某些额外功能）\n\n队列在声明（declare）后才能被使用。如果一个队列尚不存在，声明一个队列会创建它。如果声明的队列已经存在，并且属性完全相同，那么此次声明不会对原有队列产生任何影响。如果声明中的属性与已存在队列的属性有差异，那么一个错误代码为 406 的通道级异常就会被抛出。\n绑定绑定（Binding）是交换机（exchange）将消息（message）路由给队列（queue）所需遵循的规则。\n消费者\n将消息投递给应用 (“push API”)\n 应用根据需要主动获取消息 (“pull API”)\n\n消息确认\n自动确认：当消息代理（broker）将消息发送给应用后立即删除。\n显式确认：待应用（application）发送一个确认回执（acknowledgement）后再删除消息。\n\n拒绝消息当拒绝一条消息时，可以\n\n销毁消息\n重新放入消息队列\n\n当此队列只有一个消费者时，请确认不要由于拒绝消息并且选择了重新放入队列的行为而引起消息在同一个消费者身上无限循环的情况发生。\nHello World\n生产 (Producing) 的意思就是发送。发送消息的程序就是一个生产者 (producer)。我们一般用”P” 来表示:\n队列 (queue) 就是存在于 RabbitMQ 中邮箱的名称。虽然消息的传输经过了 RabbitMQ 和你的应用程序，但是它只能被存储于队列当中。实质上队列就是个巨大的消息缓冲区，它的大小只受主机内存和硬盘限制。多个生产者（producers）可以把消息发送给同一个队列，同样，多个消费者（consumers）也能够从同一个队列（queue）中获取数据。队列可以绘制成这样（图上是队列的名称）：\n在这里，消费（Consuming）和接收 (receiving) 是同一个意思。一个消费者（consumer）就是一个等待获取消息的程序。我们把它绘制为”C”：\n\n需要指出的是生产者、消费者、代理需不要待在同一个设备上；事实上大多数应用也确实不在会将他们放在一台机器上。\n创建 gradle 项目，并配置 build.gradle：\ncompile group: 'com.rabbitmq', name: 'amqp-client', version: '5.6.0'\n\n创建生产者public class Send {    private final static String QUEUE_NAME = \"hello\";    public static void main(String[] args) throws IOException, TimeoutException {        // 1. 创建RabbitMQ连接工厂        ConnectionFactory factory = new ConnectionFactory();        // 2. 设置host,rabbitmq-server的监听地址        factory.setHost(\"localhost\");        Connection connection = factory.newConnection();        // 4. 创建频道        Channel channel = connection.createChannel();        // 5. 连接到具体频道        channel.queueDeclare(QUEUE_NAME, false, false, false, null);        String message = \"Hello World!\";        // 6. 发布消息        channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes());        System.out.println(\" [x] Sent '\" + message + \"'\");    }}\n\n创建消费者public class Recv {    private final static String QUEUE_NAME = \"hello\";    public static void main(String[] args) throws IOException, TimeoutException {        ConnectionFactory factory = new ConnectionFactory();        factory.setHost(\"localhost\");        Connection connection = factory.newConnection();        // 4. 创建频道        Channel channel = connection.createChannel();        channel.queueDeclare(QUEUE_NAME, false, false, false, null);        System.out.println(\" [*] Waiting for messages. To exit press CTRL+C\");                DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; {            String message = new String(delivery.getBody(), \"UTF-8\");            System.out.println(\" [x] Received '\" + message + \"'\");        };        channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; {        });    }}\n\n可以看出，生产者和消费者需要声明是同一个队列\n测试我们先执行 Send.main，控制台将打印：\n[x] Sent 'Hello World!'\n\n然后执行 Recv.main，控制台将打印：\n[*] Waiting for messages. To exit press CTRL+C[x] Received 'Hello World!'\n\n任务队列\n工作队列（又称：任务队列 ——Task Queues）是为了避免等待一些占用大量资源、时间的操作。当我们把任务（Task）当作消息发送到队列中，一个运行在后台的工作者（worker）进程就会取出任务然后处理。当你运行多个工作者（workers），任务就会在它们之间共享。\n这个概念在网络应用中是非常有用的，它可以在短暂的 HTTP 请求中处理一些复杂的任务。\n修改 Send.java 代码，来间隔 10 秒发送一个消息：\nfor (int i = 1; i &lt;= 100; i++) {    String message = String.format(\"发送第%d条消息\", i);    // 6. 发布消息    channel.basicPublish(\"\", \"hello\", null, message.getBytes());    System.out.println(\" [x] Sent '\" + message + \"'\");    Thread.sleep(10000);}\n\n修改 Recv.java，来完成一个任务，这里，假装任务执行需要耗时 1s：\nDeliverCallback deliverCallback = (consumerTag, delivery) -&gt; {    String message = new String(delivery.getBody(), \"UTF-8\");    System.out.println(\" [x] Received '\" + message + \"'\");    try {        doWork(message);    } catch (InterruptedException e) {        e.printStackTrace();    } finally {        System.out.println(\" [x] Done\");    }};channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -&gt; {});private static void doWork(String task) throws InterruptedException {    for (char ch : task.toCharArray()) {        if (ch == '-') {            Thread.sleep(1000);        }    }}\n\n我们先开启 Recv.java，然后开启 Send.java，控制台将会打印\nSend.java\n[x] Sent '发送第1条消息'[x] Sent '发送第2条消息'[x] Sent '发送第3条消息'\n\nRecv.java\n[*] Waiting for messages. To exit press CTRL+C[x] Received '发送第1条消息'[x] Done[x] Received '发送第2条消息'[x] Done[x] Received '发送第3条消息'\n\n循环调度使用工作队列的一个好处就是它能够并行的处理队列。如果堆积了很多任务，我们只需要添加更多的工作者（workers）就可以了，扩展很简单。\n让我们尝试同时运行两个 worker 实例，他们都会从队列中获取消息：\nSend.java\n[x] Sent '发送第1条消息'[x] Sent '发送第2条消息'[x] Sent '发送第3条消息'\n\nRecv.java-1\n[*] Waiting for messages. To exit press CTRL+C[x] Received '发送第1条消息'[x] Done[x] Received '发送第3条消息'[x] Done\n\nRecv.java-2\n[*] Waiting for messages. To exit press CTRL+C[x] Received '发送第2条消息'[x] Done\n\n默认来说，RabbitMQ 会按顺序得把消息发送给每个消费者（consumer）。平均每个消费者都会收到同等数量得消息。这种发送消息得方式叫做 —— 轮询（round-robin）。试着添加三个或更多得工作者（workers）。\n消息确认当处理一个比较耗时得任务的时候，你也许想知道消费者（consumers）是否运行到一半就挂掉。当前的代码中，当消息被 RabbitMQ 发送给消费者（consumers）之后，马上就会在内存中移除。这种情况，你只要把一个工作者（worker）停止，正在处理的消息就会丢失。同时，所有发送到这个工作者的还没有处理的消息都会丢失。\n我们不想丢失任何任务消息。如果一个工作者（worker）挂掉了，我们希望任务会重新发送给其他的工作者（worker）。\n为了防止消息丢失，RabbitMQ 提供了消息响应（acknowledgments）。消费者会通过一个 ack（响应），告诉 RabbitMQ 已经收到并处理了某条消息，然后 RabbitMQ 就会释放并删除这条消息。\n如果消费者（consumer）挂掉了，没有发送响应，RabbitMQ 就会认为消息没有被完全处理，然后重新发送给其他消费者（consumer）。这样，及时工作者（workers）偶尔的挂掉，也不会丢失消息。\n消息是没有超时这个概念的；当工作者与它断开连的时候，RabbitMQ 会重新发送消息。这样在处理一个耗时非常长的消息任务的时候就不会出问题了。\n消息响应默认是开启的。之前的例子中我们可以使用 no_ack=True 标识把它关闭。是时候移除这个标识了，当工作者（worker）完成了任务，就发送一个响应。\n修改 Worker.java\n// 一次只接受一条消息channel.basicQos(1);DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; {    String message = new String(delivery.getBody(), \"UTF-8\");    System.out.println(\" [x] Received '\" + message + \"'\");    try {        doWork(message);    } catch (InterruptedException e) {        e.printStackTrace();    } finally {        channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);    }};boolean autoAck = false;channel.basicConsume(QUEUE_NAME, autoAck, deliverCallback, consumerTag -&gt; {});\n\n运行上面的代码，我们发现即使使用 CTRL+C 杀掉了一个工作者（worker）进程，消息也不会丢失。当工作者（worker）挂掉这后，所有没有响应的消息都会重新发送。\n消息持久化如果你没有特意告诉 RabbitMQ，那么在它退出或者崩溃的时候，将会丢失所有队列和消息。为了确保信息不会丢失，有两个事情是需要注意的：我们必须把 “队列” 和 “消息” 设为持久化。\n首先，为了不让队列消失，需要把队列声明为持久化（durable）：\nboolean durable = true;channel.queueDeclare(QUEUE_NAME, durable, false, false, null);\n\n尽管这行代码本身是正确的，但是仍然不会正确运行。因为我们已经定义过一个叫 hello 的非持久化队列。RabbitMq 不允许你使用不同的参数重新定义一个队列，它会返回一个错误。但我们现在使用一个快捷的解决方法 —— 用不同的名字，例如 task_queue。\nboolean durable = true;channel.queueDeclare(\"task_queue\", durable, false, false, null);\n\n这时候，我们就可以确保在 RabbitMq 重启之后 queue_declare 队列不会丢失。现在我们需要将消息标记为持久性 - 通过将 MessageProperties（实现 BasicProperties）设置为值 PERSISTENT_TEXT_PLAIN。\nimport com.rabbitmq.client.MessageProperties;channel.basicPublish(\"\", \"task_queue\",            MessageProperties.PERSISTENT_TEXT_PLAIN,            message.getBytes());\n\n公平调度你应该已经发现，它仍旧没有按照我们期望的那样进行分发。比如有两个工作者（workers），处理奇数消息的比较繁忙，处理偶数消息的比较轻松。然而 RabbitMQ 并不知道这些，它仍然一如既往的派发消息。\n这时因为 RabbitMQ 只管分发进入队列的消息，不会关心有多少消费者（consumer）没有作出响应。它盲目的把第 n-th 条消息发给第 n-th 个消费者。\n\n我们可以使用 basicQos 方法，并设置 prefetchCount = 1。这样是告诉 RabbitMQ，再同一时刻，不要发送超过 1 条消息给一个工作者（worker），直到它已经处理了上一条消息并且作出了响应。这样，RabbitMQ 就会把消息分发给下一个空闲的工作者（worker）。\nint prefetchCount = 1;channel.basicQos(prefetchCount);\n\n发布／订阅在上篇教程中，我们搭建了一个工作队列，每个任务只分发给一个工作者（worker）。在本篇教程中，我们要做的跟之前完全不一样 —— 分发一个消息给多个消费者（consumers）。这种模式被称为 “发布／订阅”。\n为了描述这种模式，我们将会构建一个简单的日志系统。\n交换机（Exchanges）RabbitMQ 中完整的消息模型：\n\n发布者（producer）是发布消息的应用程序。\n队列（queue）用于消息存储的缓冲。\n消费者（consumer）是接收消息的应用程序。\n\nRabbitMQ 消息模型的核心理念是：发布者（producer）不会直接发送任何消息给队列。事实上，发布者（producer）甚至不知道消息是否已经被投递到队列。\n发布者（producer）只需要把消息发送给一个交换机（exchange）。交换机非常简单，它一边从发布者方接收消息，一边把消息推送到队列。交换机必须知道如何处理它接收到的消息，是应该推送到指定的队列还是是多个队列，或者是直接忽略消息。这些规则是通过交换机类型（exchange type）来定义的。\n\n有几个可供选择的交换机类型：直连交换机（direct）, 主题交换机（topic）, （头交换机）headers 和 扇型交换机（fanout）。我们在这里主要说明最后一个 —— 扇型交换机（fanout）。先创建一个 fanout 类型的交换机，命名为 logs：\nchannel.exchangeDeclare(\"logs\", \"fanout\");\n\n扇型交换机（fanout）很简单，你可能从名字上就能猜测出来，它把消息发送给它所知道的所有队列。\n现在，我们就可以发送消息到一个具名交换机了：\nchannel.basicPublish( \"logs\", \"\", null, message.getBytes());\n\n临时队列要创建一个临时队列，我们需要做两件事情：\n\n当我们连接上 RabbitMQ 的时候，我们需要一个全新的、空的队列。我们可以手动创建一个随机的队列名，或者让服务器为我们选择一个随机的队列名（推荐）。\n\n当与消费者（consumer）断开连接的时候，这个队列应当被立即删除。\n\n\n在 Java 客户端中，当我们没有向 queueDeclare（）提供参数时，我们使用生成的名称创建一个非持久的，独占的自动删除队列：\n// 服务器分配的随机队列名，可能像这样 amq.gen-U0srCoW8TsaXjNh73pnVAw==String queueName = channel.queueDeclare().getQueue();\n\n绑定（Bindings）\n我们已经创建了一个扇型交换机（fanout）和一个队列。现在我们需要告诉交换机如何发送消息给我们的队列。交换器和队列之间的联系我们称之为绑定（binding）。\nchannel.queueBind(queueName, \"logs\", \"\");\n\n路由 (Routing)前面的例子，我们已经创建过绑定（bindings），代码如下：\nchannel.queueBind(queueName, EXCHANGE_NAME, \"\");\n\n绑定（binding）是指交换机（exchange）和队列（queue）的关系。可以简单理解为：这个队列（queue）对这个交换机（exchange）的消息感兴趣。\n绑定的时候可以带上一个额外的 routing_key 参数。为了避免与 basic_publish 的参数混淆，我们把它叫做绑定键（binding key）。以下是如何创建一个带绑定键的绑定。\nchannel.queueBind(queueName, EXCHANGE_NAME, \"black\");\n\n绑定键的意义取决于交换机（exchange）的类型。我们之前使用过的扇型交换机（fanout exchanges）会忽略这个值。\n直连交换机（Direct exchange）我们的日志系统广播所有的消息给所有的消费者（consumers）。我们打算扩展它，使其基于日志的严重程度进行消息过滤。\n我们使用的扇型交换机（fanout exchange）没有足够的灵活性 —— 它能做的仅仅是广播。\n我们将会使用直连交换机（direct exchange）来代替。路由的算法很简单 —— 交换机将会对绑定键（binding key）和路由键（routing key）进行精确匹配，从而确定消息该分发到哪个队列。\n下图能够很好的描述这个场景：\n\n在这个场景中，我们可以看到直连交换机 X 和两个队列进行了绑定。第一个队列使用 orange 作为绑定键，第二个队列有两个绑定，一个使用 black 作为绑定键，另外一个使用 green。\n这样以来，当路由键为 orange 的消息发布到交换机，就会被路由到队列 Q1。路由键为 black 或者 green 的消息就会路由到 Q2。其他的所有消息都将会被丢弃。\n多个绑定（Multiple bindings）\n多个队列使用相同的绑定键是合法的。这个例子中，我们可以添加一个 X 和 Q1 之间的绑定，使用 black 绑定键。这样一来，直连交换机就和扇型交换机的行为一样，会将消息广播到所有匹配的队列。带有 black 路由键的消息会同时发送到 Q1 和 Q2。\n发送日志我们将会发送消息到一个直连交换机，把日志级别作为路由键。这样接收日志的脚本就可以根据严重级别来选择它想要处理的日志。我们先看看发送日志。\n我们需要创建一个交换机（exchange）：\nchannel.exchangeDeclare(EXCHANGE_NAME, \"direct\");\n\n然后我们发送一则消息：\nchannel.basicPublish(EXCHANGE_NAME, severity, null, message.getBytes());\n\n订阅处理接收消息的方式和之前差不多，只有一个例外，我们将会为我们感兴趣的每个严重级别分别创建一个新的绑定。\nString queueName = channel.queueDeclare().getQueue();for(String severity : argv){  channel.queueBind(queueName, EXCHANGE_NAME, severity);}\n\n示例代码\nRouting\n主题交换机直连交换机的限制 —— 没办法基于多个标准执行路由操作。\n发送到主题交换机（topic exchange）的消息不可以携带随意什么样子的路由键（routing_key），它的路由键必须是一个由. 分隔开的词语列表。这些单词随便是什么都可以，但是最好是跟携带它们的消息有关系的词汇。以下是几个推荐的例子：”stock.usd.nyse”, “nyse.vmw”, “quick.orange.rabbit”。词语的个数可以随意，但是不要超过 255 字节。\n绑定键也必须拥有同样的格式。主题交换机背后的逻辑跟直连交换机很相似 —— 一个携带着特定路由键的消息会被主题交换机投递给绑定键与之想匹配的队列。但是它的绑定键和路由键有两个特殊应用方式：\n\n* (星号) 用来表示一个单词.\n# (井号) 用来表示任意数量（零个或多个）单词。\n\n下边用图说明：\n\n我们创建了三个绑定：Q1 的绑定键为 *.orange.*，Q2 的绑定键为 *.*.rabbit 和 lazy.# 。\n这三个绑定键被可以总结为：\n\nQ1 对所有的桔黄色动物都感兴趣。\nQ2 则是对所有的兔子和所有懒惰的动物感兴趣。\n\n\n主题交换机是很强大的，它可以表现出跟其他交换机类似的行为\n当一个队列的绑定键为 “#”（井号） 的时候，这个队列将会无视消息的路由键，接收所有的消息。\n当 * (星号) 和 # (井号) 这两个特殊字符都未在绑定键中出现的时候，此时主题交换机就拥有的直连交换机的行为。\n\n远程过程调用（RPC）如果我们需要将一个函数运行在远程计算机上并且等待从那儿获取结果时，这种模式通常被称为远程过程调用（Remote Procedure Call）或者 RPC。\n我们会使用 RabbitMQ 来构建一个 RPC 系统：包含一个客户端和一个 RPC 服务器。\n客户端接口为了展示 RPC 服务如何使用，我们创建了一个简单的客户端类。它会暴露出一个名为 “call” 的方法用来发送一个 RPC 请求，并且在收到回应前保持阻塞。\nFibonacciRpcClient fibonacciRpc = new FibonacciRpcClient();String result = fibonacciRpc.call(\"4\");System.out.println( \"fib(4) is \" + result);\n\n回调队列一般来说通过 RabbitMQ 来实现 RPC 是很容易的。一个客户端发送请求信息，服务器端将其应用到一个回复信息中。为了接收到回复信息，客户端需要在发送请求的时候同时发送一个回调队列（callback queue）的地址。\ncallbackQueueName = channel.queueDeclare().getQueue();BasicProperties props = new BasicProperties                            .Builder()                            .replyTo(callbackQueueName)                            .build();channel.basicPublish(\"\", \"rpc_queue\", props, message.getBytes());\n\n\n消息属性AMQP 协议给消息预定义了一系列的 14 个属性。大多数属性很少会用到，除了以下几个：\n\ndelivery_mode（投递模式）：将消息标记为持久的（值为 2）或暂存的（除了 2 之外的其他任何值）。第二篇教程里接触过这个属性，记得吧？\ncontent_type（内容类型）: 用来描述编码的 mime-type。例如在实际使用中常常使用 application/json 来描述 JOSN 编码类型。\nreply_to（回复目标）：通常用来命名回调队列。\ncorrelation_id（关联标识）：用来将 RPC 的响应和请求关联起来。\n\n\n关联标识上边介绍的方法中，我们建议给每一个 RPC 请求新建一个回调队列。这不是一个高效的做法，幸好这儿有一个更好的办法 —— 我们可以为每个客户端只建立一个独立的回调队列。\n这就带来一个新问题，当此队列接收到一个响应的时候它无法辨别出这个响应是属于哪个请求的。correlation_id 就是为了解决这个问题而来的。我们给每个请求设置一个独一无二的值。稍后，当我们从回调队列中接收到一个消息的时候，我们就可以查看这条属性从而将响应和请求匹配起来。如果我们接手到的消息的 correlation_id 是未知的，那就直接销毁掉它，因为它不属于我们的任何一条请求。\n为什么我们接收到未知消息的时候不抛出一个错误，而是要将它忽略掉？这是为了解决服务器端有可能发生的竞争情况。尽管可能性不大，但 RPC 服务器还是有可能在已将应答发送给我们但还未将确认消息发送给请求的情况下死掉。如果这种情况发生，RPC 在重启后会重新处理请求。这就是为什么我们必须在客户端优雅的处理重复响应，同时 RPC 也需要尽可能保持幂等性。\n总结\n我们的 RPC 如此工作:\n\n当客户端启动的时候，它创建一个匿名独享的回调队列。\n在 RPC 请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。\n将请求发送到一个 rpc_queue 队列中。\nRPC 工作者（又名：服务器）等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给 reply_to 字段指定的队列。\n客户端等待回调队列里的数据。当有消息出现的时候，它会检查 correlation_id 属性。如果此属性的值与请求匹配，将它返回给应用。\n\n","categories":["个人笔记"],"tags":["消息中间件","RabbitMQ"]},{"title":"理解 Nginx 负载均衡","url":"/p/52703/","content":"前言工作以来，一直都在使用 Nginx 作为负载均衡服务器，但是关于 Nginx 的负载均衡算法一直没有深入理解过，这次好好的整理下。\n\n\n准备服务器\n搭建三台用于测试的虚拟机\n\n\n\n\n名称\n IP\n 服务\n\n\n\n node01\n192.168.198.131\nNginx、模拟业务（8080）\n\n\nnode02\n192.168.198.130\n 模拟业务（8080）\n\n\nnode03\n192.168.198.132\n 模拟业务（8080）\n\n\n修改 hostname 和 hosts\n$ vim /etc/hosts192.168.198.131 node01$ vim /etc/hostnamenode01$ reboot## 其余两台也改下，并重启使配置生效\n\n在 node01 上安装 Nginx 服务\n$ echo -e \"deb http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\\ndeb-src http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list$ wget -O- http://nginx.org/keys/nginx_signing.key | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install nginx\n\n模拟业务使用 https://start.spring.io 快速新建 Spring Boot 项目，添加 Web 模块，并编写以下代码：\n@RestController@RequestMapping(\"/test\")public class DemoController {    @GetMapping    public String test() throws UnknownHostException {        return \"this is : \" + Inet4Address.getLocalHost();    }}\n\n打包并部署到服务器，我使用的是 The Application Plugin，部署完毕启动\n\n测试下：\n## node01$ curl 192.168.198.131:8080/test...this is : node01/192.168.198.131## node02$ curl 192.168.198.130:8080/test...this is : node02/192.168.198.130## node03$ curl 192.168.198.132:8080/test...this is : node03/192.168.198.132\n\nNginx 负载均衡Round Robin（轮询）\n请求在服务器之间均匀分布，可以设置服务器权重。\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\tserver 192.168.198.131:8080;\tserver 192.168.198.132:8080;\tserver 192.168.198.130:8080;}server {\tlisten 80;\tserver_name 192.168.198.131;\t\tlocation / {\t\tproxy_pass http://backend;\t}}$ service nginx restart\n\n测试下\n$ curl 192.168.198.131/test...this is : node01/192.168.198.131 # node01$ curl 192.168.198.131/test...this is : node03/192.168.198.132 # node03$ curl 192.168.198.131/test...this is : node03/192.168.198.130 # node02$ curl 192.168.198.131/test...this is : node01/192.168.198.131 # node01\n\n可以看到，每台服务器访问到的次数是相等的。\nLeast Connections\n请求分配到连接数最少的服务器，可以设置服务器权重。\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\tleast_conn;\tserver 192.168.198.131:8080;\tserver 192.168.198.132:8080;\tserver 192.168.198.130:8080;}$ service nginx restart\n\n这个不知道怎么模拟出连接数最少场景。\nIP Hash\n从客户端的 IP 地址来确定请求应该发送给哪台服务器。在这种情况下，使用 IPv4 地址的前三个八位字节或整个 IPv6 地址来计算散列值。该方法能保证来自同一地址的请求分配到同一台服务器，除非该服务器不可用。\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\tip_hash;\tserver 192.168.198.131:8080;\tserver 192.168.198.132:8080;\tserver 192.168.198.130:8080;}$ service nginx restart\n\n测试下\n$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node01/192.168.198.131\n\n可以看到，请求都被分配到 node01 节点。\n接下来，将 node01 节点关闭，看看会发生什么：\n$ ps -ef | grep demoroot       3343   1764  0 11:52 pts/0    00:00:23 java -jar /home/demo/demo-boot-0.0.1-SNAPSHOT/lib/demo-0.0.1-SNAPSHOT.jarroot       4529   1764  0 13:11 pts/0    00:00:00 grep --color=auto demo$ kill -9 3343$ ps -ef | grep demoroot       4529   1764  0 13:11 pts/0    00:00:00 grep --color=auto demo$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132\n\n由于 node01 节点不可用，请求都被分配到 node03 节点。\nGeneric Hash\n与上面的 IP_HASH 类似，通用 HASH 按照用户定义的参数来计算散列值，参数可以是文本字符串，变量或组合。例如，参数可以是远端地址：\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\thash $remote_addr consistent;\tserver 192.168.198.131:8080;\tserver 192.168.198.132:8080;\tserver 192.168.198.130:8080;}$ service nginx restart\n\n测试下\n$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node02/192.168.198.130\n\n可以看到，请求都被分配到了 node02 节点。\n👉上面的 consistent 是可选参数，如果设置了，将采用 Ketama 一致性 hash 算法计算散列值。\n关于一致性 Hash，可以查看我的另一篇博客：理解一致性 Hash 算法\n Random\n请求会被随机分配到一台服务器，可以设置服务器权重。\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\trandom;\tserver 192.168.198.131:8080;\tserver 192.168.198.132:8080;\tserver 192.168.198.130:8080;}$ service nginx restart\n\n测试下\n$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node02/192.168.198.130$ curl 192.168.198.131/test...this is : node01/192.168.198.130\n\n可以看到，请求是被随机分配到三台服务器的。\nWeights\n除了设置负载均衡算法，我们还可以为服务器设置权重，权重默认值是 1\n\n$ vim /etc/nginx/conf/demo.confupstream backend {\tserver 192.168.198.131:8080 weight=5;\tserver 192.168.198.132:8080 weight=10;\tserver 192.168.198.130:8080;}$ service nginx restart\n\n测试下\n$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node03/192.168.198.132$ curl 192.168.198.131/test...this is : node01/192.168.198.131\n\n可以看到，5 次请求中，node03(weight=10) 占了 3 次，node01(weight=5) 占了 2 次，node02(weight=1)1 次都没有。\n理论上来说，上面的配置，访问 16 次，node03 应被分配 10 次，node01 应被分配 5 次，node02 应被分配 1 次。\n参考资料http-load-balancer\n","categories":["个人笔记"],"tags":["Nginx"]},{"title":"理解一致性 Hash 算法","url":"/p/61241/","content":"简介\n一致性哈希算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的，设计目标是为了解决因特网中的热点 (Hot spot) 问题，初衷和 CARP 十分类似。一致性哈希修正了 CARP 使用的简单哈希算法带来的问题，使得 DHT 可以在 P2P 环境中真正得到应用。\n\n现在一致性 hash 算法在分布式系统中也得到了广泛应用，研究过 memcached 缓存数据库的人都知道，memcached 服务器端本身不提供分布式 cache 的一致性，而是由客户端来提供，具体在计算一致性 hash 时采用如下步骤：\n\n\n\n首先求出 memcached 服务器（节点）的哈希值，并将其配置到 0～2^32 的圆（continuum）上。\n\n采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。\n\n从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器，就会保存到第一台 memcached 服务器上。\n\n\n\n从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但 Consistent Hashing 中，只有在圆（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示：\n\n一致性 Hash 性质\n考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的。\n尤其是在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其 hash 值（通常与系统中的节点数目有关），由于 hash 值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性 hash 就显得至关重要。\n\n良好的分布式 cahce 系统中的一致性 hash 算法应该满足以下几个方面：\n\n平衡性 (Balance)\n\n\n平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。\n\n\n单调性 (Monotonicity)\n\n\n单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。\n简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希 x = (ax + b) mod (P)，在上式中，P 表示全部缓冲的大小。不难看出，当缓冲大小发生变化时 (从 P1 到 P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。\n哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在 P2P 系统内，缓冲的变化等价于 Peer 加入或退出系统，这一情况在 P2P 系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。\n\n\n分散性 (Spread)\n\n\n在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。\n当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。\n分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。\n\n\n负载 (Load)\n\n\n负载问题实际上是从另一个角度看待分散性问题。\n既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。\n与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。\n\n\n平滑性 (Smoothness)\n\n\n平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。\n\n原理基本概念一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。\n简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-2^32-1（即哈希值是一个 32 位无符号整形），整个哈希空间环如下：\n\n整个空间按顺时针方向组织。0 和 2^32-1 在零点中方向重合。\n下一步将各个服务器使用 Hash 进行一个哈希，具体可以选择服务器的 ip 或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用 ip 地址哈希后在环空间的位置如下：\n\n接下来使用如下算法定位数据访问到相应服务器：将数据 key 使用相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针 “行走”，第一台遇到的服务器就是其应该定位到的服务器。\n例如我们有 Object A、Object B、Object C、Object D 四个数据对象，经过哈希计算后，在环空间上的位置如下：\n\n根据一致性哈希算法，数据 A 会被定为到 Node A 上，B 被定为到 Node B 上，C 被定为到 Node C 上，D 被定为到 Node D 上。\n容错性现假设 Node C 不幸宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。\n一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。\n可扩展性如果在系统中增加一台服务器 Node X，如下图所示：\n\n此时对象 Object A、B、D 不受影响，只有对象 C 需要重定位到新的 Node X 。\n一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。\n综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。\n数据倾斜问题另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下：\n\n此时必然造成大量数据集中到 Node A 上，而只有极少量会定位到 Node B 上。\n为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。\n具体做法可以在服务器 ip 或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3” 的哈希值，于是形成六个虚拟节点：\n\n同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到 “Node A#1”、“Node A#2”、“Node A#3” 三个虚拟节点的数据均定位到 Node A 上。这样就解决了服务节点少时数据倾斜的问题。\n在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。\n代码测试一致性 Hash 模拟类：\npackage com.example.demo.hash;import java.util.*;/** * 一致性Hash * * @author gaochen * @date 2019/5/29 */public class ConsistentHash&lt;T&gt; {    /**     * 节点的复制因子,实际节点个数 * numberOfReplicas     */    private final int numberOfReplicas;    /**     * 虚拟节点个数,存储虚拟节点的hash值到真实节点的映射     */    private final SortedMap&lt;Integer, T&gt; circle = new TreeMap&lt;&gt;();    public ConsistentHash(int numberOfReplicas, Collection&lt;T&gt; nodes) {        this.numberOfReplicas = numberOfReplicas;        for (T node : nodes) {            add(node);        }    }    /**     * 模拟添加一个节点     * &lt;p&gt;     * 对于一个实际机器节点 node, 对应 numberOfReplicas 个虚拟节点     * 不同的虚拟节点(i不同)有不同的hash值,但都对应同一个实际机器node     * 虚拟node一般是均衡分布在环上的,数据存储在顺时针方向的虚拟node上     * &lt;/P&gt;     *     * @param node 哈希环节点     */    public void add(T node) {        for (int i = 0; i &lt; numberOfReplicas; i++) {            String nodestr = node.toString() + i;            int hashcode = nodestr.hashCode();            System.out.println(\"hashcode:\" + hashcode);            circle.put(hashcode, node);        }    }    /**     * 删除一个节点     *     * @param node 待删除节点     */    public void remove(T node) {        for (int i = 0; i &lt; numberOfReplicas; i++) {            circle.remove((node.toString() + i).hashCode());        }    }    /**     * 获得一个最近的顺时针节点,根据给定的key 取Hash     * 然后再取得顺时针方向上最近的一个虚拟节点对应的实际节点     * 再从实际节点中取得 数据     *     * @param key 模拟缓存Key     */    public T get(Object key) {        if (circle.isEmpty()) {            return null;        }        // node 用String来表示,获得node在哈希环中的hashCode        int hash = key.hashCode();        System.out.println(\"hashcode-----&gt;:\" + hash);        //数据映射在两台虚拟机器所在环之间,就需要按顺时针方向寻找机器        if (!circle.containsKey(hash)) {            SortedMap&lt;Integer, T&gt; tailMap = circle.tailMap(hash);            hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey();        }        return circle.get(hash);    }    /**     * 获取当前哈希环节点数     *     * @return 哈希环节点数     */    public long getSize() {        return circle.size();    }    /**     * 查看表示整个哈希环中各个虚拟节点位置     */    public void showBalance() {        //获得TreeMap中所有的Key        Set&lt;Integer&gt; sets = circle.keySet();        //将获得的Key集合排序        SortedSet&lt;Integer&gt; sortedSets = new TreeSet&lt;Integer&gt;(sets);        for (Integer hashCode : sortedSets) {            System.out.println(hashCode);        }        System.out.println(\"----each location 's distance are follows: ----\");        //查看相邻两个hashCode的差值        Iterator&lt;Integer&gt; it = sortedSets.iterator();        Iterator&lt;Integer&gt; it2 = sortedSets.iterator();        if (it2.hasNext()) {            it2.next();        }        long keyPre, keyAfter;        while (it.hasNext() &amp;&amp; it2.hasNext()) {            keyPre = it.next();            keyAfter = it2.next();            System.out.println(keyAfter - keyPre);        }    }}\n\n测试代码：\npackage com.example.demo.hash;import org.junit.Before;import org.junit.Test;import java.util.Arrays;import java.util.HashSet;import java.util.List;import java.util.Set;/** * @author gaochen * @date 2019/5/29 */public class ConsistentHashTest {    private static ConsistentHash&lt;String&gt; consistentHash;    @Before    public void initHash() {        Set&lt;String&gt; nodes = new HashSet&lt;&gt;();        consistentHash = new ConsistentHash&lt;&gt;(2, nodes);    }    @Test    public void testBalance() {        // 分配三个节点        consistentHash.add(\"A1\");        consistentHash.add(\"C1\");        consistentHash.add(\"D1\");        System.out.println(\"hash circle size: \" + consistentHash.getSize());        System.out.println(\"location of each node are follows: \");//        consistentHash.showBalance();        // hash值在当前哈希环内        final String key1 = \"A31\";        // hash值超出了当前哈希环        final String key2 = \"Apple\";        final List&lt;String&gt; keys = Arrays.asList(key1, key2);        // 模拟节点分配        showAllocate(keys);        // 模拟增加节点, A31被分配到更近的B1节点        consistentHash.add(\"B1\");        System.out.println(\"增加节点B1\");        showAllocate(keys);        System.out.println(\"-------------------------------------\");        // 模拟删除节点, A31被分配到更近的C1节点        consistentHash.remove(\"B1\");        System.out.println(\"删除节点B1\");        showAllocate(keys);    }    /**     * 模拟缓存分配     *     * @param keys 缓存键     */    private void showAllocate(List&lt;String&gt; keys) {        keys.forEach(key -&gt; {            String node = consistentHash.get(key);            // A31被分配到更近的C1节点            System.out.println(String.format(\"key %s is allocated to node %s\", key, node));        });    }}\n\n控制台输出：\nhashcode:64032hashcode:64033hashcode:65954hashcode:65955hashcode:66915hashcode:66916hash circle size: 6location of each node are follows: hashcode-----&gt;:64095key A31 is allocated to node C1hashcode-----&gt;:63476538key Apple is allocated to node A1hashcode:64993hashcode:64994增加节点B1hashcode-----&gt;:64095key A31 is allocated to node B1hashcode-----&gt;:63476538key Apple is allocated to node A1-------------------------------------删除节点B1hashcode-----&gt;:64095key A31 is allocated to node C1hashcode-----&gt;:63476538key Apple is allocated to node A1\n\n可以看出，增加或删除节点，只会影响到节点与上一个节点之间的元素，所以一致性 Hash 算法在容错性和可扩展性上面较普通 Hash 是有巨大提升的。\n参考资料五分钟看懂一致性哈希算法\n维基百科 - 散列函数\n一致性哈希算法及其在分布式系统中的应用\n","categories":["个人笔记"],"tags":["一致性Hash"]},{"title":"BigDecimal 精确计算工具类","url":"/p/14256/","content":"前言在实际开发中，遇到例如货币，统计等商业计算的时候，一般需要采用 java.math.BigDecimal 类来进行精确计算。而这类操作通常都是可预知的，也就是通用的。所以，写了个工具类来方便以后的工作。这是仓库地址：仓库地址\n\nBigDecimal 的构建一般而言，我们主要从 int,long,double,float 来进行计算，在构建的时候推荐使用\nBigDecimal BigDecimal(String s);\n因为通过 double 构造会损失精度，而 String 构造是固定的值。创建以下方法作为通用 BigDecimal 转化器：\n/** * Number -&gt; BigDecimal */public static &lt;T extends Number&gt; BigDecimal transform(T v) {    if (v instanceof Double) {        return new BigDecimal(Double.toString((Double) v));    } else if (v instanceof Integer) {        return new BigDecimal(Integer.toString((Integer) v));    } else if (v instanceof Long) {        return new BigDecimal(Long.toString((Long) v));    } else if (v instanceof Short) {        return new BigDecimal(Short.toString((Short) v));    } else if (v instanceof Float) {        return new BigDecimal(Float.toString((Float) v));    } else {        return (BigDecimal) v;    }}\nBigDecimal 方法计算类型加减乘除四种，BigDecimal 提供的方法也是围绕这四种计算类型设计的。\nBigDecimal add(BigDecimal augend) //加BigDecimal subtract(BigDecimal subtrahend) //减BigDecimal multiply(BigDecimal multiplicand) //乘BigDecimal divide(BigDecimal divisor, int scale, RoundingMode roundingMode) //除\n工具类在加减乘除基础上，提供了\n\n链式计算，类似 JDK8 lamada api，爽快丝滑的编程体验\n支持集合求和、求平均\n支持复合计算，例如 2*(2+8)\n\nBigDecimal 精确计算工具类实用案例精确转换为 BigDecimal，不指定精度System.out.println(PreciseCalculations.transform(121.11)); //转化double -&gt; 121.11System.out.println(PreciseCalculations.transform(Integer.MAX_VALUE)); //转化int -&gt; 2147483647System.out.println(PreciseCalculations.transform(Short.MAX_VALUE)); //转化Short -&gt; 32767System.out.println(PreciseCalculations.transform(Long.MAX_VALUE)); //转化long -&gt; 9223372036854775807System.out.println(PreciseCalculations.transform(121.19F)); //转化float -&gt; 121.19\n精确转换为 BigDecimal，指定精度System.out.println(PreciseCalculations.transform(121.1111111111, 5)); //精度大于指定精度 -&gt; 121.11111System.out.println(PreciseCalculations.transform(121.11, 5)); //精度小于指定精度，补零 -&gt; 121.11000\n加减乘除System.out.println(PreciseCalculations.add(12.11, 12.11)); //加法 -&gt; 24.22System.out.println(PreciseCalculations.subtract(12.11, 12.11)); //减法 -&gt; 0.00System.out.println(PreciseCalculations.multiply(12.11, 12.11)); //乘法 -&gt; 146.6521System.out.println(PreciseCalculations.divide(12.11, 2.35, 5)); //除法 -&gt; 5.15319\n负数计算// -1.11 * 13 - 90 = -104.43System.out.println(new PreciseCalculation(-1.11).multiply(13).add(-90).getValue()); // -11.11111111 + 90 = 78.88888889System.out.println(PreciseCalculations.add(-11.11111111,90));\n\n集合 求和 求平均值List&lt;Double&gt; list = Arrays.asList(12.11D, 13.11D, 14.11D, 15.321312D);System.out.println(PreciseCalculations.sum(list)); //求和 -&gt; Optional[54.651312]System.out.println(PreciseCalculations.average(list)); //平均值 -&gt; Optional[13.66283]System.out.println(PreciseCalculations.average(Collections.emptyList())); //空集合 -&gt; Optional.empty\n复合计算// 计算 121.11 * 13 / 60 + 100 - 12 = 114.24050System.out.println(new PreciseCalculation(121.11).multiply(13).divide(60, 5).add(100).subtract(12).getValue());//计算 121.11 * 128.59 / (100 + 12) - 100 = 39.04942System.out.println(new PreciseCalculation(121.11).multiply(128.59).divide(       new PreciseCalculation(100).add(12), 5).subtract(100).getValue());\n注意事项\nPreciseCalculation 核心类，提供加减乘除、集合精确计算方法，内部维护 value 值，每次计算该 value 都会改变。\nPreciseCalculations 基于上述的工具类，方便简单计算时使用。\n\n","categories":["个人笔记"],"tags":["Java"]},{"title":"算法很美（蓝桥） | 位运算的奇技淫巧","url":"/p/60563/","content":"我有话说前阵子跑去面试，有家公司是做电商广告数据分析的，很有意思，上来先做了三道算法题（答得不好），然后面试官花了很长时间为我解答了三道题目，一度让我以为已经是这家公司的员工了。临了，征求了下面试官对我的建议，“不是科班出身（我本科学的物理），计算机基础和算法是偏弱些，这两块要好好打磨打磨。”\n前言\n在学习算法很美课程的时候，学习到了一些位运算的奇技淫巧，收录在此\n\n判断奇偶数判断奇数 1 &amp; x == 1System.out.println((1991 &amp; 1) == 1);\n\n判断偶数 1 &amp; x == 0\nSystem.out.println((1990 &amp; 1) == 0);\n\n获取二进制数 x 位 y 是 1 还是 0\n将 x 右移 y - 1 位，与 1\n\nint x = 0b010110010;int y = 5;int res = (x &gt;&gt; (y - 1)) &amp; 1;System.out.println(res);\n\n交换两个整数变量的值不用判断语句，求整数的绝对值\n异或，可以理解为不进位加法，1 + 1 = 0，0 + 0 = 0，1 + 0 = 1\n\n异或的性质\n交换律：可任意交换运算因子的位置，结果不变\n结合律：即（a ^ b）^ c == a ^ ( b ^ c )\n对于任何数 x，都有 x ^ x = 0, x ^ 0 = x，同自己求异或为 0，同 0 求异或为自己\n自反性：A ^ B ^ B = A ^ 0 = A，连续和同一个因子做异或运算，最终结果为自己 \n\nint a = -100;// a为正数，a &gt;&gt; 31 = 00000000 00000000 00000000 00000000，求绝对值就是自己// a为负数，a &gt;&gt; 31 = 11111111 11111111 11111111 11111111，求绝对值就是自己 * -1（00000000 00000000 00000000 00000001）System.out.println((a + (a &gt;&gt; 31) ^ (a &gt;&gt; 31)));\n这边课程讲的不是很明白，可以参考下位运算求整数的绝对值，写的很好。\n","categories":["个人笔记"],"tags":["algorithm"]},{"title":"解决 Spring Security 自定义 filter 重复执行问题","url":"/p/356/","content":"今天做项目的时候，发现每次拦截器日志都会打两遍，很纳闷，怀疑是 Filter 被执行了两遍。结果 debug 之后发现还真是！记录一下这个神奇的 BUG！\n问题描述项目中使用的是 Spring-security 作为权限框架，然后做了一个 JwtAuthenticationTokenFilter 作为拦截器拦截请求，校验 Token，但是每次请求都会打两遍日志。下面是精简的源代码:\n自定义的 Filter 类\n@Slf4j@Componentpublic class JwtAuthenticationTokenFilter extends OncePerRequestFilter {    @Override    protected void doFilterInternal(            HttpServletRequest request,            HttpServletResponse response,            FilterChain chain) throws ServletException, IOException {        //...省略        //打出两遍日志的地方        log.info(\"User:{} request path:{}, method:{}, param:{}\", username, request.getServletPath(),                request.getMethod(), request.getParameterMap() == null ? null : OBJECT_MAPPER.writeValueAsString(request.getParameterMap()));        //...省略        chain.doFilter(request, response);    }}\n\nWebSecurityConfig 配置类\n@Configuration@EnableWebSecurity@EnableGlobalMethodSecurity(prePostEnabled = true)public class WebSecurityConfig extends WebSecurityConfigurerAdapter {    //...省略    @Bean    public JwtAuthenticationTokenFilter authenticationTokenFilterBean() throws Exception {        return new JwtAuthenticationTokenFilter();    }            @Override    protected void configure(HttpSecurity httpSecurity) throws Exception {        //...省略        //把JwtAuthenticationTokenFilter加入到RememberMeAuthenticationFilter之前        httpSecurity.addFilterBefore(authenticationTokenFilterBean(), RememberMeAuthenticationFilter.class);    }    //...省略}\n\n请求日志如下:\n\n问题解决把自定义 FilterJwtAuthenticationTokenFilter 的 @Component 取消掉就可以了，不让它被 Spring 容器管理。\n原因在 spring 容器托管的 OncePerRequestFilter 的 bean，都会自动加入到 servlet 的 filter chain，而上面的定义，还额外把 filter 加入到了 spring security 的ememberMeAuthenticationFilter 之前。而 spring security 也是一系列的 filter，在 mvc 的 filter 之前执行。因此在鉴权通过的情况下，就会先后各执行一次。\n参考资料解决 spring security 自定义 filter 重复执行问题\n","categories":["个人笔记"],"tags":["Spring-Security"]},{"title":"记一次 Postgres CPU 爆满故障","url":"/p/7877/","content":"问题描述公司项目测试环境调用某些接口的时候，服务器立即崩溃，并一定时间内无法提供服务。\n问题排查服务器配置不够第一反应是服务器需要升配啦，花钱解决一切！毕竟测试服务器配置确实不高，2CPU + 4Gib，能干啥？不过问题是今天突然发生的，而且说崩就崩。凭着严谨的态度，还是要刨根问底地找下问题。\n查看服务器负载\nfree -m\n\n内存占用并不大，忘记截图了，反正看下来不是内存过高导致的崩溃\n\ntop\n\n\n数据库占用 CPU 过高连接数过多\n业务高峰活跃连接陡增，活跃的连接数是否比平时多很多\n\nSELECT   COUNT(*) FROM   pg_stat_activity WHERE   STATE NOT LIKE '%idle';\n\n查询下来只有 3 个连接，所以不是连接数导致的 CPU 过高\n慢 SQL\n如果活跃连接数的变化处于正常范围，则可能是当时有性能很差的 SQL 被大量执行。\n\nselect   datname,   usename,   client_addr,   application_name,   state,   backend_start,   xact_start,   xact_stay,   query_start,   query_stay,   replace(    query,     chr(10),     ' '  ) as query from   (    select       pgsa.datname as datname,       pgsa.usename as usename,       pgsa.client_addr client_addr,       pgsa.application_name as application_name,       pgsa.state as state,       pgsa.backend_start as backend_start,       pgsa.xact_start as xact_start,       extract(        epoch         from           (now() - pgsa.xact_start)      ) as xact_stay,       pgsa.query_start as query_start,       extract(        epoch         from           (now() - pgsa.query_start)      ) as query_stay,       pgsa.query as query     from       pg_stat_activity as pgsa     where       pgsa.state != 'idle'       and pgsa.state != 'idle in transaction'       and pgsa.state != 'idle in transaction (aborted)'  ) idleconnections order by   query_stay desc limit   5;\n\n\n可以看到，确实有一条慢 SQL，而且属于奇慢无比，执行了接近 1 分钟还没执行完毕，基本可以定位，是慢 SQL 导致的 CPU 占用陡增。\n问题解决对于上面的方法查出来的慢 SQL，首先需要做的是 Kill 掉他们，使业务先恢复。\nselect pg_cancel_backend(pid) from pg_stat_activity where  query like '%&lt;query text&gt;%' and pid != pg_backend_pid();select pg_terminate_backend(pid) from pg_stat_activity where  query like '%&lt;query text&gt;%' and pid != pg_backend_pid();\n\n如果这些 SQL 确实是业务上必需的，则需要对他们做如下优化：\n\n对查询涉及的表，执行 ANALYZE &lt;table&gt; 或 VACUUM ANZLYZE &lt;table&gt;，更新表的统计信息，使查询计划更准确。为避免对业务影响，最好在业务低峰执行。\n执行 explain &lt;query text&gt; 或 explain (buffers true, analyze true, verbose true) &lt;query text&gt; 命令，查看 SQL 的执行计划（前者不会实际执行 SQL，后者会实际执行而且能得到详细的执行信息），对其中的 Table Scan 涉及的表，建立索引。\n重新编写 SQL，去除掉不必要的子查询、改写 UNION ALL、使用 JOIN CLAUSE 固定连接顺序等，都是进一步深度优化 SQL 的手段，这里不再深入说明。\n\n总结在查询语句中，尽量减少不必要的子查询，公司使用的 ORM 框架是 Spring JPA，针对一些特别慢的 HQL，可以采用直接执行 SQL 的方式来优化查询效率。\n@Query(value = \"select count(*) from example_table where example_id = :exampleId\", nativeQuery = true)int exampleNativeQuery(@Param(\"exampleId\") Long exampleId);\n\n参考PostgreSQL/PPAS CPU 使用率高的原因及解决办法\n","categories":["个人笔记"],"tags":["数据库"]},{"title":"记一次生产事故 -- 磁盘被占满","url":"/p/58700/","content":"写在前面今天，跑在阿里云 ECS 上的生产环境，突然间访问异常，接口各种报错，无奈公司没有专业的运维人员，只能硬着头皮解决一下。\n问题排查先从表面看起，数据库首先报错\nCaused by: org.postgresql.util.PSQLException: ERROR: could not extend file \"base/16385/16587_fsm\": No space left on device  建议：Check free disk space.\n\n直观上看，设备没有可用空间，也就是磁盘满了。\n进入服务器后台，执行\n$ df -hFilesystem            Size  Used Avail Use% Mounted onudev                  7.9G     0  7.9G   0% /devtmpfs                 1.6G  3.5M  1.6G   1% /run/dev/vda1              59G   56G     0 100% /tmpfs                 7.9G  4.0K  7.9G   1% /dev/shmtmpfs                 5.0M  4.0K  5.0M   1% /run/locktmpfs                 7.9G     0  7.9G   0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G   14G  937G   2% /datatmpfs                 1.6G     0  1.6G   0% /run/user/0\n\n发现确实磁盘满了，而且满的很彻底。系统盘占用 100%，估计什么服务都跑不动了。/dev/vda1              59G   56G     0 100% /\n不过发现 /dev/mapper/vg0-vol0 1000G   14G  937G   2% /data，1000G 只用了 2%\n\n阿里云 ECS 分为系统盘和数据盘，1000G 的是数据盘\n\n第一反应，应该是搭建的 PG 数据库的数据没有移到数据盘上。\n将 Postgres 数据库数据目录移动到系统盘\n参考如何将 PostgreSQL 数据目录移动到 Ubuntu 16.04 上的新位置\n\n$ sudo -u postgres psqlpostgres# SHOW data_directory; # 查看当前数据目录        data_directory        ------------------------------ /var/lib/postgresql/9.5/main(1 row)postgres# \\q; # 退出# 为了确保数据的完整性，我们将在实际更改数据目录之前关闭PostgreSQL$ sudo systemctl stop postgresql# 确保关闭完成$ sudo systemctl status postgresql. . .Jul 22 16:22:44 ubuntu-512mb-nyc1-01 systemd[1]: Stopped PostgreSQL RDBMS.$ sudo rsync -av /var/lib/postgresql /data # /data为要迁移到的新目录$ cd /data$ ls... postgresql# 删除原数据目录$ sudo rm -rf /var/lib/postgresql# 将新数据目录链接到原数据目录$ sudo ln -s /data/postgresql /var/lib/postgresql# 重启Postgres数据库$ sudo systemctl start postgresql$ sudo systemctl status postgresql\n\n完成以上步骤，即将 postgre 数据库数据目录移到了阿里云数据盘\n以为 OK 了，执行\n$ df -hFilesystem            Size  Used Avail Use% Mounted onudev                  7.9G     0  7.9G   0% /devtmpfs                 1.6G  3.5M  1.6G   1% /run/dev/vda1              59G   56G   51M 100% /tmpfs                 7.9G  4.0K  7.9G   1% /dev/shmtmpfs                 5.0M  4.0K  5.0M   1% /run/locktmpfs                 7.9G     0  7.9G   0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G   14G  937G   2% /datatmpfs                 1.6G     0  1.6G   0% /run/user/0\n\n纹丝未动。。。\nUbuntu 查询大文件猜测是存在大文件导致磁盘被占满\n$ cd /$ find . -type f -size +800M  -print0 | xargs -0 du -h5.6G ./var/log/syslog.16.7G ./var/log/syslog...$ rm ...\n\n如果发现是 log 字眼的大文件，我们可以毫不留情的删掉，要是遇见一些不认识的，不要贸然删掉，一定要查清楚文件的作用，能删则删，千万不要不小心删库跑路。。。\n删除完毕后，再次查看\n$ df -hFilesystem            Size  Used Avail Use% Mounted onudev                  7.9G     0  7.9G   0% /devtmpfs                 1.6G  3.4M  1.6G   1% /run/dev/vda1              59G   45G   12G  80% /tmpfs                 7.9G  4.0K  7.9G   1% /dev/shmtmpfs                 5.0M  4.0K  5.0M   1% /run/locktmpfs                 7.9G     0  7.9G   0% /sys/fs/cgroup/dev/mapper/vg0-vol0 1000G   14G  936G   2% /datatmpfs                 1.6G     0  1.6G   0% /run/user/0\n\n多出了 12G。\n查看已删除空间却没有释放的进程这时候，服务应该可以恢复成功。但你马上会发现，磁盘又被占满，而这次，日志文件却不算大。\n查看已经删除的文件，空间有没有释放，没有的话 kill 掉 pid\n\n使用 rm 删除文件的时候，虽然文件已经被删除，但是由于文件被其他进程占用，空间却没有释放\n\n$ sudo lsof -n |grep deletedjava      17866                  root  237r      REG              253,1    163541    1709285 /tmp/tomcat.8250394289784312179.8080/work/Tomcat/localhost/ROOT/upload_c6db0c17_6e6a_4141_bfb6_ac1b2d8a3b0b_00000000.tmp (deleted)...$ sudo kill -9 17866\n\n再次使用 df -h 命令，磁盘使用率一下子减少了好多。\n总结\n服务器系统盘被占满是非常可怕的！届时，一切服务都将变得不可用，业务系统也会莫名其妙多出奇怪的问题。所以，运维需要经常性的查看服务器磁盘占用情况，阿里云 ECS 用户，可以开启报警，及时发现问题，解决问题！\n\n阿里云 ECS 提供了系统盘和数据盘，记住，例如 Pg、Redis、Cassandra 等容易占磁盘的服务，一定要将数据目录放在阿里云 ECS 提供的数据盘上。\n\n/var/log 是系统日志目录，可以经常性的关注下，大容量日志尽早删除。\n\n对待进程不停对文件写日志的操作，要释放文件占用的磁盘空间，最好的方法是在线清空这个文件，可以通过如下命令完成：\n\n\n[root@localhost ~]# echo \"\" &gt;/var/log/syslog\n\n\n 通过这种方法，磁盘空间不但可以马上释放，也可保障进程继续向文件写入日志，这种方法经常用于在线清理 Apache、Tomcat、Nginx 等 Web 服务产生的日志文件。\n\n最后，有一个专业的运维是多么重要！\n","categories":["个人笔记"],"tags":["运维"]},{"title":"运维笔记（部署篇）","url":"/p/8505/","content":"前言针对 Ubuntu 16.04，汇总常用服务的搭建指南。\n\n\n系统初始化\n新买的 ECS 需要执行系统初始化\n\n$ sudo apt update$ sudo apt dist-upgrade$ sudo apt autoremove$ sudo apt clean$ cat /etc/hosts # 修改hosts，一般将本机需要使用的外部内网服务设置映射为名称172.16.0.192    kftest-config01$ cat /etc/hostname # 修改hostname，便于辨认pg_1$ reboot # 修改hostname需要重启生效# 挂载数据盘，例如阿里云数据盘 https://help.aliyun.com/document_detail/25446.html$ sudo fdisk -l # 查看实例上的数据盘Disk /dev/vdb: 1000 GiB, 1073741824000 bytes, 2097152000 sectors$ sudo fdisk -u /dev/vdbCommand (m for help): n... 一路enterCommand (m for help): w## 更多参考 https://help.aliyun.com/document_detail/108501.html$ sudo fdisk -lu /dev/vdbDevice     Boot Start        End    Sectors  Size Id Type/dev/vdb1        2048 2097151999 2097149952 1000G 83 Linux$ sudo mkfs.ext4 /dev/vdb1 # 在新分区上创建一个文件系统$ cp /etc/fstab /etc/fstab.bak # 备份 etc/fstab$ echo /dev/vdb1 /data ext4 defaults 0 0 &gt;&gt; /etc/fstab # 向 /etc/fstab 写入新分区信息$ sudo mkdir /data$ sudo mount /dev/vdb1 /data # 挂载文件系统$ df -h/dev/vdb1       985G   72M  935G   1% /data\n\nPostgresql安装 Postgresql$ sudo apt install ca-certificates$ RELEASE=$(lsb_release -cs)$ echo \"deb https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/apt/ ${RELEASE}\"-pgdg main | sudo tee  /etc/apt/sources.list.d/pgdg.list$ wget --quiet -O - https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/apt/ACCC4CF8.asc | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install postgresql-9.6 # 自行选择合适版本## 更多参考 https://www.postgresql.org/download/linux/ubuntu/\n\n修改配置文件$ sudo vim /etc/postgresql/9.6/main/postgresql.conflisten_addresses = '*'max_connections = 1000logging_collector = ondata_directory = '/data/postgresql'## 更多参考 https://www.postgresql.org/docs/current/static/runtime-config.html $ sudo vim /etc/postgresql/9.6/main/pg_hba.confhost    all             all             0.0.0.0/0             md5## 更多参考 https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html   $ sudo service postgresql restart\n\n修改数据目录参考：https://www.cnblogs.com/easonjim/p/9052836.html\n修改默认用户 Postgres 的密码$ sudo -u postgres psql# ALTER USER postgres WITH PASSWORD 'postgres';# \\q$ exit\n\n搭建集群（可选）\n\n\n主机\n ip\n\n\n\nMaster 节点\n 10.10.10.10\n\n\nSlave 节点\n 10.10.10.9\n\n\nMaster 节点和 Slave 节点分别按照上述步骤安装完成 postgres 后，开始搭建集群。\nmaster 节点：\n修改配置 \n\n$ sudo vi /etc/postgresql/9.6/main/postgresql.conflisten_addresses = '*'wal_level = hot_standbyarchive_mode = onarchive_command = 'test ! -f /var/lib/postgresql/9.6/archive/%f &amp;&amp; cp %p /var/lib/postgresql/9.6/archive/%f'max_wal_senders = 16wal_keep_segments = 100hot_standby = onlogging_collector = on## 更多参考 https://www.postgresql.org/docs/current/static/runtime-config.html$ sudo vi /etc/postgresql/9.6/main/pg_hba.confhost    all             all             10.0.0.0/8              md5host    replication     repuser         10.0.0.0/8              md5## 更多参考 https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html  $ sudo -upostgres mkdir /var/lib/postgresql/9.6/archive$ sudo chmod 0700 /var/lib/postgresql/9.6/archive  $ sudo service postgresql restart\n\n\n创建工作账户 repuser\n\n$ sudo -upostgres createuser --replication repuser$ sudo -upostgres psqlpostgres=# \\password repuser&lt;password&gt;## 更多参考 https://www.postgresql.org/docs/current/static/user-manag.html\n\nslave 节点：\n先停止服务 \n\n$ sudo service postgresql stop\n\n\n由 master 节点导入数据（postgres 免密码登录 repuser role）\n\n$ sudo -upostgres vi /var/lib/postgresql/.pgpass10.10.10.10:5432:*:repuser:&lt;password&gt;127.0.0.1:5432:*:repuser:&lt;password&gt; $ sudo chmod 0600 /var/lib/postgresql/.pgpass$ sudo mv /var/lib/postgresql/9.6/main /var/lib/postgresql/9.6/main.bak$ sudo -upostgres pg_basebackup -D /var/lib/postgresql/9.6/main -F p -X stream -v -R -h 10.10.10.10 -p 5432 -U repuser\n\n\n修改配置 \n\n$ sudo vi /var/lib/postgresql/9.6/main/recovery.confstandby_mode = 'on'primary_conninfo = 'user=repuser host=10.10.10.10 port=5432'trigger_file = 'failover.now'## 更多参考 https://www.postgresql.org/docs/current/static/recovery-config.html  $ sudo vi /etc/postgresql/9.6/main/postgresql.confhot_standby = on\n\n\n重启并检查服务 \n\n$ sudo service postgresql start  $ sudo service postgresql status...Active: active (exited)$ sudo -upostgres psqlpsql (9.6.12)...\n\n测试集群在 master 节点进行增删改操作，对照看 slave 节点是否能够从 master 节点复制操作\n常用命令$ sudo service postgresql start$ sudo service postgresql status$ sudo service postgresql restart\n\n👉 PG 数据库常用命令\nRedis安装 Redis（单机）$ sudo apt-get install redis-server$ sudo vim /etc/redis/redis.conf# bind 127.0.0.1# protected-mode yesprotected-mode no$ sudo systemctl restart redis-server\n\n安装 Redis（集群）\n\n\n主机\n ip\nredis-server\nsentinel\n\n\n\nnode01\n10.10.10.5\n 主\n√\n\n\nnode02\n10.10.10.4\n 从\n√\n\n\nnode03\n10.10.10.6\n 从\n√\n\n\n安装 Redis-Servernode01:$ sudo apt-get install redis-server$ sudo vi /etc/redis/redis.confbind: 10.10.10.5$ sudo service redis-server restartnode02:$ sudo apt-get install redis-server$ sudo vi /etc/redis/redis.confbind: 10.10.10.4slaveof 10.10.10.5 $ sudo service redis-server restartnode03 同node02\n\n测试主从同步node01:$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt;info....# Replicationrole:masterconnected_slaves:2slave0:ip=10.10.10.4,port=6379,state=online,offset=99,lag=0slave1:ip=10.10.10.6,port=6379,state=online,offset=99,lag=1master_repl_offset:99....10.10.10.5:6379&gt;set testkey testvalueOK10.10.10.5:6379&gt;get testkey\"testvalue\"  node02:$ redis-cli -h 10.10.10.4 -p 637910.9.8.203:6379&gt;info...# Replicationrole:slavemaster_host:10.10.10.5master_port:6379master_link_status:up...10.10.10.4:6379&gt;get testkey\"testvalue\"\n\n配置 Sentinel（可选）\n一个稳健的 Redis Sentinel 集群，应该使用至少 三个 Sentinel 实例，并且保证将这些实例放到 不同的机器 上，甚至不同的 物理区域。\n\n$ sudo wget http://download.redis.io/redis-stable/sentinel.conf -O /etc/redis/sentinel.conf$ sudo chown redis:redis /etc/redis/sentinel.conf$ sudo vi /etc/redis/sentinel.confsentinel monitor mymaster 10.10.10.5 6379 2sentinel down-after-milliseconds mymaster 60000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000## 自启动配置$ sudo vi /etc/redis/sentinel.service[Unit]Documentation=http://redis.io/topics/sentinel[Service]ExecStart=/usr/bin/redis-server /etc/redis/sentinel.conf --sentinelUser=redisGroup=redis[Install]WantedBy=multi-user.target  $ sudo ln -s /etc/redis/sentinel.service /lib/systemd/system/sentinel.service$ sudo systemctl enable sentinel.service$ sudo service sentinel startnode02 node03 sentinel 配置同node01，所有节点配置完成，再继续下一步\n\n\n配置好 sentinel 之后，redis.conf 和 sentinel.conf 都由 sentinel 接管；sentinel 监控主节点发生改变的话，会更改对应的配置文件 sentinel.conf 和 redis.conf。\n\n测试 Sentinel 监控、通知、自动故障转移# 查看所有节点哨兵配置node01,node02,node03:$ redis-cli -h 10.10.10.5 -p 2637910.10.10.5:26379&gt; info# Serverredis_version:3.0.6...config_file:/etc/redis/sentinel.conf# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.5:6379,slaves=2,sentinels=1# 在从节点查看哨兵详情，关注主节点$ redis-cli -h 10.10.10.4 -p 2637910.10.10.5:26379&gt; sentinel master mymaster 1) \"name\" 2) \"mymaster\" 3) \"ip\" 4) \"10.10.10.5\" 5) \"port\" 6) \"6379\"...# 停止主节点所在redis-servernode01:$ systemctl stop redis-server.service# 查看从节点的哨兵详情，一般来说，过1分钟~2分钟，会自动选举出新的主节点，例如node03被推举为主节点node02:$ redis-cli -h 10.10.10.4 -p 2637910.10.10.4:26379&gt; info...# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.6:6379,slaves=2,sentinels=3$ redis-cli -h 10.10.10.6 -p 637910.10.10.6:6379&gt; info# Replicationrole:masterconnected_slaves:1slave0:ip=10.10.10.4,port=6379,state=online,offset=19874,lag=0master_repl_offset:19874...# 启动刚才被停止的原主节点redis-server，将作为从节点加入到redis集群node01:$ systemctl start redis-server$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt; info...# Replicationrole:slavemaster_host:10.10.10.6master_port:6379master_link_status:up...$ redis-cli -h 10.10.10.5 -p 2637910.10.10.5:26379&gt; info...# Sentinelsentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0master0:name=mymaster,status=ok,address=10.10.10.6:6379,slaves=2,sentinels=3\n\n客户端连接 Sentinel配置完 sentinel，客户端连接方式就改变了，拿 Redisson 举例，需要增加以下配置，并删除单机模式下 spring.redis.host 配置，端口号改成哨兵的端口号\nspring.redis.sentinel.master=mymasterspring.redis.sentinel.nodes=10.10.10.4:26379,10.10.10.5:26379,10.10.10.6:26379\n\n引入的 jar 是\ncompile \"org.redisson:redisson-spring-boot-starter:3.9.1\"\n\n配置类所在位置：\norg.springframework.boot.autoconfigure.data.redis.RedisProperties.Sentinel\n\n常用命令$ sudo systemctl start redis$ sudo systemctl enable redis$ sudo systemctl restart$ sudo systemctl stop redis\n\n常见问题\n有时可能会遇到关闭或重启不了，这时候可以使用 redis-cli 提供的命令行来强制关闭 \n\n$ redis-cli -h 10.10.10.5 -p 637910.10.10.5:6379&gt; shutdown nosave## 更多参考 https://redis.io/commands/SHUTDOWN\n\n\nRedis is configured to save RDB snapshots, but is currently not able to persist on disk.\n\n\nRedis 被配置为保存数据库快照，但它目前不能持久化到硬盘。\n\n$ vim /etc/sysctl.conf## 添加一行vm.overcommit_memory=1$ sudo sysctl -p /etc/sysctl.conf## 重启所有节点redis-server和sentinel\n\n如果改好后，还不行，就需要查看下 Redis 的 dump 文件配置是不是被更改了\n$ redis-cli -h 10.10.10.510.10.10.5:6379&gt; CONFIG GET dbfilename1) \"dbfilename\"2) \".rdb\" ## 默认是dump.rdb10.10.10.5:6379&gt; CONFIG GET dir1) \"dir\"2) \"/var/spool/cron\" ## 默认是dump.rdb\n\n以上配置，如果不是自己更改的，则可怀疑是被黑客篡改了\n\n检查 Redis 端口是否在公网开放，如果是，立马关闭\n设置 Redis 访问密码\n恢复 Redis 默认配置 \n\n$ vim /etc/redis/redis.confdbfilename \"dump.rdb\"dir \"/var/lib/redis\"$ service redis-server restartnode01 node02 node03均按此修改并重启## 了解更多 https://serverfault.com/questions/800295/redis-spontaneously-failed-failed-opening-rdb-for-saving-permission-denied\n\nConsul安装 Consul（单机）$ sudo mkdir -p /data/consul/{current/{bin,etc},data}$ sudo wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip -O /data/consul/consul_1.5.3_linux_amd64.zip$ sudo apt-get install unzip$ sudo unzip /data/consul/consul_1.5.3_linux_amd64.zip -d /data/consul/current/bin$ sudo vi /data/consul/current/etc/consul.json{    \"bootstrap\": true,    \"datacenter\": \"test-datacenter\",    \"data_dir\": \"/data/consul/data\",    \"log_level\": \"INFO\",    \"server\": true,    \"client_addr\": \"0.0.0.0\",    \"ui\": true,    \"start_join\": [\"ip:8301\"],    \"enable_syslog\": true}## 更多参考：https://www.consul.io/docs/agent/options.html#configuration_files$ sudo ln -s /data/consul/current/etc /data/consul/etc$ sudo vi /etc/systemd/system/consul.service[Unit]Description=consul service[Service]ExecStart=/data/consul/current/bin/consul agent -bind={ip} -config-dir /data/consul/etc/consul.jsonUser=root[Install]WantedBy=multi-user.target$ sudo systemctl enable consul.service$ sudo systemctl start consul.service\n\n安装 Consul（集群）\n\n\n主机\n ip\n\n\n\nnode01\n10.10.10.5\n\n\nnode02\n10.10.10.4\n\n\nnode03\n10.10.10.6\n\n\nnode01 node02 node03$ sudo mkdir -p /data/consul/{current/{bin,etc},data}$ sudo wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip -O /data/consul/consul_1.5.3_linux_amd64.zip$ sudo apt-get install unzip$ sudo unzip /data/consul/consul_1.5.3_linux_amd64.zip -d /data/consul/current/bin$ sudo vi /data/consul/current/etc/consul.json{    \"datacenter\": \"roc-datacenter\",    \"data_dir\": \"/data/consul/data\",    \"log_level\": \"INFO\",    \"server\": true,    \"bootstrap_expect\": 3,    \"client_addr\": \"10.10.10.4\",    \"ui\": true,    \"start_join\": [\"10.10.10.4:8301\",\"10.10.10.5:8301\",\"10.10.10.6:8301\"],    \"enable_syslog\": true}## 更多参考：https://www.consul.io/docs/agent/options.html#configuration_files$ sudo ln -s /data/consul/current/etc /data/consul/etc$ sudo vi /etc/systemd/system/consul.service[Unit]Description=consul service[Service]ExecStart=/data/consul/current/bin/consul agent -config-dir /data/consul/etc/consul.jsonUser=root[Install]WantedBy=multi-user.target$ sudo systemctl enable consul.service$ sudo systemctl start consul.service\n\n需要开放的端口：8300, 8301, 8500，如果网络不通，则子节点将无法 join 到主节点，可能会出现\nfailed to sync remote state: No cluster leader\n\n无法选举出 leader，其实是节点之间无法通信，如果通信正常，启动之时所有节点会自动推举出 leader。\n常用命令$ sudo systemctl start consul.service$ sudo systemctl stop consul.service$ sudo systemctl restart consul.service## 更多参考：https://www.consul.io/docs/commands/index.html\n\nNginx安装 Nginx$ echo -e \"deb http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\\ndeb-src http://nginx.org/packages/ubuntu/ $(lsb_release -cs) nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list$ wget -O- http://nginx.org/keys/nginx_signing.key | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install nginx## 更多参考：http://nginx.org/en/linux_packages.html#stable\n\n常用命令$ sudo service nginx start$ sudo service nginx stop$ sudo service nginx restart$ sudo service nginx reload # 重新加载配置\n\nCassandra 集群\n\n\n主机\n IP\n\n\n\ncassandra-1\n192.168.0.1\n\n\ncassandra-2\n192.168.0.2\n\n\n安装 Cassandra$ echo \"deb http://www.apache.org/dist/cassandra/debian 39x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list$ curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -$ sudo apt update$ sudo apt -y install cassandra$ sudo apt install openjdk-8-jdk-headless## 更多参考：http://cassandra.apache.org/download/#installation-from-debian-packages\n\n修改配置文件$ sudo vi /etc/cassandra/cassandra.yamlseed_provider:          - seeds: \"192.168.0.1,192.168.0.2\" concurrent_writes: 64concurrent_counter_writes: 64concurrent_counter_writes: 64concurrent_materialized_view_writes: 64compaction_throughput_mb_per_sec: 128file_cache_size_in_mb: 1024buffer_pool_use_heap_if_exhausted: truedisk_optimization_strategy: spinning#listen_address: localhostlisten_interface: eth0#rpc_address: localhostrpc_interface: eth0enable_user_defined_functions: trueauto_bootstrap: false## 优化cassandra jvm配置$ sudo vi /etc/cassandra/jvm.options#-XX:+UseParNewGC#-XX:+UseConcMarkSweepGC#-XX:+CMSParallelRemarkEnabled#-XX:SurvivorRatio=8#-XX:MaxTenuringThreshold=1#-XX:CMSInitiatingOccupancyFraction=75#-XX:+UseCMSInitiatingOccupancyOnly#-XX:CMSWaitDuration=10000#-XX:+CMSParallelInitialMarkEnabled#-XX:+CMSEdenChunksRecordAlways-XX:+UseG1GC-XX:G1RSetUpdatingPauseTimePercent=5-XX:MaxGCPauseMillis=500-XX:InitiatingHeapOccupancyPercent=70-XX:ParallelGCThreads=16-XX:ConcGCThreads=16$ sudo vi /etc/cassandra/cassandra-env.sh## 配置为主机内网地址JVM_OPTS=\"$JVM_OPTS -Djava.rmi.server.hostname=192.168.0.1\"#if [ \"x$LOCAL_JMX\" = \"x\" ]; then#      LOCAL_JMX=yes#  fi  if [ \"x$LOCAL_JMX\" = \"x\" ]; then      LOCAL_JMX=no  fi#JVM_OPTS=\"$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=true\"JVM_OPTS=\"$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false\"#JVM_OPTS=\"$JVM_OPTS -Dcom.sun.management.jmxremote.password.file=/etc/cassandra/jmxremote.password\"$ sudo systemctl stop cassandra\n\n迁移配置导数据盘（可选）$ sudo mv /var/lib/cassandra /data/cassandra$ sudo ln -s /data/cassandra /var/lib/cassandra$ sudo systemctl start cassandra\n\n集群内其余机器，重复上述步骤，修改对应 IP\nZookeeper 集群\n\n\n主机\n IP\n\n\n\nzk-01\n192.168.0.1\n\n\nzk-02\n192.168.0.2\n\n\nzk-03\n192.168.0.3\n\n\n安装 Zookeeper$ sudo apt install zookeeperd\n\n修改配置文件$ sudo vim /etc/zookeeper/conf/zoo.cfgserver.1=192.168.0.1:2888:3888server.2=192.168.0.2:2888:3888server.3=192.168.0.3:2888:3888$ sudo vim /etc/zookeeper/conf/myid1# 每台主机id各不相同，比如zk-01=1,zk-02=2,zk-03=3$ sudo systemctl restart zookeeper\n\n安装 ZK-UI（可选）# 安装zkui$ cd /data &amp;&amp; wget https://github.com/zifangsky/zkui/releases/download/v2.0/zkui-2.0.zip$ sudo unzip zkui-2.0.zip$ sudo vi /data/zkui/config.cfg  zkServer=192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181userSet = {\"users\": [{ \"username\":\"&lt;username&gt;\" , \"password\":\"&lt;password&gt;\",\"role\": \"ADMIN\" },{ \"username\":\"appconfig\" , \"password\":\"appconfig\",\"role\": \"USER\" }]}  $ cd /data/zkui &amp;&amp; sudo bash start.sh\n\n集群内其余机器，重复上述步骤\nKafka 集群\n\n\n主机\n IP\n\n\n\nzk-01\n192.168.0.1\n\n\nzk-02\n192.168.0.2\n\n\nzk-03\n192.168.0.3\n\n\n安装 Kafka$ sudo mkdir /data/kafka &amp;&amp; cd ~$ wget \"http://www-eu.apache.org/dist/kafka/1.0.1/kafka_2.12-1.0.1.tgz\"$ curl http://kafka.apache.org/KEYS | gpg --import$ wget https://dist.apache.org/repos/dist/release/kafka/1.0.1/kafka_2.12-1.0.1.tgz.asc$ gpg --verify kafka_2.12-1.0.1.tgz.asc kafka_2.12-1.0.1.tgz$ sudo tar -xvzf kafka_2.12-1.0.1.tgz --directory /data/kafka --strip-components 1$ sudo rm -rf kafka_2.12-1.0.1.tgz kafka_2.12-1.0.1.tgz.asc## 更多参考 https://tecadmin.net/install-apache-kafka-ubuntu/\n\n修改配置文件$ sudo mkdir /data/kafka-logs$ sudo cp /data/kafka/config/server.properties{,.bak}$ sudo vim /data/kafka/config/server.properties broker.id=0    # 每台主机各不相同listeners=PLAINTEXT://0.0.0.0:9092advertised.listeners=PLAINTEXT://&lt;ip&gt;:9092delete.topic.enable = trueleader.imbalance.check.interval.seconds=5  # leader不平衡检查间隔leader.imbalance.per.broker.percentage=1log.dirs=/data/kafka-logsoffsets.topic.replication.factor=3log.retention.hours=72log.segment.bytes=1073741824zookeeper.connect=192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181  $ sudo vim /data/kafka/bin/kafka-server-start.shexport JMX_PORT=12345    # 暴露jmx端口，留待监控使用\n\n注册为 Systemd 服务$ sudo adduser --system --no-create-home --disabled-password --disabled-login kafka$ sudo chown -R kafka:nogroup /data/kafka$ sudo chown -R kafka:nogroup /data/kafka-logs  $ sudo vim /etc/systemd/system/kafka.service[Unit]Description=High-available, distributed message brokerAfter=network.target[Service]User=kafkaExecStart=/data/kafka/bin/kafka-server-start.sh /data/kafka/config/server.properties[Install]WantedBy=multi-user.target## 启用服务$ sudo systemctl enable kafka.service$ sudo systemctl start kafka.service## 更多参考 https://kafka.apache.org/quickstart\n\n测试 Kafka 的使用（可选）$ /data/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test$ /data/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181  $ /data/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt; Hello World  # 另外一个terminal$ /data/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningHello World\n\n部署 Kafka-manager$ cd /data &amp; sudo wget https://github.com/yahoo/kafka-manager/archive/1.3.3.17.zip$ sudo unzip kafka-manager-1.3.3.17.zip$ sudo mv kafka-manager-1.3.3.17 kafka-manager$ sudo chown -R kafka:nogroup /data/kafka-manager$ sudo vim /data/kafka-manager/conf/application.confkafka-manager.zkhosts=\"192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181\"basicAuthentication.enabled=truebasicAuthentication.username=\"&lt;username&gt;\"basicAuthentication.password=\"&lt;password&gt;\"  $ sudo vim /etc/systemd/system/kafka-manager.service[Unit]Description=High-available, distributed message broker managerAfter=network.target[Service]User=kafkaExecStart=/data/kafka-manager/bin/kafka-manager[Install]WantedBy=multi-user.target## 启用服务$ sudo systemctl enable kafka-manager.service$ sudo systemctl start kafka-manager.service\n\nMysql安装 Mysql$ sudo apt-get update$ sudo apt-get install mysql-server\n\n在安装过程中，系统将提示您创建 root 密码。请务必记住 root 密码\n配置 Mysql运行安全脚本\n$ mysql_secure_installation\n\n值得一提的是，Disallow root login remotely?，如果你需要使用 root 账号进行远程连接，请选择 No\n验证接下来测试下是否安装成功了\n\n运行状态 \n\n$ systemctl status mysql.service● mysql.service - MySQL Community Server   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)   Active: active (running) since Thu 2019-07-18 23:38:43 PDT; 11min ago Main PID: 2948 (mysqld)    Tasks: 28   Memory: 142.6M      CPU: 545ms   CGroup: /system.slice/mysql.service           └─2948 /usr/sbin/mysqld\n\n\n登录查看版本 \n\n$ mysqladmin -p -u root versionmysqladmin  Ver 8.42 Distrib 5.7.26, for Linux on x86_64Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Server version\t\t5.7.26-0ubuntu0.16.04.1Protocol version\t10Connection\t\tLocalhost via UNIX socketUNIX socket\t\t/var/run/mysqld/mysqld.sockUptime:\t\t\t12 min 18 secThreads: 1  Questions: 36  Slow queries: 0  Opens: 121  Flush tables: 1  Open tables: 40  Queries per second avg: 0.048\n\n到这里，Mysql 安装完成！\n参考\nSystemd 入门教程：命令篇\n\n","categories":["个人笔记"],"tags":["Ubuntu","运维"]},{"title":"阿里云短信开箱即用","url":"/p/28056/","content":"简介\n使用 SpringBoot 自动装配简化对接阿里云短信过程。\n\n小工具一枚，欢迎使用和 Star 支持，如使用过程中碰到问题，可以提出 Issue，我会尽力完善该 Starter。\n\n\n版本基础aliyun-java-sdk-core:4.1.0\n如何使用Maven&lt;dependency&gt;  &lt;groupId&gt;io.github.gcdd1993&lt;/groupId&gt;  &lt;artifactId&gt;ali-sms-spring-boot-starter&lt;/artifactId&gt;  &lt;version&gt;1.0.0.RELEASE&lt;/version&gt;&lt;/dependency&gt;\n\nGradlecompile 'io.github.gcdd1993:ali-sms-spring-boot-starter:1.0.0.RELEASE'\n\n👉注意：需要引入 Jcenter 仓库\n参数配置以 application.yml 举例\nali:  sms:  \tdomain: \"dysmsapi.aliyuncs.com\" ## 默认dysmsapi.aliyuncs.com  \tversion: \"2017-05-25\" ## 默认2017-05-25  \taction: \"SendSms\" ## 默认SendSms    access-key:      id: \"${阿里云短信AccessKeyId}\"      secret: \"${阿里云短信AccessKeySecret}\"    region-id: \"${阿里云短信地域}\"    sign-name: \"${阿里云短信签名}\" ## 如果不填，必须在发送方法中指定\n\n基本使用同步发送短信为了方便使用，接口上进行了方法的重载，提供 5 种不同的参数列表供选择，你可以自行选择使用\n/** * 同步发送短信 * &lt;p&gt; *     参数1：使用的短信模板ID *     参数2：接收者的手机号，如\"17602526129,17602923211\" *     参数3：Map，key对应模板中的参数名，value对应值（这里是使用Jackson来序列化） * &lt;/p&gt; */@Testpublic void sendSync() {    SmsResponse smsResponse = sendService.sendSync(TEMPLATE_ID, PHONE_NUMBER, MAP);    Assert.assertTrue(smsResponse.isSuccess());}/** * 同步发送短信 * &lt;p&gt; *     参数1：使用的短信模板ID *     参数2：接收者的手机号，如\"17602526129,17602923211\" *     参数3：要发送的短信写入值，你可以自己进行json的拼装。注意要进行json的转义，例如：\"{\\\"code\\\":\\\"112233\\\"}\" * &lt;/p&gt; */@Testpublic void sendSync1() {    SmsResponse smsResponse = sendService.sendSync(TEMPLATE_ID, PHONE_NUMBER, \"{\\\"code\\\":\\\"112233\\\"}\");    Assert.assertTrue(smsResponse.isSuccess());}/** * 同步发送短信 * &lt;p&gt; *     参数1：短信签名，适用于同一模板需要有不同短信签名的 *     参数2：使用的短信模板ID *     参数3：接收者的手机号，如\"17602526129,17602923211\" *     参数4：Map，key对应模板中的参数名，value对应值（这里是使用Jackson来序列化） * &lt;/p&gt; */@Testpublic void sendSync2() {    SmsResponse smsResponse = sendService.sendSync(SIGN_NAME, TEMPLATE_ID, PHONE_NUMBER, MAP);    Assert.assertTrue(smsResponse.isSuccess());}/** * 同步发送短信 * &lt;p&gt; *     参数1：短信签名，适用于同一模板需要有不同短信签名的 *     参数2：使用的短信模板ID *     参数3：接收者的手机号，如\"17602526129,17602923211\" *     参数4：要发送的短信写入值，你可以自己进行json的拼装。注意要进行json的转义，例如：\"{\\\"code\\\":\\\"112233\\\"}\" * &lt;/p&gt; */@Testpublic void sendSync3() {    SmsResponse smsResponse = sendService.sendSync(SIGN_NAME, TEMPLATE_ID, PHONE_NUMBER, \"{\\\"code\\\":\\\"112233\\\"}\");    Assert.assertTrue(smsResponse.isSuccess());}\n\n最后一个提供了一个参数对象来定义短信发送请求，如果不嫌麻烦，可以使用这个。\n/** * 阿里云短信请求体 * * @author gaochen * @date 2019/6/6 */@Datapublic class SmsRequest {    /**     * 接收短信的手机号码。以英文逗号（,）分隔。     */    private String phoneNumbers;    /**     * 短信签名名称。请在控制台签名管理页面签名名称一列查看。     */    private String signName;    /**     * 短信模板ID，前缀为SMS_     */    private Integer templateId;    /**     * 阿里云短信内容,key:短信模板中的字段名，value：短信模板字段对应值     * 使用此字段需要{@link com.fasterxml.jackson.databind.ObjectMapper}     */    private Map&lt;String, String&gt; params;    /**     * json str of  {@link #getParams()}     * 使用此字段请设置params为Null     */    private String paramStr;}\n\n使用：\n@Testpublic void sendSync4() {    SmsRequest smsRequest = new SmsRequest();    smsRequest.setPhoneNumbers(PHONE_NUMBER);    smsRequest.setTemplateId(TEMPLATE_ID);    smsRequest.setParams(MAP);    SmsResponse smsResponse = sendService.sendSync(smsRequest);    Assert.assertTrue(smsResponse.isSuccess());}\n\n异步发送短信\n考虑到发短信的需求，一般来说都需要异步加持，对以上 5 种方法分别提供了异步接口 sendAsync，使用方法基本一致，唯一不同的是，你可以异步处理短信发送返回值。\n\nCompletableFuture&lt;SmsResponse&gt; smsResponse = sendService.sendAsync(TEMPLATE_ID, PHONE_NUMBER, MAP);smsResponse.thenAcceptAsync(sr -&gt; {    if (sr.isSuccess()) {        System.out.println(\"发短信成功\");    } else {        System.out.println(\"发送到消息队列，准备重试此次短信\");    }});\n\n高级使用除了使用以上方法发送短信外，你还可以使用官方的 IAcsClient 来发送短信，如\npackage io.github.gcdd1993.demo;import com.aliyuncs.CommonRequest;import com.aliyuncs.IAcsClient;import com.aliyuncs.request;import com.aliyuncs.CommonResponse;import com.aliyuncs.exceptions.ClientException;import com.aliyuncs.http.MethodType;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import io.github.gcdd1993.alisms.domain.SmsRequest;import io.github.gcdd1993.alisms.domain.SmsResponse;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;/** * @author gaochen * @date 2019/6/8 */@Servicepublic class SendService {    @Autowired    private IAcsClient acsClient;    public SmsResponse sendSync() {        try {            CommonRequest request = new CommonRequest();            request.setMethod(MethodType.POST);            request.setDomain(\"dysmsapi.aliyuncs.com\");            request.setVersion(\"2017-05-25\");            request.setAction(\"SendSms\");            request.putQueryParameter(\"RegionId\", \"region\");            request.putQueryParameter(\"PhoneNumbers\", \"1771636783\");            request.putQueryParameter(\"SignName\", \"SignName\");            request.putQueryParameter(\"TemplateCode\", \"SMS_12345678\");            request.putQueryParameter(\"TemplateParam\", \"{\\\"code\\\":\\\"112233\\\"}\");            CommonResponse commonResponse = acsClient.getCommonResponse(request);            return SmsResponse.SmsResponseBuilder.build(commonResponse);        } catch (ClientException e) {            log.error(\"send msg error.\", e);            return SmsResponse.SmsResponseBuilder.buildFail(e.getMessage());        } catch (JsonProcessingException e) {            log.error(\"write json failed.\", e);            return SmsResponse.SmsResponseBuilder.buildFail(\"短信参数在json序列化时出错\");        }    }}\n\nLicensesThe Apache License, Version 2.0\nIssuesIssues Welcome\n支持\nClick Github to star, Thanks!\n\n更多参考阿里云短信服务 API 参考\n","categories":["个人笔记"],"tags":["Spring Boot"]}]